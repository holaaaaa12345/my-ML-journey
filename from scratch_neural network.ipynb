{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02737cbd",
   "metadata": {},
   "source": [
    "# Neural network learning from scratch (Math + Numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c588d68a",
   "metadata": {},
   "source": [
    "This notebook is a part of the series __From Scratch__, where i try to code machine learning algorithms from scratch (just numpy and math)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7a2fbf",
   "metadata": {},
   "source": [
    "This is a major milestone during my self-taught machine learning journey. Understanding the backpropagation and calculus in it is quite an experience. That said, this is really the simplest implementation :p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "285ac7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Normalization\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid, softmax\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df75c1df",
   "metadata": {},
   "source": [
    "### Objective: Determine a tumor's malignancy given its texture_mean and radius_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9e7e12",
   "metadata": {},
   "source": [
    "#### The dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c8caac3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>radius_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10.38</td>\n",
       "      <td>17.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17.77</td>\n",
       "      <td>20.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>21.25</td>\n",
       "      <td>19.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>20.38</td>\n",
       "      <td>11.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>14.34</td>\n",
       "      <td>20.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>1</td>\n",
       "      <td>22.39</td>\n",
       "      <td>21.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>1</td>\n",
       "      <td>28.25</td>\n",
       "      <td>20.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1</td>\n",
       "      <td>28.08</td>\n",
       "      <td>16.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>1</td>\n",
       "      <td>29.33</td>\n",
       "      <td>20.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>0</td>\n",
       "      <td>24.54</td>\n",
       "      <td>7.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     diagnosis  texture_mean  radius_mean\n",
       "0            1         10.38        17.99\n",
       "1            1         17.77        20.57\n",
       "2            1         21.25        19.69\n",
       "3            1         20.38        11.42\n",
       "4            1         14.34        20.29\n",
       "..         ...           ...          ...\n",
       "564          1         22.39        21.56\n",
       "565          1         28.25        20.13\n",
       "566          1         28.08        16.60\n",
       "567          1         29.33        20.60\n",
       "568          0         24.54         7.76\n",
       "\n",
       "[569 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"bdiag.csv\")[[\"diagnosis\", \"texture_mean\", \"radius_mean\"]]\n",
    "df[\"diagnosis\"] = df[\"diagnosis\"].replace({\"M\":1, \"B\":0})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "0da2e048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "mal = df[df[\"diagnosis\"]==1]\n",
    "ben = df[df[\"diagnosis\"]==0]\n",
    "\n",
    "ax.scatter(mal[\"texture_mean\"], mal[\"radius_mean\"], label=\"malignant\", c=\"r\")\n",
    "ax.scatter(ben[\"texture_mean\"], ben[\"radius_mean\"], label=\"benign\", c=\"b\")\n",
    "ax.set_xlabel(\"texture_mean\")\n",
    "ax.set_ylabel(\"radius_mean\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b617ade",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b641944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"texture_mean\", \"radius_mean\"]].values\n",
    "y = df[\"diagnosis\"].values.reshape(-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b827b2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (455, 2)\n",
      "Y_train: (455, 1)\n",
      "X_test:  (114, 2)\n",
      "Y_test:  (114, 1)\n"
     ]
    }
   ],
   "source": [
    "print('X_train: ' + str(X_train.shape))\n",
    "print('Y_train: ' + str(y_train.shape))\n",
    "print('X_test:  '  + str(X_test.shape))\n",
    "print('Y_test:  '  + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca568a5",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbfafdc",
   "metadata": {},
   "source": [
    "predict only based on the mean of radius_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "02168e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8070175438596491"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rad_mean = X_test\n",
    "diag = y_test\n",
    "mean = rad_mean.mean()\n",
    "pred = np.where(rad_mean>mean, 1, 0)\n",
    "(pred==diag).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7056c1",
   "metadata": {},
   "source": [
    "### Feature normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c85e917",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = Normalization(axis=1)\n",
    "norm.adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d033f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(norm(X_train))  # converts back to numpy because tensor is slower\n",
    "X_test = np.array(norm(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63535d4f",
   "metadata": {},
   "source": [
    "### The Model overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cce2cc6",
   "metadata": {},
   "source": [
    "![alt text](model_nn_scratch.jpg \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1324519",
   "metadata": {},
   "source": [
    "## Doing it the \"easy\" way (using library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "95555dbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "15/15 [==============================] - 10s 8ms/step - loss: 0.7970 - accuracy: 0.5055\n",
      "Epoch 2/80\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.7055 - accuracy: 0.5824\n",
      "Epoch 3/80\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.6626 - accuracy: 0.6066\n",
      "Epoch 4/80\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.6311 - accuracy: 0.6176\n",
      "Epoch 5/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5961 - accuracy: 0.6154\n",
      "Epoch 6/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5547 - accuracy: 0.6176\n",
      "Epoch 7/80\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.5097 - accuracy: 0.6242\n",
      "Epoch 8/80\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4647 - accuracy: 0.6352\n",
      "Epoch 9/80\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4240 - accuracy: 0.6901\n",
      "Epoch 10/80\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.3872 - accuracy: 0.7692\n",
      "Epoch 11/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.3575 - accuracy: 0.8242\n",
      "Epoch 12/80\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.3316 - accuracy: 0.8593\n",
      "Epoch 13/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.3126 - accuracy: 0.8681\n",
      "Epoch 14/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2971 - accuracy: 0.8747\n",
      "Epoch 15/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2876 - accuracy: 0.8835\n",
      "Epoch 16/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2808 - accuracy: 0.8857\n",
      "Epoch 17/80\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2776 - accuracy: 0.8879\n",
      "Epoch 18/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2753 - accuracy: 0.8945\n",
      "Epoch 19/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2734 - accuracy: 0.8967\n",
      "Epoch 20/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2721 - accuracy: 0.8945\n",
      "Epoch 21/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2717 - accuracy: 0.8923\n",
      "Epoch 22/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2706 - accuracy: 0.8945\n",
      "Epoch 23/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2702 - accuracy: 0.8945\n",
      "Epoch 24/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2691 - accuracy: 0.8967\n",
      "Epoch 25/80\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2687 - accuracy: 0.8923\n",
      "Epoch 26/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2681 - accuracy: 0.8879\n",
      "Epoch 27/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2680 - accuracy: 0.8901\n",
      "Epoch 28/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2674 - accuracy: 0.8945\n",
      "Epoch 29/80\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2678 - accuracy: 0.8901\n",
      "Epoch 30/80\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2662 - accuracy: 0.8901\n",
      "Epoch 31/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2663 - accuracy: 0.8923\n",
      "Epoch 32/80\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2658 - accuracy: 0.8967\n",
      "Epoch 33/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2654 - accuracy: 0.8923\n",
      "Epoch 34/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2653 - accuracy: 0.8923\n",
      "Epoch 35/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2648 - accuracy: 0.8945\n",
      "Epoch 36/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2649 - accuracy: 0.8857\n",
      "Epoch 37/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2654 - accuracy: 0.8857\n",
      "Epoch 38/80\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2648 - accuracy: 0.8857\n",
      "Epoch 39/80\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2643 - accuracy: 0.8879\n",
      "Epoch 40/80\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2639 - accuracy: 0.8901\n",
      "Epoch 41/80\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2638 - accuracy: 0.8945\n",
      "Epoch 42/80\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2640 - accuracy: 0.8923\n",
      "Epoch 43/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2633 - accuracy: 0.8879\n",
      "Epoch 44/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2634 - accuracy: 0.8879\n",
      "Epoch 45/80\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2635 - accuracy: 0.8879\n",
      "Epoch 46/80\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2634 - accuracy: 0.8835\n",
      "Epoch 47/80\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2630 - accuracy: 0.8879\n",
      "Epoch 48/80\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2633 - accuracy: 0.8901\n",
      "Epoch 49/80\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2627 - accuracy: 0.8835\n",
      "Epoch 50/80\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2638 - accuracy: 0.8901\n",
      "Epoch 51/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2626 - accuracy: 0.8857\n",
      "Epoch 52/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2627 - accuracy: 0.8835\n",
      "Epoch 53/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2621 - accuracy: 0.8835\n",
      "Epoch 54/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2621 - accuracy: 0.8857\n",
      "Epoch 55/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2619 - accuracy: 0.8835\n",
      "Epoch 56/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2618 - accuracy: 0.8813\n",
      "Epoch 57/80\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2617 - accuracy: 0.8813\n",
      "Epoch 58/80\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2618 - accuracy: 0.8813\n",
      "Epoch 59/80\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2619 - accuracy: 0.8835\n",
      "Epoch 60/80\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2616 - accuracy: 0.8835\n",
      "Epoch 61/80\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2614 - accuracy: 0.8813\n",
      "Epoch 62/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2621 - accuracy: 0.8813\n",
      "Epoch 63/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2614 - accuracy: 0.8835\n",
      "Epoch 64/80\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2613 - accuracy: 0.8813\n",
      "Epoch 65/80\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2610 - accuracy: 0.8813\n",
      "Epoch 66/80\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.2616 - accuracy: 0.8791\n",
      "Epoch 67/80\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2615 - accuracy: 0.8813\n",
      "Epoch 68/80\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2610 - accuracy: 0.8813\n",
      "Epoch 69/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2610 - accuracy: 0.8813\n",
      "Epoch 70/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2612 - accuracy: 0.8791\n",
      "Epoch 71/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2613 - accuracy: 0.8835\n",
      "Epoch 72/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2617 - accuracy: 0.8879\n",
      "Epoch 73/80\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2609 - accuracy: 0.8835\n",
      "Epoch 74/80\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2606 - accuracy: 0.8791\n",
      "Epoch 75/80\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2612 - accuracy: 0.8791\n",
      "Epoch 76/80\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2614 - accuracy: 0.8835\n",
      "Epoch 77/80\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2608 - accuracy: 0.8813\n",
      "Epoch 78/80\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2608 - accuracy: 0.8813\n",
      "Epoch 79/80\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2609 - accuracy: 0.8813\n",
      "Epoch 80/80\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2606 - accuracy: 0.8813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f00a608f40>"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    \n",
    "    Dense(units=2, activation=relu),\n",
    "    Dense(units=1, activation=linear)\n",
    "])\n",
    "\n",
    "model.compile(loss=BinaryCrossentropy(from_logits=True),  # to avoid numerical instability\n",
    "              optimizer=Adam(learning_rate=0.01),\n",
    "              metrics=\"accuracy\"\n",
    "             )\n",
    "model.fit(X_train, y_train, epochs=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5610e0c6",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "4df90265",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 1s 2ms/step - loss: 0.2026 - accuracy: 0.9123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.20262475311756134, 0.9122806787490845]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf855c6c",
   "metadata": {},
   "source": [
    "# Doing it the hard way (scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce210e61",
   "metadata": {},
   "source": [
    "### The math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7843ec1c",
   "metadata": {},
   "source": [
    "2 features --> 1 hidden layer (2 neuron relu) --> 1 output layer (1 neuron sigmoid)\n",
    "\n",
    "#### Forward propagation:\n",
    "\n",
    "$$z_{1}^{[1]} = w_{1}^{[1]}x_{1} + w_{2}^{[1]}x_{2} + b_{1}^{[1]} $$ <br>\n",
    "$$z_{2}^{[1]} = w_{3}^{[1]}x_{1} + w_{4}^{[1]}x_{2} + b_{2}^{[1]} $$ <br>\n",
    "$$a_{1}^{[1]} = \\text{relu}(z_{1}^{[1]})$$ <br>\n",
    "$$a_{2}^{[1]} = \\text{relu}(z_{2}^{[1]})$$ <br>\n",
    "$$z^{[2]} = w_{1}^{[2]}x_{1} + w_{2}^{[2]}x_{2} + b^{[2]} $$ <br>\n",
    "$$a^{[2]} = \\frac{1}{1 + e^{-z^{[2]}}}$$ <br>\n",
    "\n",
    "#### cost & loss function: <br>\n",
    "$$\\text{cross entropy cost}(\\hat{a}^{[2]}, \\hat{y}) = -\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{j=1}^{m}y_{ij}\\text{log}(a^{[2]}[ij])$$ <br>\n",
    "$$\\text{binary cross entropy cost}(\\hat{a}^{[2]}, \\hat{y}) = -\\frac{1}{n}\\sum_{i=1}^{n}y_{i}\\text{log}(a^{[2]}[i]) + (1-y_{i})\\text{log}(1-a^{[2]}[i])$$ <br>\n",
    "\n",
    "#####  Derivative of cost function w.r.t parameters\n",
    "$$\\frac{\\partial ( \\text{binary cross entropy cost}(\\vec{a}^{[2]}, \\vec{y}))}{\\partial \\vec{w}} = \\frac{1}{n}(\\frac {\\partial (\\text{binary cross entropy loss}(a^{[2]}, \\hat{y}))_{1}}{\\partial \\vec{w}} + \\frac {\\partial (\\text{binary cross entropy loss}(a^{[2]}, \\hat{y}))_{2}}{\\partial \\vec{w}}+ ...+ \\frac {\\partial (\\text{binary cross entropy loss}(a^{[2]}, \\hat{y}))_{n}}{\\partial \\vec{w}})\\space \\text{for n=number of examples}$$<br>\n",
    "\n",
    "$$\\text{binary cross entropy loss}(a^{[2]}, \\hat{y}) = -(y_{i}\\text{log}(a^{[2]}) + (1-y_{i})\\text{log}(1-a^{[2]}))$$\n",
    "\n",
    "- The point of backpropagation is to determine the gradient of cost function wrt to each parameter $w$\n",
    "- Intuitively speaking, the gradient of cost function wrt to parameter $w$ is the average of all the gradient of loss function wrt to parameter $w_{1}, w_{2},..., w_{n}$ where $n=$number of examples.\n",
    "- Backpropagation involves the derivation of loss function wrt to each parameters, then averaging them later for gradient descent\n",
    "\n",
    "#### Backward propagation:\n",
    "\n",
    "##### output layer\n",
    "$\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z^{[2]}}=\\frac{\\partial a^{[2]}}{\\partial z^{[2]}}\\times\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial a^{[2]}} = (1-a^{[2]})\\times \\frac{a^{[2]}-y}{a^{[2]}(1-a^{[2]})} = a^{[2]} - y$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial w_{1}^{[2]}} = \\frac{\\partial z^{[2]}}{\\partial w_{1}^{[2]}}\\times\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z^{[2]}} = a_{1}^{[1]}\\times (a^{[2]} - y)$$\n",
    "\n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial w_{2}^{[2]}} = \\frac{\\partial z^{[2]}}{\\partial w_{2}^{[2]}}\\times\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z^{[2]}} = a_{2}^{[1]}\\times (a^{[2]} - y)$$\n",
    "\n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial b^{[2]}} = \\frac{\\partial z^{[2]}}{\\partial b^{[2]}}\\times\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z^{[2]}} = a^{[2]} - y$$\n",
    "\n",
    "\n",
    "##### hidden layer\n",
    "$\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z_{1}^{[1]}}=\\frac{\\partial a_{1}^{[1]}}{\\partial z_{1}^{[1]}}\\times\\frac{\\partial z^{[2]}}{\\partial a_{1}^{[1]}}\\times\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z^{[2]}} = \\text{relu'}(z_1^{[1]}) \\times w_1^{[2]}\\times (a^{[2]} - y)$ <br>\n",
    "\n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial w_{1}^{[1]}} = \\frac{\\partial z_{1}^{[1]}}{\\partial w_{1}^{[1]}}\\times \\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z_{1}^{[1]}} = x_1\\times \\text{relu'}(z_1^{[1]}) \\times w_1^{[2]}\\times (a^{[2]} - y)$$\n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial w_{2}^{[1]}} = \\frac{\\partial z_{1}^{[1]}}{\\partial w_{2}^{[1]}}\\times \\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z_{1}^{[1]}} = x_2\\times \\text{relu'}(z_1^{[1]}) \\times w_1^{[2]}\\times (a^{[2]} - y)$$ \n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial b_{1}^{[1]}} = \\frac{\\partial z_{1}^{[1]}}{\\partial b_{1}^{[1]}}\\times \\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z_{1}^{[1]}} = \\text{relu'}(z_1^{[1]}) \\times w_1^{[2]}\\times (a^{[2]} - y)$$ \n",
    "<br>\n",
    "\n",
    "$\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z_{2}^{[1]}}=\\frac{\\partial a_{1}^{[1]}}{\\partial z_{2}^{[1]}}\\times\\frac{\\partial z^{[2]}}{\\partial a_{1}^{[1]}}\\times\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z^{[2]}} = \\text{relu'}(z_2^{[1]}) \\times w_2^{[2]}\\times (a^{[2]} - y)$ <br>\n",
    "\n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial w_{3}^{[1]}} = \\frac{\\partial z_{2}^{[1]}}{\\partial w_{3}^{[1]}}\\times \\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z_{1}^{[1]}} = x_1\\times \\text{relu'}(z_2^{[1]}) \\times w_2^{[2]}\\times (a^{[2]} - y)$$\n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial w_{4}^{[1]}} = \\frac{\\partial z_{1}^{[1]}}{\\partial w_{4}^{[1]}}\\times \\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z_{1}^{[1]}} = x_2\\times \\text{relu'}(z_1^{[1]}) \\times w_1^{[2]}\\times (a^{[2]} - y)$$ \n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial b_{2}^{[1]}} = \\frac{\\partial z_{1}^{[1]}}{\\partial b_{2}^{[1]}}\\times \\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z_{1}^{[1]}} = \\text{relu'}(z_1^{[1]}) \\times w_1^{[2]}\\times (a^{[2]} - y)$$ \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f41c1db",
   "metadata": {},
   "source": [
    "### No vectorization (just for loop), vanilla gradient descent, and non-modular implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7782d8bb",
   "metadata": {},
   "source": [
    "This implementation is tailored for learning, hence not for production usage. It is meant to be explicit and not use any vectorization. It is also non-modular, as it can only be of the specified architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "9564fd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    \n",
    "    def initialize(self):\n",
    "        self.w11, self.w12, self.w13, self.w14, self.w21, self.w22, self.b11, self.b12, self.b2 = np.random.randn(9)\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def binary_cross_entropy(self, x, y_true):\n",
    "        return -(y_true * np.log(x) + (1 - y_true) * np.log(1 - x))\n",
    "    \n",
    "    def derivative_relu(self, x):\n",
    "        return 1 if x>0 else 0\n",
    "    \n",
    "    def forward_prop(self, x1, x2):\n",
    "        forward_dict = {} \n",
    "        \n",
    "        # hidden layer\n",
    "        forward_dict[\"z11\"] = self.w11 * x1 + self.w12 * x2 + self.b11\n",
    "        forward_dict[\"z12\"] = self.w13 * x1 + self.w14 * x2 + self.b12\n",
    "        forward_dict[\"a11\"] = self.relu(forward_dict[\"z11\"])\n",
    "        forward_dict[\"a12\"] = self.relu(forward_dict[\"z12\"])\n",
    "        \n",
    "        # output layer\n",
    "        forward_dict[\"z2\"] = self.w21 * forward_dict[\"a11\"] + self.w22 * forward_dict[\"a12\"] + self.b2\n",
    "        forward_dict[\"a2\"] = self.sigmoid(forward_dict[\"z2\"])\n",
    "        \n",
    "        return forward_dict\n",
    "\n",
    "    def back_prop(self, x1, x2, y_true, forward_dict):\n",
    "        # gradient will be calculated for each training example and averaged later#\n",
    "        \n",
    "        deriva_dict = {}\n",
    "        \n",
    "        # output layer\n",
    "        error = forward_dict[\"a2\"] - y_true\n",
    "        \n",
    "        deriva_dict[\"dloss_dw21\"] = forward_dict[\"a11\"] * error\n",
    "        deriva_dict[\"dloss_dw22\"] = forward_dict[\"a12\"] * error\n",
    "        deriva_dict[\"dloss_db2\"] = error\n",
    "        \n",
    "        # hidden layer\n",
    "        dcost_dz11 = self.derivative_relu(forward_dict[\"z11\"]) * self.w21 * error\n",
    "        dcost_dz12 = self.derivative_relu(forward_dict[\"z12\"]) * self.w22 * error\n",
    "        \n",
    "        deriva_dict[\"dloss_dw11\"] = x1 * dcost_dz11\n",
    "        deriva_dict[\"dloss_dw12\"] = x2 * dcost_dz11\n",
    "        deriva_dict[\"dloss_db11\"] = dcost_dz11\n",
    "        \n",
    "        deriva_dict[\"dloss_dw13\"] = x1 * dcost_dz12\n",
    "        deriva_dict[\"dloss_dw14\"] = x2 * dcost_dz12\n",
    "        deriva_dict[\"dloss_db12\"] = dcost_dz12\n",
    "        \n",
    "        return deriva_dict\n",
    "    \n",
    "    def get_prob_class(self, x1, x2):\n",
    "        prob = self.forward_prop(x1, x2)[\"a2\"]\n",
    "        class_ = self.encode_class(prob)\n",
    "        return prob, class_\n",
    "    \n",
    "    def encode_class(self, x):\n",
    "        return 1 if x>0.5 else 0\n",
    "    \n",
    "    def get_accuracy_cost(self, X, y):\n",
    "        n = X.shape[0]\n",
    "        n_correct = 0\n",
    "        cost = 0\n",
    "        for i in range(n):\n",
    "            x1, x2 = X[i]\n",
    "            prediction = self.get_prob_class(x1, x2)\n",
    "            if prediction[1] == y[i][0]:\n",
    "                n_correct += 1\n",
    "            cost += self.binary_cross_entropy(prediction[0], y[i][0])\n",
    "        return n_correct/n, cost/n\n",
    "\n",
    "    def fit(self, X, y, alpha, epochs):\n",
    "        n, m = X.shape\n",
    "        \n",
    "        self.initialize()\n",
    "        costs = []\n",
    "\n",
    "        for i in range(1, epochs+1):\n",
    "            dcost_dw21 = 0\n",
    "            dcost_dw22 = 0\n",
    "            dcost_db2 = 0\n",
    "            dcost_dw11 = 0\n",
    "            dcost_dw12 = 0\n",
    "            dcost_db11 = 0\n",
    "            dcost_dw13 = 0\n",
    "            dcost_dw14 = 0\n",
    "            dcost_db12 = 0\n",
    "            \n",
    "            for j in range(n):\n",
    "                x1 , x2 = X[j]\n",
    "                y_true = y[j][0]\n",
    "                forward_prop = self.forward_prop(x1, x2)\n",
    "                back_prop = self.back_prop(x1, x2, y_true, forward_prop)\n",
    "                \n",
    "                dcost_dw21 += back_prop[\"dloss_dw21\"]\n",
    "                dcost_dw22 += back_prop[\"dloss_dw22\"]\n",
    "                dcost_db2 += back_prop[\"dloss_db2\"]\n",
    "                dcost_dw11 += back_prop[\"dloss_dw11\"]\n",
    "                dcost_dw12 += back_prop[\"dloss_dw12\"]\n",
    "                dcost_db11 += back_prop[\"dloss_db11\"]\n",
    "                dcost_dw13 += back_prop[\"dloss_dw13\"]\n",
    "                dcost_dw14 += back_prop[\"dloss_dw14\"]\n",
    "                dcost_db12 += back_prop[\"dloss_db12\"]\n",
    "                    \n",
    "            self.w21 -= alpha*(1/n)*dcost_dw21 \n",
    "            self.w22 -= alpha*(1/n)*dcost_dw22 \n",
    "            self.b2  -= alpha*(1/n)*dcost_db2\n",
    "            self.w11 -= alpha*(1/n)*dcost_dw11 \n",
    "            self.w12 -= alpha*(1/n)*dcost_dw12 \n",
    "            self.b11 -= alpha*(1/n)*dcost_db11\n",
    "            self.w13 -= alpha*(1/n)*dcost_dw13\n",
    "            self.w14 -= alpha*(1/n)*dcost_dw14\n",
    "            self.b12 -= alpha*(1/n)*dcost_db12\n",
    "            \n",
    "            accuracy, cost = self.get_accuracy_cost(X, y)\n",
    "            costs.append(cost)\n",
    "            \n",
    "            print(f\"epoch: {i}, cost: {cost:.4f}, accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d3e8e3",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "e3e3aaaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, cost: 0.8378, accuracy: 61.76%\n",
      "epoch: 2, cost: 0.8057, accuracy: 61.76%\n",
      "epoch: 3, cost: 0.7775, accuracy: 61.76%\n",
      "epoch: 4, cost: 0.7525, accuracy: 61.76%\n",
      "epoch: 5, cost: 0.7306, accuracy: 61.76%\n",
      "epoch: 6, cost: 0.7115, accuracy: 62.20%\n",
      "epoch: 7, cost: 0.6950, accuracy: 62.20%\n",
      "epoch: 8, cost: 0.6807, accuracy: 62.20%\n",
      "epoch: 9, cost: 0.6684, accuracy: 62.42%\n",
      "epoch: 10, cost: 0.6578, accuracy: 63.08%\n",
      "epoch: 11, cost: 0.6485, accuracy: 63.74%\n",
      "epoch: 12, cost: 0.6405, accuracy: 63.74%\n",
      "epoch: 13, cost: 0.6332, accuracy: 63.96%\n",
      "epoch: 14, cost: 0.6268, accuracy: 63.52%\n",
      "epoch: 15, cost: 0.6211, accuracy: 63.74%\n",
      "epoch: 16, cost: 0.6160, accuracy: 63.96%\n",
      "epoch: 17, cost: 0.6113, accuracy: 63.74%\n",
      "epoch: 18, cost: 0.6071, accuracy: 63.52%\n",
      "epoch: 19, cost: 0.6031, accuracy: 63.30%\n",
      "epoch: 20, cost: 0.5993, accuracy: 64.40%\n",
      "epoch: 21, cost: 0.5957, accuracy: 64.62%\n",
      "epoch: 22, cost: 0.5924, accuracy: 65.05%\n",
      "epoch: 23, cost: 0.5892, accuracy: 65.49%\n",
      "epoch: 24, cost: 0.5858, accuracy: 65.93%\n",
      "epoch: 25, cost: 0.5826, accuracy: 66.15%\n",
      "epoch: 26, cost: 0.5793, accuracy: 67.03%\n",
      "epoch: 27, cost: 0.5762, accuracy: 67.69%\n",
      "epoch: 28, cost: 0.5732, accuracy: 67.47%\n",
      "epoch: 29, cost: 0.5702, accuracy: 68.35%\n",
      "epoch: 30, cost: 0.5673, accuracy: 68.57%\n",
      "epoch: 31, cost: 0.5644, accuracy: 68.79%\n",
      "epoch: 32, cost: 0.5615, accuracy: 69.45%\n",
      "epoch: 33, cost: 0.5586, accuracy: 69.89%\n",
      "epoch: 34, cost: 0.5556, accuracy: 70.33%\n",
      "epoch: 35, cost: 0.5527, accuracy: 70.99%\n",
      "epoch: 36, cost: 0.5498, accuracy: 71.21%\n",
      "epoch: 37, cost: 0.5469, accuracy: 72.31%\n",
      "epoch: 38, cost: 0.5441, accuracy: 72.09%\n",
      "epoch: 39, cost: 0.5412, accuracy: 72.97%\n",
      "epoch: 40, cost: 0.5384, accuracy: 73.19%\n",
      "epoch: 41, cost: 0.5358, accuracy: 73.19%\n",
      "epoch: 42, cost: 0.5332, accuracy: 73.41%\n",
      "epoch: 43, cost: 0.5306, accuracy: 73.63%\n",
      "epoch: 44, cost: 0.5280, accuracy: 74.07%\n",
      "epoch: 45, cost: 0.5255, accuracy: 74.07%\n",
      "epoch: 46, cost: 0.5230, accuracy: 74.51%\n",
      "epoch: 47, cost: 0.5205, accuracy: 74.51%\n",
      "epoch: 48, cost: 0.5180, accuracy: 75.16%\n",
      "epoch: 49, cost: 0.5155, accuracy: 75.60%\n",
      "epoch: 50, cost: 0.5130, accuracy: 75.60%\n",
      "epoch: 51, cost: 0.5105, accuracy: 75.82%\n",
      "epoch: 52, cost: 0.5082, accuracy: 76.04%\n",
      "epoch: 53, cost: 0.5058, accuracy: 76.26%\n",
      "epoch: 54, cost: 0.5036, accuracy: 76.48%\n",
      "epoch: 55, cost: 0.5013, accuracy: 76.70%\n",
      "epoch: 56, cost: 0.4990, accuracy: 76.92%\n",
      "epoch: 57, cost: 0.4966, accuracy: 76.92%\n",
      "epoch: 58, cost: 0.4942, accuracy: 76.92%\n",
      "epoch: 59, cost: 0.4918, accuracy: 76.92%\n",
      "epoch: 60, cost: 0.4894, accuracy: 77.36%\n",
      "epoch: 61, cost: 0.4870, accuracy: 78.02%\n",
      "epoch: 62, cost: 0.4846, accuracy: 78.24%\n",
      "epoch: 63, cost: 0.4821, accuracy: 78.24%\n",
      "epoch: 64, cost: 0.4795, accuracy: 78.46%\n",
      "epoch: 65, cost: 0.4770, accuracy: 79.12%\n",
      "epoch: 66, cost: 0.4745, accuracy: 79.12%\n",
      "epoch: 67, cost: 0.4721, accuracy: 79.12%\n",
      "epoch: 68, cost: 0.4697, accuracy: 79.12%\n",
      "epoch: 69, cost: 0.4674, accuracy: 79.34%\n",
      "epoch: 70, cost: 0.4651, accuracy: 79.34%\n",
      "epoch: 71, cost: 0.4629, accuracy: 79.34%\n",
      "epoch: 72, cost: 0.4606, accuracy: 79.56%\n",
      "epoch: 73, cost: 0.4584, accuracy: 79.78%\n",
      "epoch: 74, cost: 0.4562, accuracy: 80.00%\n",
      "epoch: 75, cost: 0.4541, accuracy: 80.22%\n",
      "epoch: 76, cost: 0.4519, accuracy: 80.22%\n",
      "epoch: 77, cost: 0.4497, accuracy: 80.44%\n",
      "epoch: 78, cost: 0.4476, accuracy: 80.66%\n",
      "epoch: 79, cost: 0.4453, accuracy: 80.66%\n",
      "epoch: 80, cost: 0.4430, accuracy: 80.66%\n",
      "epoch: 81, cost: 0.4408, accuracy: 80.88%\n",
      "epoch: 82, cost: 0.4386, accuracy: 80.88%\n",
      "epoch: 83, cost: 0.4364, accuracy: 80.88%\n",
      "epoch: 84, cost: 0.4344, accuracy: 80.66%\n",
      "epoch: 85, cost: 0.4324, accuracy: 80.66%\n",
      "epoch: 86, cost: 0.4304, accuracy: 80.88%\n",
      "epoch: 87, cost: 0.4285, accuracy: 81.10%\n",
      "epoch: 88, cost: 0.4266, accuracy: 81.32%\n",
      "epoch: 89, cost: 0.4247, accuracy: 81.76%\n",
      "epoch: 90, cost: 0.4228, accuracy: 82.20%\n",
      "epoch: 91, cost: 0.4210, accuracy: 82.20%\n",
      "epoch: 92, cost: 0.4193, accuracy: 82.20%\n",
      "epoch: 93, cost: 0.4175, accuracy: 82.20%\n",
      "epoch: 94, cost: 0.4158, accuracy: 82.42%\n",
      "epoch: 95, cost: 0.4141, accuracy: 82.42%\n",
      "epoch: 96, cost: 0.4124, accuracy: 82.20%\n",
      "epoch: 97, cost: 0.4107, accuracy: 81.98%\n",
      "epoch: 98, cost: 0.4090, accuracy: 81.98%\n",
      "epoch: 99, cost: 0.4074, accuracy: 82.20%\n",
      "epoch: 100, cost: 0.4058, accuracy: 82.20%\n",
      "epoch: 101, cost: 0.4043, accuracy: 82.20%\n",
      "epoch: 102, cost: 0.4027, accuracy: 82.20%\n",
      "epoch: 103, cost: 0.4012, accuracy: 82.42%\n",
      "epoch: 104, cost: 0.3997, accuracy: 82.42%\n",
      "epoch: 105, cost: 0.3982, accuracy: 82.42%\n",
      "epoch: 106, cost: 0.3967, accuracy: 82.42%\n",
      "epoch: 107, cost: 0.3953, accuracy: 82.42%\n",
      "epoch: 108, cost: 0.3938, accuracy: 82.64%\n",
      "epoch: 109, cost: 0.3924, accuracy: 82.64%\n",
      "epoch: 110, cost: 0.3910, accuracy: 82.64%\n",
      "epoch: 111, cost: 0.3896, accuracy: 82.86%\n",
      "epoch: 112, cost: 0.3883, accuracy: 83.08%\n",
      "epoch: 113, cost: 0.3869, accuracy: 83.30%\n",
      "epoch: 114, cost: 0.3855, accuracy: 83.30%\n",
      "epoch: 115, cost: 0.3841, accuracy: 83.30%\n",
      "epoch: 116, cost: 0.3828, accuracy: 83.30%\n",
      "epoch: 117, cost: 0.3814, accuracy: 83.30%\n",
      "epoch: 118, cost: 0.3800, accuracy: 83.30%\n",
      "epoch: 119, cost: 0.3787, accuracy: 83.74%\n",
      "epoch: 120, cost: 0.3773, accuracy: 83.74%\n",
      "epoch: 121, cost: 0.3760, accuracy: 83.74%\n",
      "epoch: 122, cost: 0.3747, accuracy: 83.96%\n",
      "epoch: 123, cost: 0.3735, accuracy: 84.18%\n",
      "epoch: 124, cost: 0.3723, accuracy: 84.40%\n",
      "epoch: 125, cost: 0.3711, accuracy: 84.40%\n",
      "epoch: 126, cost: 0.3699, accuracy: 84.40%\n",
      "epoch: 127, cost: 0.3688, accuracy: 84.40%\n",
      "epoch: 128, cost: 0.3676, accuracy: 84.62%\n",
      "epoch: 129, cost: 0.3665, accuracy: 84.84%\n",
      "epoch: 130, cost: 0.3654, accuracy: 84.84%\n",
      "epoch: 131, cost: 0.3643, accuracy: 84.84%\n",
      "epoch: 132, cost: 0.3632, accuracy: 84.84%\n",
      "epoch: 133, cost: 0.3621, accuracy: 84.62%\n",
      "epoch: 134, cost: 0.3610, accuracy: 84.62%\n",
      "epoch: 135, cost: 0.3599, accuracy: 84.40%\n",
      "epoch: 136, cost: 0.3589, accuracy: 84.40%\n",
      "epoch: 137, cost: 0.3578, accuracy: 84.84%\n",
      "epoch: 138, cost: 0.3568, accuracy: 85.05%\n",
      "epoch: 139, cost: 0.3557, accuracy: 85.05%\n",
      "epoch: 140, cost: 0.3546, accuracy: 85.27%\n",
      "epoch: 141, cost: 0.3536, accuracy: 85.27%\n",
      "epoch: 142, cost: 0.3525, accuracy: 85.05%\n",
      "epoch: 143, cost: 0.3515, accuracy: 85.05%\n",
      "epoch: 144, cost: 0.3505, accuracy: 85.27%\n",
      "epoch: 145, cost: 0.3496, accuracy: 85.27%\n",
      "epoch: 146, cost: 0.3486, accuracy: 85.27%\n",
      "epoch: 147, cost: 0.3476, accuracy: 85.27%\n",
      "epoch: 148, cost: 0.3467, accuracy: 85.49%\n",
      "epoch: 149, cost: 0.3458, accuracy: 85.49%\n",
      "epoch: 150, cost: 0.3449, accuracy: 85.49%\n",
      "epoch: 151, cost: 0.3440, accuracy: 85.49%\n",
      "epoch: 152, cost: 0.3431, accuracy: 85.49%\n",
      "epoch: 153, cost: 0.3423, accuracy: 85.49%\n",
      "epoch: 154, cost: 0.3414, accuracy: 85.49%\n",
      "epoch: 155, cost: 0.3406, accuracy: 85.49%\n",
      "epoch: 156, cost: 0.3397, accuracy: 85.49%\n",
      "epoch: 157, cost: 0.3389, accuracy: 85.71%\n",
      "epoch: 158, cost: 0.3381, accuracy: 85.93%\n",
      "epoch: 159, cost: 0.3374, accuracy: 85.93%\n",
      "epoch: 160, cost: 0.3366, accuracy: 85.71%\n",
      "epoch: 161, cost: 0.3358, accuracy: 85.71%\n",
      "epoch: 162, cost: 0.3350, accuracy: 85.93%\n",
      "epoch: 163, cost: 0.3342, accuracy: 85.93%\n",
      "epoch: 164, cost: 0.3334, accuracy: 85.93%\n",
      "epoch: 165, cost: 0.3327, accuracy: 86.15%\n",
      "epoch: 166, cost: 0.3319, accuracy: 86.37%\n",
      "epoch: 167, cost: 0.3312, accuracy: 86.37%\n",
      "epoch: 168, cost: 0.3305, accuracy: 86.15%\n",
      "epoch: 169, cost: 0.3298, accuracy: 86.15%\n",
      "epoch: 170, cost: 0.3291, accuracy: 85.93%\n",
      "epoch: 171, cost: 0.3284, accuracy: 85.93%\n",
      "epoch: 172, cost: 0.3277, accuracy: 85.93%\n",
      "epoch: 173, cost: 0.3271, accuracy: 85.93%\n",
      "epoch: 174, cost: 0.3264, accuracy: 85.93%\n",
      "epoch: 175, cost: 0.3257, accuracy: 85.71%\n",
      "epoch: 176, cost: 0.3251, accuracy: 85.71%\n",
      "epoch: 177, cost: 0.3245, accuracy: 85.71%\n",
      "epoch: 178, cost: 0.3238, accuracy: 85.93%\n",
      "epoch: 179, cost: 0.3232, accuracy: 85.93%\n",
      "epoch: 180, cost: 0.3225, accuracy: 85.93%\n",
      "epoch: 181, cost: 0.3219, accuracy: 85.93%\n",
      "epoch: 182, cost: 0.3213, accuracy: 86.15%\n",
      "epoch: 183, cost: 0.3207, accuracy: 86.15%\n",
      "epoch: 184, cost: 0.3201, accuracy: 86.15%\n",
      "epoch: 185, cost: 0.3195, accuracy: 86.15%\n",
      "epoch: 186, cost: 0.3189, accuracy: 86.15%\n",
      "epoch: 187, cost: 0.3184, accuracy: 86.37%\n",
      "epoch: 188, cost: 0.3178, accuracy: 86.59%\n",
      "epoch: 189, cost: 0.3172, accuracy: 86.59%\n",
      "epoch: 190, cost: 0.3167, accuracy: 86.59%\n",
      "epoch: 191, cost: 0.3162, accuracy: 86.59%\n",
      "epoch: 192, cost: 0.3157, accuracy: 86.59%\n",
      "epoch: 193, cost: 0.3151, accuracy: 86.81%\n",
      "epoch: 194, cost: 0.3146, accuracy: 86.81%\n",
      "epoch: 195, cost: 0.3141, accuracy: 87.03%\n",
      "epoch: 196, cost: 0.3136, accuracy: 87.25%\n",
      "epoch: 197, cost: 0.3132, accuracy: 87.47%\n",
      "epoch: 198, cost: 0.3127, accuracy: 87.47%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 199, cost: 0.3122, accuracy: 87.47%\n",
      "epoch: 200, cost: 0.3117, accuracy: 87.47%\n"
     ]
    }
   ],
   "source": [
    "classifier = NeuralNetwork()\n",
    "classifier.fit(X_train, y_train, 0.1, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598580c2",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "c8a6dd82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9035087719298246, 0.2630215709733145)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.get_accuracy_cost(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe28c3c",
   "metadata": {},
   "source": [
    "## Vanilla gradient descent vs mini batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c0b02f",
   "metadata": {},
   "source": [
    "#### The following is the same neural network with the same architecture but with mini batch gradient descent instead of vanilla gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "65243079",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork_batch():\n",
    "    \n",
    "    def initialize(self):\n",
    "        self.w11, self.w12, self.w13, self.w14, self.w21, self.w22, self.b11, self.b12, self.b2 = np.random.randn(9)\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def binary_cross_entropy(self, x, y_true):\n",
    "        return -(y_true * np.log(x) + (1 - y_true) * np.log(1 - x))\n",
    "    \n",
    "    def derivative_relu(self, x):\n",
    "        return 1 if x>0 else 0\n",
    "    \n",
    "    def forward_prop(self, x1, x2):\n",
    "        forward_dict = {} \n",
    "        \n",
    "        # hidden layer\n",
    "        forward_dict[\"z11\"] = self.w11 * x1 + self.w12 * x2 + self.b11\n",
    "        forward_dict[\"z12\"] = self.w13 * x1 + self.w14 * x2 + self.b12\n",
    "        forward_dict[\"a11\"] = self.relu(forward_dict[\"z11\"])\n",
    "        forward_dict[\"a12\"] = self.relu(forward_dict[\"z12\"])\n",
    "        \n",
    "        # output layer\n",
    "        forward_dict[\"z2\"] = self.w21 * forward_dict[\"a11\"] + self.w22 * forward_dict[\"a12\"] + self.b2\n",
    "        forward_dict[\"a2\"] = self.sigmoid(forward_dict[\"z2\"])\n",
    "        \n",
    "        return forward_dict\n",
    "\n",
    "    def back_prop(self, x1, x2, y_true, forward_dict):\n",
    "        # gradient will be calculated for each training example and averaged later#\n",
    "        \n",
    "        deriva_dict = {}\n",
    "        \n",
    "        # output layer\n",
    "        error = forward_dict[\"a2\"] - y_true\n",
    "        \n",
    "        deriva_dict[\"dloss_dw21\"] = forward_dict[\"a11\"] * error\n",
    "        deriva_dict[\"dloss_dw22\"] = forward_dict[\"a12\"] * error\n",
    "        deriva_dict[\"dloss_db2\"] = error\n",
    "        \n",
    "        # hidden layer\n",
    "        dcost_dz11 = self.derivative_relu(forward_dict[\"z11\"]) * self.w21 * error\n",
    "        dcost_dz12 = self.derivative_relu(forward_dict[\"z12\"]) * self.w22 * error\n",
    "        \n",
    "        deriva_dict[\"dloss_dw11\"] = x1 * dcost_dz11\n",
    "        deriva_dict[\"dloss_dw12\"] = x2 * dcost_dz11\n",
    "        deriva_dict[\"dloss_db11\"] = dcost_dz11\n",
    "        \n",
    "        deriva_dict[\"dloss_dw13\"] = x1 * dcost_dz12\n",
    "        deriva_dict[\"dloss_dw14\"] = x2 * dcost_dz12\n",
    "        deriva_dict[\"dloss_db12\"] = dcost_dz12\n",
    "        \n",
    "        return deriva_dict\n",
    "    \n",
    "    def get_prob_class(self, x1, x2):\n",
    "        prob = self.forward_prop(x1, x2)[\"a2\"]\n",
    "        class_ = self.encode_class(prob)\n",
    "        return prob, class_\n",
    "    \n",
    "    def encode_class(self, x):\n",
    "        return 1 if x>0.5 else 0\n",
    "    \n",
    "    def get_accuracy_cost(self, X, y):\n",
    "        n = X.shape[0]\n",
    "        n_correct = 0\n",
    "        cost_total = 0\n",
    "        for i in range(n):\n",
    "            x1, x2 = X[i]\n",
    "            prediction = self.get_prob_class(x1, x2)\n",
    "            if prediction[1] == y[i][0]:\n",
    "                n_correct += 1\n",
    "            cost_total += self.binary_cross_entropy(prediction[0], y[i][0])\n",
    "        return n_correct/n, cost_total/n\n",
    "\n",
    "    def fit(self, X, y, alpha, epochs, batch_size=32):\n",
    "        n, m = X.shape\n",
    "        combined = np.hstack([X, y])\n",
    "        self.initialize()\n",
    "        n_iter_batch = np.ceil(n/batch_size)  # Number of training iteration for one epoch\n",
    "        \n",
    "\n",
    "        for i in range(1, epochs+1):\n",
    "            dcost_dw21 = 0\n",
    "            dcost_dw22 = 0\n",
    "            dcost_db2 = 0\n",
    "            dcost_dw11 = 0\n",
    "            dcost_dw12 = 0\n",
    "            dcost_db11 = 0\n",
    "            dcost_dw13 = 0\n",
    "            dcost_dw14 = 0\n",
    "            dcost_db12 = 0\n",
    "            \n",
    "            #Shuffled the dataset before splitting\n",
    "            combined_shuffled = np.random.permutation(combined)\n",
    "            X_shuffled = combined_shuffled[:, 0:-1]\n",
    "            y_shuffled = combined_shuffled[:, -1].reshape(-1, 1)\n",
    "            \n",
    "            costs_batch = 0\n",
    "            accuracies_batch = 0\n",
    "            for k in tqdm(range(0, n, batch_size), position=0):\n",
    "                X_batch = X_shuffled[k: k+batch_size]\n",
    "                y_batch = y_shuffled[k: k+batch_size]\n",
    "                n_batch = X_batch.shape[0]\n",
    "            \n",
    "                for j in range(n_batch):\n",
    "                    x1 , x2 = X_batch[j]\n",
    "                    y_true = y_batch[j][0]\n",
    "                    forward_prop = self.forward_prop(x1, x2)\n",
    "                    back_prop = self.back_prop(x1, x2, y_true, forward_prop)\n",
    "\n",
    "                    dcost_dw21 += back_prop[\"dloss_dw21\"]\n",
    "                    dcost_dw22 += back_prop[\"dloss_dw22\"]\n",
    "                    dcost_db2 += back_prop[\"dloss_db2\"]\n",
    "                    dcost_dw11 += back_prop[\"dloss_dw11\"]\n",
    "                    dcost_dw12 += back_prop[\"dloss_dw12\"]\n",
    "                    dcost_db11 += back_prop[\"dloss_db11\"]\n",
    "                    dcost_dw13 += back_prop[\"dloss_dw13\"]\n",
    "                    dcost_dw14 += back_prop[\"dloss_dw14\"]\n",
    "                    dcost_db12 += back_prop[\"dloss_db12\"]\n",
    "\n",
    "                self.w21 -= alpha*(1/n)*dcost_dw21 \n",
    "                self.w22 -= alpha*(1/n)*dcost_dw22 \n",
    "                self.b2  -= alpha*(1/n)*dcost_db2\n",
    "                self.w11 -= alpha*(1/n)*dcost_dw11 \n",
    "                self.w12 -= alpha*(1/n)*dcost_dw12 \n",
    "                self.b11 -= alpha*(1/n)*dcost_db11\n",
    "                self.w13 -= alpha*(1/n)*dcost_dw13\n",
    "                self.w14 -= alpha*(1/n)*dcost_dw14\n",
    "                self.b12 -= alpha*(1/n)*dcost_db12\n",
    "\n",
    "                accuracy, cost = self.get_accuracy_cost(X_batch, y_batch)\n",
    "                costs_batch += cost\n",
    "                accuracies_batch += accuracy\n",
    "            \n",
    "            # Reported cost and accuracy are averaged across the mini batch\n",
    "            # of a given epoch\n",
    "            epoch_cost = costs_batch/n_iter_batch\n",
    "            epoch_accuracy = accuracies_batch/n_iter_batch\n",
    "            print(f\"epoch: {i}, cost: {epoch_cost:.4f}, accuracy: {epoch_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672481ba",
   "metadata": {},
   "source": [
    "### Training using small epoch (30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc64ef6",
   "metadata": {},
   "source": [
    "#### With mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "3d365253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 384.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, cost: 0.4840, accuracy: 0.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 454.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, cost: 0.4344, accuracy: 0.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 283.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, cost: 0.4079, accuracy: 0.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 348.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, cost: 0.3934, accuracy: 0.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 365.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, cost: 0.3846, accuracy: 0.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 382.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, cost: 0.3639, accuracy: 0.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 357.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, cost: 0.3647, accuracy: 0.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 288.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8, cost: 0.3415, accuracy: 0.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 348.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9, cost: 0.3344, accuracy: 0.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 405.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, cost: 0.3290, accuracy: 0.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 428.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11, cost: 0.3228, accuracy: 0.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 348.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12, cost: 0.3140, accuracy: 0.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 250.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13, cost: 0.3157, accuracy: 0.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 340.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14, cost: 0.3107, accuracy: 0.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 365.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15, cost: 0.3099, accuracy: 0.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 340.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16, cost: 0.3113, accuracy: 0.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 288.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17, cost: 0.3011, accuracy: 0.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 197.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18, cost: 0.3085, accuracy: 0.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 348.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19, cost: 0.3044, accuracy: 0.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 357.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, cost: 0.3080, accuracy: 0.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 348.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21, cost: 0.2915, accuracy: 0.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 241.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22, cost: 0.3064, accuracy: 0.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 132.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23, cost: 0.2956, accuracy: 0.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 319.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24, cost: 0.2895, accuracy: 0.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 245.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25, cost: 0.2842, accuracy: 0.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 120.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26, cost: 0.2990, accuracy: 0.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 340.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27, cost: 0.2854, accuracy: 0.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 288.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28, cost: 0.2989, accuracy: 0.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 283.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29, cost: 0.2756, accuracy: 0.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 375.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30, cost: 0.2810, accuracy: 0.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "classifier_batch = NeuralNetwork_batch()\n",
    "classifier_batch.fit(X_train, y_train, 0.1, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89f97e3",
   "metadata": {},
   "source": [
    "#### Without mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "15e1457e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, cost: 3.2572, accuracy: 38.24%\n",
      "epoch: 2, cost: 2.7787, accuracy: 38.24%\n",
      "epoch: 3, cost: 2.3645, accuracy: 38.24%\n",
      "epoch: 4, cost: 2.0116, accuracy: 38.24%\n",
      "epoch: 5, cost: 1.7123, accuracy: 38.24%\n",
      "epoch: 6, cost: 1.4607, accuracy: 38.24%\n",
      "epoch: 7, cost: 1.2541, accuracy: 38.24%\n",
      "epoch: 8, cost: 1.0876, accuracy: 38.24%\n",
      "epoch: 9, cost: 0.9567, accuracy: 38.24%\n",
      "epoch: 10, cost: 0.8560, accuracy: 38.24%\n",
      "epoch: 11, cost: 0.7782, accuracy: 38.24%\n",
      "epoch: 12, cost: 0.7157, accuracy: 38.24%\n",
      "epoch: 13, cost: 0.6661, accuracy: 40.44%\n",
      "epoch: 14, cost: 0.6281, accuracy: 56.92%\n",
      "epoch: 15, cost: 0.5987, accuracy: 65.05%\n",
      "epoch: 16, cost: 0.5755, accuracy: 69.67%\n",
      "epoch: 17, cost: 0.5569, accuracy: 72.75%\n",
      "epoch: 18, cost: 0.5416, accuracy: 74.95%\n",
      "epoch: 19, cost: 0.5290, accuracy: 79.12%\n",
      "epoch: 20, cost: 0.5184, accuracy: 79.78%\n",
      "epoch: 21, cost: 0.5093, accuracy: 81.10%\n",
      "epoch: 22, cost: 0.5013, accuracy: 81.76%\n",
      "epoch: 23, cost: 0.4944, accuracy: 83.52%\n",
      "epoch: 24, cost: 0.4881, accuracy: 85.27%\n",
      "epoch: 25, cost: 0.4824, accuracy: 85.27%\n",
      "epoch: 26, cost: 0.4771, accuracy: 85.93%\n",
      "epoch: 27, cost: 0.4723, accuracy: 85.93%\n",
      "epoch: 28, cost: 0.4678, accuracy: 86.37%\n",
      "epoch: 29, cost: 0.4636, accuracy: 86.59%\n",
      "epoch: 30, cost: 0.4596, accuracy: 86.81%\n"
     ]
    }
   ],
   "source": [
    "classifier = NeuralNetwork()\n",
    "classifier.fit(X_train, y_train, 0.1, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93f3701",
   "metadata": {},
   "source": [
    "### Evaluation & Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd367ec",
   "metadata": {},
   "source": [
    "#### With mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "692f6920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9035087719298246, 0.22738679940705572)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_batch.get_accuracy_cost(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81337c3",
   "metadata": {},
   "source": [
    "#### Without mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "7b9d9374",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8771929824561403, 0.4361894183611597)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.get_accuracy_cost(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebcf361",
   "metadata": {},
   "source": [
    "It appears that, in the long run, mini batch gradient descent can achieve better metrics using less epoch, hence more efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b479547",
   "metadata": {},
   "source": [
    "# That's it. not bad at all :p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

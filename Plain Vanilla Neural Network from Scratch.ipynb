{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02737cbd",
   "metadata": {},
   "source": [
    "# Neural network learning from scratch (Math + Numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e849b0f3",
   "metadata": {},
   "source": [
    "This is a major milestone during my self-taught machine learning journey. Understanding the backpropagation and calculus in it is quite an experience. That said, this is really the simplest implementation :p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "285ac7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Normalization\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid, softmax\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df75c1df",
   "metadata": {},
   "source": [
    "### The dataset(diagnosing malignancy given texture_mean and radius_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c8caac3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>radius_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10.38</td>\n",
       "      <td>17.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17.77</td>\n",
       "      <td>20.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>21.25</td>\n",
       "      <td>19.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>20.38</td>\n",
       "      <td>11.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>14.34</td>\n",
       "      <td>20.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>1</td>\n",
       "      <td>22.39</td>\n",
       "      <td>21.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>1</td>\n",
       "      <td>28.25</td>\n",
       "      <td>20.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1</td>\n",
       "      <td>28.08</td>\n",
       "      <td>16.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>1</td>\n",
       "      <td>29.33</td>\n",
       "      <td>20.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>0</td>\n",
       "      <td>24.54</td>\n",
       "      <td>7.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     diagnosis  texture_mean  radius_mean\n",
       "0            1         10.38        17.99\n",
       "1            1         17.77        20.57\n",
       "2            1         21.25        19.69\n",
       "3            1         20.38        11.42\n",
       "4            1         14.34        20.29\n",
       "..         ...           ...          ...\n",
       "564          1         22.39        21.56\n",
       "565          1         28.25        20.13\n",
       "566          1         28.08        16.60\n",
       "567          1         29.33        20.60\n",
       "568          0         24.54         7.76\n",
       "\n",
       "[569 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"bdiag.csv\")[[\"diagnosis\", \"texture_mean\", \"radius_mean\"]]\n",
    "df[\"diagnosis\"] = df[\"diagnosis\"].replace({\"M\":1, \"B\":0})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b617ade",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b641944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"texture_mean\", \"radius_mean\"]].values\n",
    "y = df[\"diagnosis\"].values.reshape(-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b827b2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (455, 2)\n",
      "Y_train: (455, 1)\n",
      "X_test:  (114, 2)\n",
      "Y_test:  (114, 1)\n"
     ]
    }
   ],
   "source": [
    "print('X_train: ' + str(X_train.shape))\n",
    "print('Y_train: ' + str(y_train.shape))\n",
    "print('X_test:  '  + str(X_test.shape))\n",
    "print('Y_test:  '  + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7056c1",
   "metadata": {},
   "source": [
    "### Feature normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c85e917",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = Normalization(axis=1)\n",
    "norm.adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d033f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(norm(X_train))  # converts back to numpy because tensor is slower\n",
    "X_test = np.array(norm(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3e260e",
   "metadata": {},
   "source": [
    "### The Model overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5771bd",
   "metadata": {},
   "source": [
    "![alt text](model_nn_scratch.jpg \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1324519",
   "metadata": {},
   "source": [
    "## Doing it the \"easy\" way (using library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95555dbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "15/15 [==============================] - 3s 8ms/step - loss: 0.5537 - accuracy: 0.8527\n",
      "Epoch 2/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5467 - accuracy: 0.8527\n",
      "Epoch 3/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5397 - accuracy: 0.8593\n",
      "Epoch 4/30\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.5328 - accuracy: 0.8615\n",
      "Epoch 5/30\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.5261 - accuracy: 0.8659\n",
      "Epoch 6/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5193 - accuracy: 0.8659\n",
      "Epoch 7/30\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.5126 - accuracy: 0.8681\n",
      "Epoch 8/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5061 - accuracy: 0.8681\n",
      "Epoch 9/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.4995 - accuracy: 0.8681\n",
      "Epoch 10/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.4929 - accuracy: 0.8681\n",
      "Epoch 11/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.4863 - accuracy: 0.8703\n",
      "Epoch 12/30\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4799 - accuracy: 0.8747\n",
      "Epoch 13/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.4736 - accuracy: 0.8769\n",
      "Epoch 14/30\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4676 - accuracy: 0.8791\n",
      "Epoch 15/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.4616 - accuracy: 0.8813\n",
      "Epoch 16/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.4556 - accuracy: 0.8813\n",
      "Epoch 17/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.4497 - accuracy: 0.8813\n",
      "Epoch 18/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.4437 - accuracy: 0.8813\n",
      "Epoch 19/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.4378 - accuracy: 0.8813\n",
      "Epoch 20/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.4320 - accuracy: 0.8813\n",
      "Epoch 21/30\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4261 - accuracy: 0.8813\n",
      "Epoch 22/30\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4202 - accuracy: 0.8857\n",
      "Epoch 23/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.4147 - accuracy: 0.8879\n",
      "Epoch 24/30\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4090 - accuracy: 0.8879\n",
      "Epoch 25/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.4033 - accuracy: 0.8901\n",
      "Epoch 26/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.3978 - accuracy: 0.8901\n",
      "Epoch 27/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.3920 - accuracy: 0.8901\n",
      "Epoch 28/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.3865 - accuracy: 0.8923\n",
      "Epoch 29/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.3812 - accuracy: 0.8923\n",
      "Epoch 30/30\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3755 - accuracy: 0.8923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c5bc400f70>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    \n",
    "    Dense(units=2, activation=relu),\n",
    "    Dense(units=1, activation=linear)\n",
    "])\n",
    "\n",
    "model.compile(loss=BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=Adam(learning_rate=0.001),\n",
    "              metrics=\"accuracy\"\n",
    "             )\n",
    "model.fit(X_train, y_train, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5610e0c6",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4df90265",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 5ms/step - loss: 0.3928 - accuracy: 0.8947\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3927581310272217, 0.8947368264198303]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20c9b60",
   "metadata": {},
   "source": [
    "# Doing it the hard way (scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce210e61",
   "metadata": {},
   "source": [
    "### The math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7843ec1c",
   "metadata": {},
   "source": [
    "2 features --> 1 hidden layer (2 neuron relu) --> 1 output layer (1 neuron sigmoid)\n",
    "\n",
    "#### Forward propagation:\n",
    "\n",
    "$$z_{1}^{[1]} = w_{1}^{[1]}x_{1} + w_{2}^{[1]}x_{2} + b_{1}^{[1]} $$ <br>\n",
    "$$z_{2}^{[1]} = w_{3}^{[1]}x_{1} + w_{4}^{[1]}x_{2} + b_{2}^{[1]} $$ <br>\n",
    "$$a_{1}^{[1]} = \\text{relu}(z_{1}^{[1]})$$ <br>\n",
    "$$a_{2}^{[1]} = \\text{relu}(z_{2}^{[1]})$$ <br>\n",
    "$$z^{[2]} = w_{1}^{[2]}x_{1} + w_{2}^{[2]}x_{2} + b^{[2]} $$ <br>\n",
    "$$a^{[2]} = \\frac{1}{1 + e^{-z^{[2]}}}$$ <br>\n",
    "\n",
    "#### cost & loss function: <br>\n",
    "$$\\text{cross entropy cost}(\\hat{a}^{[2]}, \\hat{y}) = -\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{j=1}^{m}y_{ij}\\text{log}(a^{[2]}[ij])$$ <br>\n",
    "$$\\text{binary cross entropy cost}(\\hat{a}^{[2]}, \\hat{y}) = -\\frac{1}{n}\\sum_{i=1}^{n}y_{i}\\text{log}(a^{[2]}[i]) + (1-y_{i})\\text{log}(1-a^{[2]}[i])$$ <br>\n",
    "\n",
    "$$\\frac{\\partial ( \\text{binary cross entropy cost}(\\hat{a}^{[2]}, \\hat{y}))}{\\partial \\vec{w}} = \\frac{1}{n}(\\frac {\\partial (\\text{binary cross entropy loss}(a^{[2]}, \\hat{y}))_{1}}{\\partial \\vec{w}} + \\frac {\\partial (\\text{binary cross entropy loss}(a^{[2]}, \\hat{y}))_{2}}{\\partial \\vec{w}}+ ...+ \\frac {\\partial (\\text{binary cross entropy loss}(a^{[2]}, \\hat{y}))_{n}}{\\partial \\vec{w}})\\space \\text{for n=number of examples}$$<br>\n",
    "\n",
    "$$\\text{binary cross entropy loss}(a^{[2]}, \\hat{y}) = -(y_{i}\\text{log}(a^{[2]}) + (1-y_{i})\\text{log}(1-a^{[2]}))$$\n",
    "\n",
    "- The point of backpropagation is to determine the gradient of cost function wrt to each parameter $w$\n",
    "- Intuitively speaking, the gradient of cost function wrt to parameter $w$ is the average of all the gradient of loss function wrt to parameter $w_{1}, w_{2},..., w_{n}$ where $n=$number of examples.\n",
    "- Backpropagation involves the derivation of loss function wrt to each parameters, then averaging them later for gradient descent\n",
    "\n",
    "#### Backward propagation:\n",
    "\n",
    "##### first layer\n",
    "$\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z^{[2]}}=\\frac{\\partial a^{[2]}}{\\partial z^{[2]}}\\times\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial a^{[2]}} = (1-a^{[2]})\\times \\frac{a^{[2]}-y}{a^{[2]}(1-a^{[2]})} = a^{[2]} - y$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial w_{1}^{2}} = \\frac{\\partial z^{[2]}}{\\partial w_{1}^{[2]}}\\times\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z^{[2]}} = a_{1}^{[1]}\\times (a^{[2]} - y)$$\n",
    "\n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial w_{2}^{2}} = \\frac{\\partial z^{[2]}}{\\partial w_{2}^{[2]}}\\times\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z^{[2]}} = a_{2}^{[1]}\\times (a^{[2]} - y)$$\n",
    ". <br>\n",
    ".(continue yourself :p) <br>\n",
    ". <br>\n",
    "\n",
    "##### Second layer\n",
    "$\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z_{1}^{[1]}}=\\frac{\\partial a_{1}^{[1]}}{\\partial z_{1}^{[1]}}\\times\\frac{\\partial z^{[2]}}{\\partial a_{1}^{[1]}}\\times\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z^{[2]}} = \\text{relu'}(z_1^{[1]}) \\times w_1^{[2]}\\times (a^{[2]} - y)$ <br>\n",
    "\n",
    "$\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z_{2}^{[1]}}=\\frac{\\partial a_{1}^{[1]}}{\\partial z_{2}^{[1]}}\\times\\frac{\\partial z^{[2]}}{\\partial a_{1}^{[1]}}\\times\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z^{[2]}} = \\text{relu'}(z_2^{[1]}) \\times w_2^{[2]}\\times (a^{[2]} - y)$ <br>\n",
    "\n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial w_{1}^{1}} = \\frac{\\partial z_{1}^{[1]}}{\\partial w_{1}^{[1]}}\\times \\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z_{1}^{[1]}} = x_1\\times \\text{relu'}(z_1^{[1]}) \\times w_1^{[2]}\\times (a^{[2]} - y)$$ \n",
    ". <br>\n",
    ".(continue yourself :p) <br>\n",
    ". <br>\n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial w_{3}^{1}} = \\frac{\\partial z_{2}^{[1]}}{\\partial w_{3}^{[1]}}\\times \\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z_{1}^{[1]}} = x_2\\times \\text{relu'}(z_2^{[1]}) \\times w_2^{[2]}\\times (a^{[2]} - y)$$\n",
    ". <br>\n",
    ".(continue yourself :p) <br>\n",
    ". <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b07bd4",
   "metadata": {},
   "source": [
    "### No vectorization and non-modular implementation (Not efficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65243079",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    \n",
    "    def initialize(self):\n",
    "        self.w11, self.w12, self.w13, self.w14, self.w21, self.w22, self.b11, self.b12, self.b2 = np.random.randn(9)\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def binary_cross_entropy(self, x, y_true):\n",
    "        return -(y_true * np.log(x) + (1 - y_true) * np.log(1 - x))\n",
    "    \n",
    "    def derivative_relu(self, x):\n",
    "        return 1 if x>0 else 0\n",
    "    \n",
    "    def forward_prop(self, x1, x2):\n",
    "        forward_dict = {} \n",
    "        \n",
    "        forward_dict[\"z11\"] = self.w11 * x1 + self.w12 * x2 + self.b11\n",
    "        forward_dict[\"z12\"] = self.w13 * x1 + self.w14 * x2 + self.b12\n",
    "        \n",
    "        forward_dict[\"a11\"] = self.relu(forward_dict[\"z11\"])\n",
    "        forward_dict[\"a12\"] = self.relu(forward_dict[\"z12\"])\n",
    "        \n",
    "        forward_dict[\"z2\"] = self.w21 * forward_dict[\"a11\"] + self.w22 * forward_dict[\"a12\"] + self.b2\n",
    "        forward_dict[\"a2\"] = self.sigmoid(forward_dict[\"z2\"])\n",
    "        \n",
    "        return forward_dict\n",
    "\n",
    "    def back_prop(self, x1, x2, y_true, forward_dict):\n",
    "        #Gradient will be calculated for each training example and averaged later#\n",
    "        \n",
    "        deriva_dict = {}\n",
    "        \n",
    "        error = forward_dict[\"a2\"] - y_true\n",
    "        deriva_dict[\"dloss_dw21\"] = forward_dict[\"a11\"] * error\n",
    "        deriva_dict[\"dloss_dw22\"] = forward_dict[\"a12\"] * error\n",
    "        deriva_dict[\"dloss_db2\"] = error\n",
    "        \n",
    "        dcost_dz11 = self.derivative_relu(forward_dict[\"z11\"]) * self.w21 * error\n",
    "        dcost_dz12 = self.derivative_relu(forward_dict[\"z12\"]) * self.w22 * error\n",
    "        \n",
    "        deriva_dict[\"dloss_dw11\"] = x1 * dcost_dz11\n",
    "        deriva_dict[\"dloss_dw12\"] = x2 * dcost_dz11\n",
    "        deriva_dict[\"dloss_db11\"] = dcost_dz11\n",
    "        \n",
    "        deriva_dict[\"dloss_dw13\"] = x1 * dcost_dz12\n",
    "        deriva_dict[\"dloss_dw14\"] = x2 * dcost_dz12\n",
    "        deriva_dict[\"dloss_db12\"] = dcost_dz12\n",
    "        \n",
    "        return deriva_dict\n",
    "    \n",
    "    def get_prob_class(self, x1, x2):\n",
    "        prob = self.forward_prop(x1, x2)[\"a2\"]\n",
    "        class_ = self.encode_class(prob)\n",
    "        return prob, class_\n",
    "    \n",
    "    def encode_class(self, x):\n",
    "        return 1 if x>0.5 else 0\n",
    "    \n",
    "    def get_accuracy_cost(self, X, y):\n",
    "        n = X.shape[0]\n",
    "        n_correct = 0\n",
    "        cost = 0\n",
    "        for i in range(n):\n",
    "            x1, x2 = X[i]\n",
    "            prediction = self.get_prob_class(x1, x2)\n",
    "            if prediction[1] == y[i][0]:\n",
    "                n_correct += 1\n",
    "            cost += self.binary_cross_entropy(prediction[0], y[i][0])\n",
    "        return n_correct/n, cost\n",
    "\n",
    "    def fit(self, X, y, alpha, epochs):\n",
    "        n, m = X.shape\n",
    "        \n",
    "        self.initialize()\n",
    "        costs = []\n",
    "\n",
    "        for i in range(1, epochs+1):\n",
    "            dcost_dw21 = 0\n",
    "            dcost_dw22 = 0\n",
    "            dcost_db2 = 0\n",
    "            dcost_dw11 = 0\n",
    "            dcost_dw12 = 0\n",
    "            dcost_db11 = 0\n",
    "            dcost_dw13 = 0\n",
    "            dcost_dw14 = 0\n",
    "            dcost_db12 = 0\n",
    "            \n",
    "            for j in range(n):\n",
    "                x1 , x2 = X[j]\n",
    "                y_true = y[j][0]\n",
    "                forward_prop = self.forward_prop(x1, x2)\n",
    "                back_prop = self.back_prop(x1, x2, y_true, forward_prop)\n",
    "                \n",
    "                dcost_dw21 += back_prop[\"dloss_dw21\"]\n",
    "                dcost_dw22 += back_prop[\"dloss_dw22\"]\n",
    "                dcost_db2 += back_prop[\"dloss_db2\"]\n",
    "                dcost_dw11 += back_prop[\"dloss_dw11\"]\n",
    "                dcost_dw12 += back_prop[\"dloss_dw12\"]\n",
    "                dcost_db11 += back_prop[\"dloss_db11\"]\n",
    "                dcost_dw13 += back_prop[\"dloss_dw13\"]\n",
    "                dcost_dw14 += back_prop[\"dloss_dw14\"]\n",
    "                dcost_db12 += back_prop[\"dloss_db12\"]\n",
    "                    \n",
    "            self.w21 -= alpha*(1/n)*dcost_dw21 \n",
    "            self.w22 -= alpha*(1/n)*dcost_dw22 \n",
    "            self.b2  -= alpha*(1/n)*dcost_db2\n",
    "            self.w11 -= alpha*(1/n)*dcost_dw11 \n",
    "            self.w12 -= alpha*(1/n)*dcost_dw12 \n",
    "            self.b11 -= alpha*(1/n)*dcost_db11\n",
    "            self.w13 -= alpha*(1/n)*dcost_dw13\n",
    "            self.w14 -= alpha*(1/n)*dcost_dw14\n",
    "            self.b12 -= alpha*(1/n)*dcost_db12\n",
    "            \n",
    "            accuracy, cost = self.get_accuracy_cost(X, y)\n",
    "            costs.append(cost)\n",
    "            \n",
    "            print(f\"epoch: {i}, cost: {cost:.2f}, accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13dc3d4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, cost: 467.80, accuracy: 49.89%\n",
      "epoch: 2, cost: 420.67, accuracy: 53.63%\n",
      "epoch: 3, cost: 383.45, accuracy: 57.36%\n",
      "epoch: 4, cost: 353.63, accuracy: 60.66%\n",
      "epoch: 5, cost: 329.25, accuracy: 63.52%\n",
      "epoch: 6, cost: 309.02, accuracy: 68.13%\n",
      "epoch: 7, cost: 292.33, accuracy: 70.55%\n",
      "epoch: 8, cost: 278.24, accuracy: 73.85%\n",
      "epoch: 9, cost: 266.12, accuracy: 75.60%\n",
      "epoch: 10, cost: 255.51, accuracy: 78.24%\n",
      "epoch: 11, cost: 246.21, accuracy: 78.68%\n",
      "epoch: 12, cost: 238.06, accuracy: 79.78%\n",
      "epoch: 13, cost: 230.94, accuracy: 80.66%\n",
      "epoch: 14, cost: 224.59, accuracy: 81.54%\n",
      "epoch: 15, cost: 218.89, accuracy: 81.98%\n",
      "epoch: 16, cost: 213.69, accuracy: 82.86%\n",
      "epoch: 17, cost: 209.02, accuracy: 83.08%\n",
      "epoch: 18, cost: 204.74, accuracy: 83.96%\n",
      "epoch: 19, cost: 200.83, accuracy: 84.62%\n",
      "epoch: 20, cost: 197.25, accuracy: 85.05%\n",
      "epoch: 21, cost: 193.94, accuracy: 85.71%\n",
      "epoch: 22, cost: 190.91, accuracy: 86.15%\n",
      "epoch: 23, cost: 188.09, accuracy: 85.71%\n",
      "epoch: 24, cost: 185.44, accuracy: 85.93%\n",
      "epoch: 25, cost: 182.99, accuracy: 85.93%\n",
      "epoch: 26, cost: 180.70, accuracy: 86.15%\n",
      "epoch: 27, cost: 178.57, accuracy: 86.81%\n",
      "epoch: 28, cost: 176.59, accuracy: 87.03%\n",
      "epoch: 29, cost: 174.72, accuracy: 87.03%\n",
      "epoch: 30, cost: 172.97, accuracy: 87.47%\n",
      "epoch: 31, cost: 171.34, accuracy: 88.13%\n",
      "epoch: 32, cost: 169.81, accuracy: 88.13%\n",
      "epoch: 33, cost: 168.35, accuracy: 88.35%\n",
      "epoch: 34, cost: 166.93, accuracy: 88.35%\n",
      "epoch: 35, cost: 165.59, accuracy: 88.35%\n",
      "epoch: 36, cost: 164.33, accuracy: 88.35%\n",
      "epoch: 37, cost: 163.12, accuracy: 88.35%\n",
      "epoch: 38, cost: 161.98, accuracy: 88.35%\n",
      "epoch: 39, cost: 160.86, accuracy: 88.35%\n",
      "epoch: 40, cost: 159.81, accuracy: 88.13%\n",
      "epoch: 41, cost: 158.81, accuracy: 88.13%\n",
      "epoch: 42, cost: 157.85, accuracy: 88.13%\n",
      "epoch: 43, cost: 156.96, accuracy: 87.91%\n",
      "epoch: 44, cost: 156.09, accuracy: 87.91%\n",
      "epoch: 45, cost: 155.28, accuracy: 88.13%\n",
      "epoch: 46, cost: 154.50, accuracy: 88.13%\n",
      "epoch: 47, cost: 153.76, accuracy: 88.35%\n",
      "epoch: 48, cost: 153.04, accuracy: 88.35%\n",
      "epoch: 49, cost: 152.35, accuracy: 88.13%\n",
      "epoch: 50, cost: 151.69, accuracy: 88.13%\n",
      "epoch: 51, cost: 151.05, accuracy: 88.13%\n",
      "epoch: 52, cost: 150.44, accuracy: 87.91%\n",
      "epoch: 53, cost: 149.85, accuracy: 88.35%\n",
      "epoch: 54, cost: 149.27, accuracy: 88.35%\n",
      "epoch: 55, cost: 148.72, accuracy: 88.35%\n",
      "epoch: 56, cost: 148.19, accuracy: 87.91%\n",
      "epoch: 57, cost: 147.67, accuracy: 88.13%\n",
      "epoch: 58, cost: 147.18, accuracy: 88.13%\n",
      "epoch: 59, cost: 146.69, accuracy: 88.13%\n",
      "epoch: 60, cost: 146.22, accuracy: 88.13%\n",
      "epoch: 61, cost: 145.75, accuracy: 88.13%\n",
      "epoch: 62, cost: 145.30, accuracy: 88.13%\n",
      "epoch: 63, cost: 144.86, accuracy: 88.13%\n",
      "epoch: 64, cost: 144.43, accuracy: 88.13%\n",
      "epoch: 65, cost: 144.02, accuracy: 88.13%\n",
      "epoch: 66, cost: 143.62, accuracy: 88.13%\n",
      "epoch: 67, cost: 143.22, accuracy: 88.35%\n",
      "epoch: 68, cost: 142.85, accuracy: 88.35%\n",
      "epoch: 69, cost: 142.48, accuracy: 88.35%\n",
      "epoch: 70, cost: 142.12, accuracy: 88.35%\n",
      "epoch: 71, cost: 141.78, accuracy: 88.35%\n",
      "epoch: 72, cost: 141.44, accuracy: 88.57%\n",
      "epoch: 73, cost: 141.12, accuracy: 88.57%\n",
      "epoch: 74, cost: 140.80, accuracy: 88.57%\n",
      "epoch: 75, cost: 140.49, accuracy: 88.79%\n",
      "epoch: 76, cost: 140.19, accuracy: 88.79%\n",
      "epoch: 77, cost: 139.90, accuracy: 88.57%\n",
      "epoch: 78, cost: 139.61, accuracy: 88.57%\n",
      "epoch: 79, cost: 139.33, accuracy: 88.57%\n",
      "epoch: 80, cost: 139.05, accuracy: 88.57%\n",
      "epoch: 81, cost: 138.79, accuracy: 88.57%\n",
      "epoch: 82, cost: 138.53, accuracy: 88.57%\n",
      "epoch: 83, cost: 138.28, accuracy: 88.57%\n",
      "epoch: 84, cost: 138.04, accuracy: 88.57%\n",
      "epoch: 85, cost: 137.80, accuracy: 88.57%\n",
      "epoch: 86, cost: 137.56, accuracy: 88.57%\n",
      "epoch: 87, cost: 137.33, accuracy: 88.57%\n",
      "epoch: 88, cost: 137.10, accuracy: 88.57%\n",
      "epoch: 89, cost: 136.88, accuracy: 88.57%\n",
      "epoch: 90, cost: 136.66, accuracy: 88.57%\n",
      "epoch: 91, cost: 136.45, accuracy: 88.57%\n",
      "epoch: 92, cost: 136.24, accuracy: 88.57%\n",
      "epoch: 93, cost: 136.03, accuracy: 88.57%\n",
      "epoch: 94, cost: 135.84, accuracy: 88.57%\n",
      "epoch: 95, cost: 135.64, accuracy: 88.57%\n",
      "epoch: 96, cost: 135.45, accuracy: 88.57%\n",
      "epoch: 97, cost: 135.26, accuracy: 88.57%\n",
      "epoch: 98, cost: 135.08, accuracy: 88.57%\n",
      "epoch: 99, cost: 134.90, accuracy: 88.57%\n",
      "epoch: 100, cost: 134.72, accuracy: 88.35%\n"
     ]
    }
   ],
   "source": [
    "classifier = NeuralNetwork()\n",
    "classifier.fit(X_train, y_train, 0.1, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72ce724",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6bcbfb75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9035087719298246, 32.74126953834825)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.get_accuracy_cost(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036a1cd4",
   "metadata": {},
   "source": [
    "#### not bad at all :p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

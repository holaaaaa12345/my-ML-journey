{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02737cbd",
   "metadata": {},
   "source": [
    "# Neural network from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c588d68a",
   "metadata": {},
   "source": [
    "This notebook is a part of the series __From Scratch__, where i try to code machine learning algorithms from scratch (just numpy and math)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7a2fbf",
   "metadata": {},
   "source": [
    "This is a major milestone during my self-taught machine learning journey. Understanding the backpropagation and calculus in it is quite an experience. That said, this is really the simplest implementation :p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "285ac7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Normalization\n",
    "from tensorflow.keras.activations import linear, relu\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df75c1df",
   "metadata": {},
   "source": [
    "### Objective: Determine a tumor's malignancy given its texture_mean and radius_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9e7e12",
   "metadata": {},
   "source": [
    "#### The dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0c8caac3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.8</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.6</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.9</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.8</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.5</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38           122.8     1001.0          0.11840   \n",
       "1        20.57         17.77           132.9     1326.0          0.08474   \n",
       "2        19.69         21.25           130.0     1203.0          0.10960   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.07871  ...         25.38          17.33            184.6   \n",
       "1                 0.05667  ...         24.99          23.41            158.8   \n",
       "2                 0.05999  ...         23.57          25.53            152.5   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "\n",
       "[3 rows x 30 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_load = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "raw_load[0].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9e6f28",
   "metadata": {},
   "source": [
    "#### Test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b641944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_X = raw_load[0].values\n",
    "raw_y = raw_load[1].values.reshape(-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(raw_X, raw_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073d0c3a",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction\n",
    "I deliberately pick 2 as the number of components to reduce the complexity of the Neural Network :p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bc84e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Pipeline([('scaling', StandardScaler()), ('pca', PCA(n_components=2))])\n",
    "X_train = preprocess.fit_transform(X_train)\n",
    "X_test = preprocess.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df5d58c",
   "metadata": {},
   "source": [
    "#### Scatter plot of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0da2e048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEHCAYAAABfkmooAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/OQEPoAAAACXBIWXMAAAsTAAALEwEAmpwYAABUuklEQVR4nO29eXyb1ZX//zneJMdbnMSxnTjORjYSFCeYEMIW9sCwdDwZaMu0dMJACwTK9zf5doEuLgz8Op3MlKFsvy5MCy1rarYpFAhlLaSQVTgkgWwYO4udRLGT2PJ6fn8c3TyPZcmSbO0579dLL+l59CxXV9I5957tEjNDURRFUQYjI9ENUBRFUZIfVRaKoihKSFRZKIqiKCFRZaEoiqKERJWFoiiKEpKsRDcgFowZM4YnTZqU6GYoiqKkFOvWrTvAzCWB3ktLZTFp0iSsXbs20c1QFEVJKYjo82DvqRlKURRFCYkqC0VRFCUkqiwURVGUkKSlz0JRlPShu7sbjY2N8Hq9iW5K2uB0OlFRUYHs7Oywz1FloShKUtPY2IiCggJMmjQJRJTo5qQ8zIyDBw+isbERkydPDvs8VRaKkg643UBdHdDQAFRWAjU1gMuV6FZFBa/Xq4oiihARRo8ejZaWlojOU5+FoqQ6bjewciXg8QAVFfK8cqXsTxNUUUSXofRnXJQFET1KRM1EVG/b9x9EtJWI3ET0HBGNDHLubiL6mIg2EpEmTyiKP3V1QHGxPDIyrNd1dYlumZJGxGtm8VsAS/z2vQ5gDjO7AHwK4PuDnH8eM1cxc3WM2qcoqUtDA1BU1H9fUZHsVxLOW2+9hcsvvxwA8OKLL+KnP/1p3O69ceNGvPzyy1G5VlyUBTO/A+CQ377XmLnHt7kGQEU82qIoaUdlJdDa2n9fa6vsV5KKK6+8Et/73vfidr+UUxZhsAzAK0HeYwCvEdE6Irox2AWI6EYiWktEayN13ChKSlNTI34Kjwfo67Ne19QkumWJwe0GamuBZcvkOQq+m927d2PmzJn4xje+genTp+Paa6/F6tWrceaZZ2LatGn48MMP8eGHH+KMM87AvHnzsGjRImzbtm3AdX77299i+fLlAIAdO3Zg4cKFOOWUU/CDH/wA+fn5AGQmsnjxYixduhQzZ87EtddeC7Oi6V133YXTTjsNc+bMwY033nh8/+LFi/Hd734XCxYswPTp0/Huu++iq6sLP/rRj/D000+jqqoKTz/99LD6IOHKgojuBNAD4A9BDjmLmecDuBTALUR0TqCDmPmXzFzNzNUlJQHrYClKeuJyAStWiJ+isVGeV6xIm2ioiIihs3/79u3413/9V2zduhVbt27FE088gffeew8rV67Evffei5kzZ+Ldd9/Fhg0bcNddd+GOO+4Y9Hrf/va38e1vfxsff/wxKir6G1Y2bNiA++67D5988gl27tyJv/71rwCA5cuX46OPPkJ9fT06Ojrwv//7v8fP6enpwYcffoj77rsPP/nJT5CTk4O77roL11xzDTZu3IhrrrlmWJ8/oaGzRPQNAJcDuICDLAbOzE2+52Yieg7AAgDvxK2RipIKuFwnpnLwx+7sB6znurph98/kyZNxyimnAABmz56NCy64AESEU045Bbt370Zrayuuu+46fPbZZyAidHd3D3q9Dz74AM8//zwA4Ktf/SpWrFhx/L0FCxYcVyBVVVXYvXs3zjrrLLz55pv42c9+hvb2dhw6dAizZ8/GFVdcAQCo8c0kTz31VOzevXtYnzUQCZtZENESAN8BcCUztwc5Jo+ICsxrABcDqA90rKIoSiyd/Q6H4/jrjIyM49sZGRno6enBD3/4Q5x33nmor6/HSy+9NKyMc/u9MjMz0dPTA6/Xi5tvvhmrVq3Cxx9/jBtuuKHfPcw55vhoE6/Q2ScBfABgBhE1EtH1AB4AUADgdV9Y7CO+Y8cRkfHIlAJ4j4g2AfgQwJ+Y+c/xaLOiKClIAp39ra2tGD9+PADxTYRi4cKF+OMf/wgAeOqpp0IebxTDmDFjcPToUaxatSrkOQUFBThy5EjI48IhXtFQX2HmcmbOZuYKZv4NM5/EzBN8IbFVzPwt37F7mPky3+udzDzX95jNzPfEo72KoqQoCXT2f+c738H3v/99zJs3L6yR/X333Yf/+q//gsvlwvbt21HkPyPyY+TIkbjhhhswZ84cXHLJJTjttNNC3uO8887DJ598EhUHNwVxFaQ01dXVrIsfKUp6sGXLFsyaNSv8E1Kk9El7eztyc3NBRHjqqafw5JNP4oUXXojb/QP1KxGtC5bPprWhFEVJL1LE2b9u3TosX74czIyRI0fi0UcfTXSTBkWVhaIoSgI4++yzsWnTpkQ3I2wSnmehKIqiJD+qLBRFUZSQqLJQFEVRQqLKQlEURQmJKgtFUZQQ7N69G3PmzBn2ddauXYvbbrstCi2KPxoNpSiKEieqq6tRXZ2ay/LozEJRlLQiBhXKAUhV12uvvRazZs3C0qVL0d7ejnXr1uHcc8/FqaeeiksuuQR79+4FELhkONB/IaSWlhZcdNFFmD17Nv7lX/4FEydOxIEDB7B7927MmjULN9xwA2bPno2LL74YHR0d0fkQw0CVhaIoaUMslyPftm0bbr75ZmzZsgWFhYV48MEHceutt2LVqlVYt24dli1bhjvvvPP48f4lw/35yU9+gvPPPx+bN2/G0qVL0WArdvjZZ5/hlltuwebNmzFy5MjjNaQSiZqhFEVJG2JYoRwTJkzAmWeeCQD4p3/6J9x7772or6/HRRddBADo7e1FeXn58eNDlQx/77338NxzzwEAlixZgmLTWEg59KqqqkHPjzeqLBRFSRsaGmRGYSday5ETUb/tgoICzJ49Gx988EHA44dTMty/RLmaoRRFiTqxstmnArGsUN7Q0HBcMTzxxBNYuHAhWlpaju/r7u7G5s2bw77emWeeiWeeeQYA8Nprr8Hj8Qy/kTFElYWipBGxtNmnArGsUD5jxgw8+OCDmDVrFjwez3F/xXe/+13MnTsXVVVVeP/998O+3o9//GO89tprmDNnDp599lmUlZWhoKBg+A2NEVqiXFHSiNpaEY428/fx7draRLVqeERaojxFKpSjs7MTmZmZyMrKwgcffICbbroJGzdujNv9k7JEORE9Cllru5mZ5/j2jQLwNIBJAHYDuJqZB8zDiOg6AD/wbf4bM/8uHm1WlFQkljb7VCFFKpSjoaEBV199Nfr6+pCTk4Nf/epXiW7SoMTLwf1byDKqj9n2fQ/AG8z8UyL6nm/7u/aTfArlxwCqATCAdUT0YiCloiiKjKT9ZxZxWlVUiZBp06Zhw4YNiW5G2MRrWdV3ABzy230VADNL+B2ALwU49RIArzPzIZ+CeB3Akli1U1FSnQSuKhpT0tFcnkiG0p+JdHCXMvNe3+t9AEoDHDMewBe27UbfvgEQ0Y1EtJaI1ra0tES3pYqSIrhcwIoVMrNobJTnFStSwywTDKfTiYMHD6rCiBLMjIMHD8LpdEZ0XlLkWTAzE9GwfgnM/EsAvwTEwR2VhilKCpIqNvtwqaioQGNjI3QQGD2cTicq/J1bIUiksthPROXMvJeIygE0BzimCcBi23YFgLfi0DZFUZKE7OxsTJ48OdHNOOFJpLJ4EcB1AH7qe34hwDGvAriXiIy77mIA349P8xTFj1SJyVSUGBAXnwURPQngAwAziKiRiK6HKImLiOgzABf6tkFE1UT0awBg5kMA7gbwke9xl2+fosSXEz3bTTnhicvMgpm/EuStCwIcuxbAv9i2HwXwaIyapijhEcsKdYqSAmi5D0UJh4YGyW6zc6JluyknNKosFCUcYlmhTlFSAFUWihIO6ZrtpihhospCUcIhHbPdFCUCkiIpT1FSgnTLdlOUCNCZhaIoihISVRaKoihKSFRZKIqiKCFRn4WSOLR8hqKkDKoslMRgymcUF/cvn6ERRoOi+lVJFGqGUhKDvXxGRob1uq4u0S1LWrQ8lZJIdGahJAZdLDpszGzi+ecBhwOYP9/Sr4CWp1Lig84slMSg5TPCwj6bAABm4P33gf37ZVv1qxIvVFkoiUHLZ4SF3Vo3ciRABDidwJYt8r7qVyVeqLJQEoOWzwgLe7HbmTMBr1dmF4cPq35V4ktCfRZENAPA07ZdUwD8iJnvsx2zGLKK3i7frjpmvitOTVRiiZbPCEllpSiE4mKgrAw44wxgwwaZYRQXA9dfr12oxIeEKgtm3gagCgCIKBOy5vZzAQ59l5kvj2PTFCUpqKkRnwUgMwyHA5gxQydhSvxJpmioCwDsYObPE90QJQga5B93jLXO3u06m1ASATFzotsAACCiRwGsZ+YH/PYvBvBHAI0A9gBYwcybA5x/I4AbAaCysvLUzz9XnRNV7El0RUXiWfV4RJIBqkQUJQ0gonXMXB3wvWRQFkSUA1EEs5l5v997hQD6mPkoEV0G4L+Zedpg16uurua1a9fGrsEnIrW1lvHc4PEAXV3AsWOBlYgqDEVJKQZTFskSDXUpZFax3/8NZm5j5qO+1y8DyCaiMfFu4AlPsDWo16zRTGxFOQFIFmXxFQBPBnqDiMqIiHyvF0DafDCObVOA4El0zIGViGaKKUpakXBlQUR5AC4CUGfb9y0i+pZvcymAeiLaBOB+AF/mZLCdnWgES6JbuFAzsRXlBCApfBbRRn0WMSJQNBQQ3PGtPgtFSSkG81kkU+iskuwES6LT2E5FSXtUWSjDJxaZ2MFyOjTXQ1ESQsJ9FooygGALN6xapQs6KEqCUGWhJB/BFkZ64AEN01WUBKHKQkk+guV0NDVpmK6iJAj1WSjJh73UqqG1FRg/Xp7990c5TFfdIooyEJ1ZKMlHsJyO5ctjvmCSrnOtKIFRZaEkH8EWRlq6NOYLJgVzl6hbRDnRUTOUkpwEC8eN8YJJDQ0yo7CjbhFF0ZmFovQjWAksrV6inOjozOJERj25A/Bfmc5UL7n++sS2S1ESjc4sTlTUkxuQYO6SE1yHKkp4MwsiugRABYA3mHm3bf8yZn40Rm1TYondkwtYz3V1J7xkjLFbRFFSkpAzCyK6F8CdAE4B8AYR3Wp7e3msGqbEmGCJb+rJVRQlAOGYoa4AcD4z3w7gVACXEtHPfe9RrBqmxBj15CqKEgHhKIssZu4BAGY+DFEehUT0LICcGLZNiSXBEt+imOCmKEr6EI6y2EFE55oNZu5l5usBbAMwa7gNIKLdRPQxEW0kogErFpFwPxFtJyI3Ec0f7j0VqCd3GLjdQG0tsGyZPJ/gMQHKCULIlfKIKBcAmLkjwHvjmbnJ93o2M2+OuAFEuwFUM/OBIO9fBuBWAJcBOB3AfzPz6YNdU1fKS0FSJIzXBJHpwoDJS4r8lJKSwVbKCzmzYOaOQIrC916TbfPxIbYvFFcBeIyFNQBGElF5jO6lJIIUCuPVciDJTQr9lFKOaOZZDNXZzQBeI6J1RHRjgPfHA/jCtt3o29f/5kQ3EtFaIlrb0tIyxKYoCSGFJLAGkSU3KfRTSjmiqSwGt2cF5yxmng/gUgC3ENE5Q7o58y+ZuZqZq0tKSobYFCUhpJAE1iCy5CaFfkopR8IzuI0pi5mbATwHYIHfIU0AJti2K3z7lHRhOBI4zt5mDSJLblSZx45oKouuSE8gojwiKjCvAVwMoN7vsBcBfN0XFbUQQCsz7x12a5XkYagSOFIDdRQUiwaRJTeqzGNHyGio4wcSvcHMF4TaF9HNiaZAZhOAlB55gpnvIaJvAQAzP0JEBOABAEsAtAP4Z2YeNNRJo6FSkKGEsNTWDlxRz2zX1g68voYxnRBoNNTQGSwaKmRtKCJyAhgBYAwRFcNyZBcigKM5Eph5J4C5AfY/YnvNAG4Zzn2UFGAoBZkiWXwiVC0slTBpg9b2ig3hFBL8JoDbAYwDsA6WsmiDjPiVVCQdhGOwtbr9DNRuN1D3fBUaUInKka2YU7If9S2laDhchEo0oGbOp3C96Jt12M1ZOutQlONEYoa6lZl/EeP2RAU1Q4UgXUwyts/h9k5D3capaDiYh8qLpqPmprLjE4aVK4HibWtQxB5sbx+HNU0VOKOiEVNz96CViuHxOrFi9p/hmu61rh3MnDWMpqa6blbSn2El5RmY+RdEtIiIvkpEXzeP6DVTiRvpEozu8za7O2dg5Rvz4EExKi6YAU9O2XE/9/GPOn8yMjq92OMZgcKcTjQdGoGMTi+K509GcXcL6pr8gvCiGG+piWJKOhD2SnlE9DiAqQA2Auj17WYAj0W/WUpMicTWP5QhcTyH0S4X6spcKP47mzvC95ZpQkUFgIxSYNEitD7Xh0K0orW3CFi0CCgtRVFJNhpanP2vG8V4y0iWDtEZiJKsRBI6Ww3gTGa+mZlv9T1ui1XDlAiINCQ03GD0oQyJEzCMHiwRq99HLS1F0cxytJXPRNHMcqC0FADQWjEbldl7YhZvGW6imM5AlGQmEmVRD6AsVg1RhshQJMycOcBbbwHPPAO8+Sbw6aeBheNQzFUJMHENpvv84+7HjQPa2oDx4216IbMENT+cE7PkiXB1c7pYB5X0JGwzFIAxAD4hog8BdJqdzHxl1FuVSiTabhDp8qhuN/Dii6JY6uuBpiZg2zbg//7fgcdHYq4a7ByvF3j++Zj1UU2N6EfTPOOvv/562ZeXB7zzDsAMLFwI3HIL8NJLwIcfitJYvhxwLZ0OLK2NWpsiaZ9hKN2tKPEiEmVRG6tGpCz2qKJ4h1waJfWHP8hw+eSTj5tVBpUwdXVAb6+MoMvKgEmTRHo99hhw0UX92x1maGo//M/Zt08kdWFhzPrIZFXbdbYRxObrueIKafqOHaIf584FzjlH9r34IjB9+vCbE2zcEKx9/vcbSncrSrwIW1kw89tENBHANGZeTUQjAGTGrmkpQKSj+mjgdgMPPQSsXg04ncDRo8DHHwNbtwLnnitKYzAJ09AgisLpBHJzZV9REdDSMrDd4Q6J7fifs2GDvJ4/37KtAFHvo0CJWLW1A78eU5D41FOtfdFoTqhxQziJYkPpbkWJF5FEQ90A4EYAoyBRUeMBPAJgyOU+Up542w2MRNq2DXA4ZNTe3Q1kZYkB/q23gOxsIDMzuISprATWrAHslXm9Xtk27bYPkUeMALq6RMEEGxLb8R9Gd3XJEN7MeoC42VYCfT2dnQOPG25z3G7gttvkGn19Yu4iEt3Y0ADcf394iijcGYiiJIJIzFC3QCrC/g0AmPkzIhobk1alCvG2G9TVAT09wOefy4yCSGYIWVmiPA4fFhvLYNKppgZ47jlpZ1GRKAqvF5g6VdrtP0QeSsKefRht6jfZiZNtJdDX43AMPG44zTHd1dAgX0lvL3DsmOjYrCzZH4nVTUtVKMlKJNFQncx8vLIsEWVh6GtYpAfxLnG5caM4pTMzRSoBQHu7jN5LS8UQP2VK6JH/D38ow9+WFlE2s2eLZKupiX5ITgLLgAa6dUkJMHbs0ArcBopONt3V1yfd1dsrX4/Z7utLnYgmXVtcGYxIlMXbRHQHgFwiugjAswBeik2zUoR416s+fFgkUHm5JYkAURZer8wEwhkiL10KPP44cPXVMqOYPt1qd7RXj4lCHw1ViAW69b33AvfcE1lzBotONt3ldIr+7eoSZdHVJdtOZ2pENGmOhxKKSMxQ3wNwPYCPIcUFXwbw61g0KqWIp91g5Ejg0CGRRhMnArt3i1mqoEByJzIzwx+xB2t3LExrw+ij4QacBbt1JM0ZLI7BdFdZmbiLOjtFbzudMoMZPTo1IpoSEauhpBaRREP1AfiV76FEi0jyNKqqJGmgqUmGrpMmAQcOiMJoapKEAUCG30PNaUiykJyhpJGE052RdPtgcQy33y7dNX686PHSUqC5WcxdRLI/FSKaNMdDCUXYZigiOpOIXieiT4loJxHtIqKdw7k5EU0gojeJ6BMi2kxE3w5wzGIiaiWijb7Hj4Zzz6TB7QZuukmk1CuvyLA01Ny/pkZmD+PGiRmqqUlMURdcIP6K3/0OuOOO0LaEwew68TathbAxRWIVC9eUEqnJZbAMbNNd06aJu6i8XHT6uHGyPW1aahTz1eVIlVBEYob6DYD/A1nTojfEseHSA+BfmXm9b3nVdUT0OjN/4nfcu8x8eZTumXjsIbCjRsm+NWuAM86wvKHBbCdXXgncfTewZw+Qny/nf/opMGaMDGkBoNpXYTjQMDwcu06sTWtmWL9xI7BrlzjYTzopYFsisYqFOwsJ5zj7zCMnR/TylCmBJ1vpEMGUZBPKhJPowgzJSCTKopWZX4nmzX1rae/1vT5CRFsg+Rv+yiK1CPVLM9Kqq0sym4lk/9atkpMwWPXX558Xg/jo0WLzIAI6OoAtW8JLIhiqcdr/M82ZA/fqZtStGYcGmojKheXH15AI2TdGWXk80v7Nm6UfysoGtGWAENvRAk99E66f/EegNrNf327cKJdsa5NjZ84Uv4F/dwarSPLCC/KewwF88YX4/k30sHFeh5tukmpojodFIgszJDORKIs3ieg/ANShf22o9dFoCBFNAjAPvjwOP84gok0A9gBYwcybA5x/IyRpEJWJnDuH80sz0qqoSAR9bq4ogNbWwau/GsHOLNfNzhalYc4NJ4lgKMZpc39TJuTNN+E+MA4ri+5C8bgRqMA+eN5qxspGB1bcUzz4H8qurIxU93qBtWvhximoa1yAhq5yVO7fd1z5HBdiGw+hcte7uH72p3CdRP361g0Xdu0S3WO69YMPZNIyfXr/JvjPVvbv71+R5NVXpWkVFVb08NSpUV0LKSmJdIaUrqNvdfYHJpLQ2dMhZcrvBfCfvsfKaDSCiPIB/BHA7czc5vf2egATmXkugF8AeD7QNZj5l8xczczVJfbs5HgTTp6CMRDPmiWCsqNDHjk5oau/jhwpErGkRPIk7OeOHSv7B0siGIpx2tSTqq+X9jKjrvcqFB/cgeK+g8gYkYviwj4UN2+Tj7lqFbB4sRjsFy+WbYPdCWEURU8P3J86sHL3Uni4GBXOA/C8tQkr7/TA7ZY/aG0t8GjV/ahd/LasaOfXt3V1EhDGbEUjmUmLf3f651+s9w135s2Ty3Z1SYDZ1q3WOers7U86h9pGO3o8XQhbWTDzeQEe5w+3AUSUDVEUf2DmOv/3mbmNmY/6Xr8MIJuIxgz3vjEjnF+akVY5OVIGFZBQmnnzAs917decOVOk4YgRMhRubwe2bxeBP3488I1vDO6cHkqSnH89Ka8XDTQRRWiVaCwAcDpR1LkfDW/vAr7zHckJKS+X5+98x1IYdmVllGVTE+q4BsWZR1AMDzLGlfVXPkH61r1vLGo3XoVlf7gAzz8vLpxFi6SJZtIyefLA7vT34Xd2ivXPWMHMLew6VZ29/Unncurq7A9MJLWhigD8GMA5vl1vA7iLmVuDnxXymgRxnG9h5v8KckwZgP3MzES0AKLgDg71njEnHI+sv4H40ksHn8Pbr1lWJo7wDRusAkRLloiD2JRQHcy46n9vh0MUz333WbYEoL99weGQWYyZsTmdqDzSAE/WKBR7fetWe71odZSictdbwMhCmQEB1vMDD0gyoN0JUVIi04EvvkBDxiRUOI8CJRNE6jOjqHV//9GcrR/c+8Zi5QeLUEyHUTGuF5u9Yko691yZzAADvwb/bghWkWTWLCmzVVgo+vREd/YGItqhtslk0lJnf2AiMUM9CuAIgKt9jzYA/zPM+58J4GsAzreFxl5GRN8iom/5jlkKoN7ns7gfwJeZOXnLjIQ7cne5ZF9lpfxD6ur6z+HtIaX790ttbXNNhwOYMQM4/3yRjNOnSyTUX/8qYbgXXwx861vBbQLGrnP77VLIyOGwbAl33inht59+Kvf83e9kkaSGBoneOnIEyM9HTcbz8NAoeDJHo6+9A562DHjGzkBN7yqRsnYKCyWcyNzbPqyfNg245hpUzhyB1rIZoigAS/nYR3O2vq3bMhPFdBjF7EHGybMwf74csmFD5FVF/L+ynBzRvfPmxSd6OBWJ5ug72Uxa8Y4eTxUoXLlLRBuZuSrUvmSgurqa165dm7gGhDNMsjut7cOXFSvkff/3du4UM1NXl3XN++6Tf1dzM/CXv4gpKydHTFNZWWKPufxyycOor5dwocOHZbRfVSVKKCen//D7lVfkfFPoqKVFJGhnp+xjFkk6dizcO0agLmMpGnJnWtFQty227mEw22+9FbS/3Hc8hZU7voTigl4U4TBaj2TAc9KCgQ5zX98u+8MFqBjXi4yTZx2vaLt3ryxoNG9e5KPTZBrZpgKD/Xwj7Tczs7P/DM12OgcUJCNEtI6ZqwO9F0k0VAcRncXM7/kueiaAjmg0MO0IJ6zk4YdlGGwWWKiokNmCMfr6h2NMmTLw32PMMlu3ygwhI0MEc1eXzBYyMoB335XwnlmzJB40I0OUSmcn8Le/iSmovFzeLy2V/S0tUk5k/35ROlm+n8nEiZIU2NkJnH46XP9eA5f/51y+XHwUgMwo2trk8YMfDNpfrnuBFQ+9Z4XiLi7H9TfJ56+ttQtxF1y1LlRioIBxOoGrrhqagEmHXIl4Es1QW80eTw0iURY3Afidz3dBAA4BuC4mrUp3Vq0CnnhCopgcDpkB7N4tw7NjxyTRLpx/jzGuNjeLeaijo3/Z0yNH5LmiQsKCSkvl/Z07ZRsQB3NODvD+++IddjjkGKdT3nM4pJzIiBGy/7LLZG4eTCIvXSol1H/xC7nPqFHArbfK/sFwueB6xAX/SUSwKGS1KyeeaClYXSEwNYikNtRGAHOJqNC37R/ieuIxFNuF2y0Z2D09kicBiILIy5MR+OHDYiLy//eY9UCXLet/rxUrZJ+ZWWT43FAmQa+3V/a1tUlOxvbtokQAUSodHaKonE5RYsXForxaW2Vfh2/yOGqUJZUH+xe73cCmTTLEN8dv2oTjMbARMFi8e22tJpGlC6r4U4NIoqFGQ6KhzgLARPQeJBoqeSOTYslQ0zzr6mR1O4dDBDqRPI4elVnAjh0SHmuvL7Fjh2SYLVwY+F6TJ0t7+vqsdS4MzFJSIzdXpOqxY9ZSboAokvZ2Oa+wUEqFfPGFmKCIRKkBMpsAZLZwzz2Df77hZDTZFHDDhltRsWAcAGuVPfsEK5yRbTR9EerXiA2aPZ4aRGKGegrAOwD+wbd9LYCnAVwY7UalBJEIRbuUWb9eBHdurozsvV5RGj09okDGjpVne32JpiZRFCYV2X4vQHwfZkjmryx6euT6o0fLtZhFQdiVBSAzm5kz5R4lJdbM5E9/EiWUny+mqFABEeEaoANJXqCfAq7cvA+ed5pRvLjquBM7EvPEUPR5MIWgJSBiS7L6jHSAYBGJsihn5rtt2/9GRNdEu0EpQzhC0e0GHnoIWL1ahHVVlSiC5mZRFN3dIsyZZdvhEBPRnj3iZG5qEnPOrl3yesuWgUWP6urk2rm5YsICRBGY2YPxiRgndV+fNZsB5P5Esn/mTOtzmCJINTUDw1QGmyWEY4D2RT/VtZyNhs4rUbl5P2rWPgXXBE8/BVwzbxdWvl0NrN+FoktKIzZPDKW8eTCFEM0SECqAUgMdIPQnEmXxGhF9GcAzvu2lAF6NfpOSgHD+zaGEotstOQvr18sMobVV/AWFhaIQRo4UId7ZKYJ95EgZPbe0AC+9JAogP1/yHTZuFOVQVmYVPRo3Tl6/+65c05i0zAp6ROJnyMqSdjJbGd/d3fIwCoNZrvXqq5aTfOJEiZry/9yhwlQGMUAfLzb7ZDZ2HbwZs8sO4qTRHngO5eCOd5dggvczdBaXo7KiFzXVX8BV1owV53yIur9VoGEIBfwijbIZTCFEK2JHBVDqoDWi+hOJsrgBwO0Afu/bzgBwjIi+CYCZuTDKbUsM4f6bQ3nlHn5YlINZZ/PwYRnFt7eLEti3TwR7UZE85+aKcPZ6RYj39so5n34qisLkKjid8vqvfxWBfuQIcPCgHG83EWVmin+iy7ds+rhx8myXbn194mTPyhLl0dBg5UccPChO8REj+lfiC2UHCmKAdsNlFZs9TKDMDGw+UIrCvsPAvgPY0bkALb0jcAmvg2d3Hla2zcOK8zfA5dwG15cOALVfiux7hKXPu7qAjz6SyZLRg4H87UYh7Nsn0citraJfTeL8q6/KtczkzuGIPGJHBVDqoCG9/YkkGqoglg1JGsL9N4fyyq1ZI9XoOjrE7GTMPd3d4szu7BTBP2aM7P/sMys8NTNTBP/YsSLhpk+XY0yUUlubpRi6u61UWvtMhch6DYjwHz9e2rl7tyionh5rVuIrEIgjR6RNeXlWJb6SksjCVAIYoOtqbcVmqQhFmUfhRS627ikEevNRkHkMXRkFyEAfirOOAscaUbdhMlwz1gw5LKamRiZ39fXSZVlZ8hEPHpQk9Xvv7d/MykrRzZs3y1dTWCgfee9e0eNtbfKVtrcDb78tlWjvvTeyNqWTAEp3c5qG9PYnkpkFiMgFYJL9vEDF/1KaSP7Ng3nljJAeM0aii/r6+r+XkyP7Pv9chHxfnygRQEb3EyaIwG5pESlVVialPfbtAx57TI43Mxdz7WPHRJp1dVmO7Lw8eb+tzcq5cDjkXk6nvH/kiOU3yc4WJWIUyrhxokyGsZCD2y1rRRhrW+aoInibj8GZ04HWzlyACNnciaJiAOMmAC0tKGrbh4aus4Zkn7GvrbR2rfzhMzKsbs3MlG711/81NcDXviY60qSZMFuBa4sXi9vIzDgmTIhcOKaLADoRzGka0tufSEJnHwXgArAZgJF8DFnfIn2I1r954UIZfnZ391cUgGwbYW4iorKyZH9pqUgnkwMxZowI+vHjxZexfbslxewYp3ZnpyzisGePXMPc7/BhOWbfPmt/To48269FZIXSmsUh2tuHLAWMUMnJsVwjbT0jgLwKjPB6UJTRCC+caMspwanjGsVEl5mJ1rJiVF46D4jwlvalN3butCZXeXlWoJhZ/sNf/5soZPsCSvPni4uos1O+Gl9QFvr6rGjiSEgXAXQimNM0pLc/kcwsFjLzyTFrSbIwZ44kzXV3i/ll/HgR5JH+m2++WaTJ229bgtxOV5dIsLw8yyTV1maVHwfE9nHRRVKL6bHHRAHk5IiwN/kPBrtZ6vPPZYZQViZRVGZIn5EhksmUN7ebtIyfxPhMTILf/PlyzyFKgYceEh//3r2icwoKpFszHE7wyHIUjxqBsqaP8MlRB9bvLUNnF+DgTpScXIJ7ayK+3XEhtmmTTNjsVrbCQqmobgLPAun/QPmQg60pFakpJl0EUDqZ0wYjWUN6E0EkyuIDIjo5wPrY6YPbLSW+58wRQd/SIiPyH/4w8l+MyyUG7bPPtory+WNW2mlvFwE+ZYrcr6lJ/om9vRJ2+8orUrvJ1HUismpK+cNsFRQ0CXi9vZbEy821ljQtKBApamY/OTn9HeUVFTKU7uuLWAq43eLjf+wx2c7Lk0nD0aNyy7IySRh3uYrgXlWJO+/oQ7OnC8gYAYwdBSrKC3n9QELaCDFjKhozRqxsR4/Kxzp2TPTj1KmBK9IGGvmPHWstTmifDZx99tBMMekggNLFnKaETyTK4jGIwtgHWVaVIFFQKf6zt2GfW0+bJvs8HvGQhqptFAiXy1prOydHZgMmr8JUcDX2jspKkaYFBTIMf/ddUSD5+aJA1qyRgn/GmW1CZANhZh69vVYhQJNbUVBghQU5nXKPY8fkvMJC2fZ65f3cXNkfoRQwpqBt26ymdHRYkUU9PeIKMQKzrn46plwKnBoknSPA8t948cXAQtoIMbO0an6+dNuhQ6ITMzNlzYubbw4ssAON/E3Cuv9s4EQwxQQjXcxpSvhEoix+A1l74mNYPov0Ihpza3/JNmaMmI+ys0U4mxyHnByRYh0d8tzbKxFRR4/KIyNDBHtPjyiFjAyJqjJmopyc/tFOxnRklENPj+RZHD1qFQJsa5NrdHVZ0VDGGW5mNUSiPLKyxDH/2WciCbq64B69GHXtS9CQdzIqz5+Kmh/MDigUjRA1OtJMVtrbZYbR1dW/gvlg3R7IkXr33eKWCSSkTQTUwYOiE3Ny5FqLF8tHCsf1Emzk77/PVIgP1O5EEo8opXQxpynhE4myaGHmF2PWkmRguHNrf8n22WcisUy2thnNjxsnSuTSS8Xh/N57olByc0Wom0J/XV2Wv+PIEXk9erQMk02Ird130dcn293dcp++Prkmkdh97M5t48A27zud0ububhmO2xMUsrLg3l2IlX3/D4ozW1FxZAs8f9qPlfvzsOIXk4LmKxQVyWVNVfPOTlEWFRXiGwin2wON3ru7xVJnT/+wC2lm6caxY0VXdnTI+zfdFF1hloymGLuDv7FRJqTPPSeW1KFMjgcjHcxpSvhEoiw2ENETAF6CmKEADD90loiWAPhvAJkAfs3MP/V73wExgZ0KWU71GmbePZx7BmU4c2u3G7jtNhm5jx0rj23b5Dpm9J6ZKUK7t1feN0bzl18WSdrRYQl/Ihkem1lFZqacd+iQSEKTbGd8EmaGYXwjZvZgsrRNEqAxXZnzjh2TYX5bm0jy3FyZAthnJh4P6jJ+guKso+jsc+IdPhutXQXI+agNDy37EI+4Huk3hDVCdNYs0YGHDllRuoWFwKRJ/f0Fg3V7oNF7SclAl41duUydKvUQDUag2wVbNEbf9nZ7vRKqe/AgcOGFgZP+wl0TazjtqquTr7W+Xvq8pET65u67RbmqcFeGSkYEx+ZClMTFAK7wPS4fzs2JKBPAgwAuBXAygK8QkX/E1fUAPMx8EoCfA/j34dxzUIa6nqIZzjU3y4yho0OioPr6RDmUlIgEI5JwnMWLxRDuckkW2MGDlh8DsJL3TCJfb688CgpEsBtfBBDYcQ7IuSaz+9gxka72KCeHw/Jl5OcDP/uZlWl+4IBVXqSrC/B60dBbAW9vDj7oOhUdcKKw9zC4owOrN4yB+4Wd4q2+4w7A7T6+TGlODnDeedZlCgr6f/Rwur2yUoruvvWW5Gq89ZY0Nzs78Mq1DQ2icOwEKtkVjWU8Tbu7uoA33pB9F1wgXet/vXDuGY12NTRIHzqd1qSxqEh+TqbupKIMhUgyuP85BvdfAGA7M+8EACJ6CsBVAOwRV1cBqPW9XgXgASKimK3DHcnc2gwDX3hBJOOIETJDMEl2Bw5YJqPycpFm3d0SJmR44AGRoiYhrqtLlI4xP9mTA9rbZZ+pUNvdbV0nWHju4cMieY4dk/b09Yn0KCiQtvb1iU1o6VIZjr7yirWmBSCfKysLlb078UrfpXBSJ3L7OgDuBQEYneFBXdcVcO37f+UeDz0E1yOP9LNnL10aXkhpoPfnzJGIqsJCeRw+LNe85RaxzPnby+2mIVO2w0z2zGg/mo5pl0sCxv7u7/qbo/yvF849o9GuykoxPZWUWPu8XtlOtC9FSW0iScqrAPALAGf6dr0L4NvMPITUpOOMB/CFbbsRwOnBjmHmHiJqBTAawAG/9t0I4EYAqIyH0djun2CWR3OztQhRR4cI46IiKUZ04IAI49GjZeUeIz2bmsSHsWePXDc72wq1JRIlYsxO9hlFZ2f/9vgUhRtzUIcaNGAiKvE5ajJfg6vCITahujpLQZh1tk87zXIg1NQAv/+9mJ+YRcr4MsBrmlfh933XYBQOgdEDL5zwZozAwpxNaOAJcq0DB4BnngHKyuCqqYGrdvg2j/p6yW/cs0fMKSNHAiefLIqitnbg8cY01NIi5xqf//jxVsRUpHEMoUxD4VwvWseEoqZGfBStrXKu1yuPqVM1rFUZHpGYof4HwIsAxvkeL/n2JQXM/Etmrmbm6hL7sCpWPPyw+CTeeUeGu4cOiQA2wt3kLpjSGTt3isA3zmNjXxg/3irDkZ0tysasbpeXJ+ebZD3A8lUEwI05WIkV8KAYFWiEB6Owsv0muPeWyPD3nHOstTJOPlmM66NHWw4El0uSAInknrm5YleZPx+u/J24KOMvIDDaUIRceLEoey2cmV2opAbg6FG4O2egtuv7WPbMEtR+bTvcqz4ddjc3NEhO4uLFUq198WLZDiZAjWlozx7pupEjgTPPlEjo4mIR+pWVVgSyIZhjOhzTUDjXi9YxoXC5xJnNLArT6ZTIsayswHklw8HtFoW9bJk8R2rGU1KLSJRFCTP/DzP3+B6/BTBcqdwEYIJtu8K3L+AxRJQFoAji6E4cbjfw+utW2W+Tu2D8DmVlIvhN5VdTeykrS7aNraGuDrjiCsm43rnTKgNi1p8wysdgz6swjmsbdahBMTwoxmFkZGWiOKMNxfCgrvMyOWD2bBH+8+bJUHPatIE+mZtuAmbMEMVyzjlihmpsBBYtwk3zPsCM4macU7Ae52SvQU7PMXh6ClDT90e4e2djZe/t8IyoQEWJFx4ahZV3tw9bgAxFgLpcEgm8dKkoF1Oiw4zSjU8lkM/DH7tpKCOj/1dnCOd60TomHJYuBR5/HLj6avmap0+Pfs2maPl9lNQhkmiog0T0TwCe9G1/BcMX2h8BmEZEkyFK4csAvup3zIsArgPwAWQNjb/EzF8RbiiKWXAIsMJZGxvFxJOdbTmkMzPlYYS8GbG/9ZYk3h08KIph9GgrtbizE5g7F1i3LrByMKU5AnRBAyaiwlj1ensB7kMRHUFDxlygb5dI2aws4P77g0uOQAH0o0YBTU1w5R7ClaUVeODzK9DUMwrj0YTl2Y/B5V2P2r4fonhEJ4orSwECiotkaDvcBLWhBqgNFtYaSY5AKNOQ+cm0tVkV3quqBl4vnHuaYx56SJIOicQENxSC+YCilYNxIicknqhEoiyWQXwWP4cUEHwfwDeGc3OfD2I5ZBGlTACPMvNmIroLwFpfXsdvADxORNsBHIIolOgTSRnNhgYpOfruuyLQs7LER2HyG5qarMQ4IstxbRYeOnhQHh0dVt2mvDwZAr79thRTsofQGp9ICCrxOTwoRjEO+2Y5mWgtnIDKUUciqxprJI2RLFu2SFJeRhVePHoK5ubU45z8NrT25uHFzC9hev4ONPTNRsWUbDGzAYDXi6KS3GE7VYea/BVKyYQbxzCY0rH/ZFwu6x7BBHC492xvlyxz0+5oVXONZqXYE6U2lGIRibK4C8B1zOwBACIaBWAlRIkMGWZ+GcDLfvt+ZHvtBfCPw7lHWEQyVMrJkUUPxo4Vw/DBg1Zk0tGj8jojQxRHfr7lnDYJb4cOybFOpzyOHBFT1jvv9I+AMvjPJDIy4O6d1d+RjTrUoA4rsQIAUJTVgdayGfDkV+D66rXAI49G1h9m6dPP56Ph0H/A0X0EH+E0tFM+xvYewizaitJxmcBVV6Cu6zRUvv8+PL35KDaOca8XrSdVR8WpOpTkr1BKJtwRdjClc/bZVlqNqfvY2ys/jYceAh55ZGifNZYj9mheOxkTEpXYEomycBlFAQDMfIiI5sWgTYkhkqGSMQmNGGGlKPf2WiU9TIkOIvn3OJ1idvriC1EKra0SIbV3r9gvenokkshEKhnfh3ltx+mUVed6b0ExDqECX8CDYqzECqzASqzAf6Eu6x/RkDkFlUc+x/WnfwxX1+cR2x/cd7+Ale+djuKOvcjuzsBbOA97UYYpvAMdcOJ9WoRFXdtQUgQ0NJbh9h/Owcq724GWFnhzx2Bj38k4+Lc8XFgQOEEt6H2jZCYJdY9gI2zAWgvDLE5o8kTM5Ozss8VM1NwsX+3u3XLexInyda1eLWkn9fWhP4f/5924ceBx0RqxR3M2oLWhTjwicXBnENHxcYRvZhHR4klJTSSe1M5Ocf7m5sqswOQ9ZGWJdCkpsRzcTqfMPnJzxZm9ZImsydnVJdfp6pLzjx3rr2SCmZ28XtR5l/gc2a3IIEIxWlGMVtShBi64Udv3YzzK/4za9u/C9dYvRElF4o10u1G3uhDF3c0ozjmGbZiBQrQiD8ewH2XIpU44M7uxpWuq5QdYOh0rHq9C17kX4Y3W04AReUET1IKxapUsPvTMM5KI99lnQ3eaDuaAtdeueucdeWzbBvzbv8kxn34q8QaHD8vzkSPy9dx+u0T91NfL+WPHAvv3y2d0OGSCaSqo3H136O4O1MZdu2TJEjvRGrFHI9rKMNT8VSV1iUTY/yek6uyzvu1/BHBP9JuUICIZKuXkABs2WGnJR4+KlDA5EBkZEiWVkSHhKOedZ4W0rFwpeRVbtoiyMFVo7fj7KOzObRhHdiMAFu8REYrQhgaeKMeZkF0iaeP69cCCBTKENAH448YFtz/U1aEh61RUeLcCWVlozShGYd9hlGEfdmEyOtgBR14WmjFmgB8gnAS1QLjdkgDe2irNNsuZnnba0Mwkg5lcGhpEl69ZYy2f2tEhM4IlS6wyXbm5sr+pSWIOTDvMCH3WLGm302lFPXu98vPYv19yNY2F8eSTB36OQG2cM0eUUaQr2YZDtGcDWhvqxCLsmQUzPwagBsB+36OGmR+PVcPiTrhDJbdbpEdbm0iI/HxrhbrsbPFLmIp5Z58tw7aGBivW8sorZeja0iLSJZCpyR4uC1iVZH3lQCrxOVpR6HuTAe5DK+ejEp/LLiLL8W7KvK5fb9UJ7+gQibRx48B+cLuB559HZd9utHbnYl9HITwYiU8wC/tQirKsg8h1Age6CzF2Yu6ALgqn3EYgHn5YzDpmYSJAXDtbtw7NTDJYO4y5p7dXhPq2bfKV9vXJc2urKADAWlXP/hnMCL20VEJ0e3rk6+zokJ/AF1+IVbGjQ7q/o0NW23v77dBtnDpVVuszP8OuLrF23nff8HMZdDagDIeIzEi+hY/Sd/GjcIZKdXUiISoqZHbQ3S3Sw1S26+0VI7fDIdVk//QnkUKFhcDTT4uCOHRo8Ogmk5NhZgfmuj4/Rg3qsJK+AzBQhFa0oggeFON6/EaOzcyU4a1ZT9tUvTUlPHJzRaEdPtzfaO5wiKRzOFBT9j7u6LgeOzrGIx9H4EEBjmAMjvR2IyMrF1NOKcC9DxQM6K6hOj7XrBHBClgBZIA064orBj8XGGj7z8mR+wZqR00N8JvfiBB3OKRbOzulmxoaxPfQ0SHd5PVao3DzGewj9EmTxFzmcIiQ7+mxCifau7uvT0xM4fRVVZWlGIxvZexYOfaOOyQQr7NzaD4dnQ0oQyUSn4UCWMPB0lIrrfirXxVT0y23AJddJjOKpibLiG2q3m3bJkNZU9AvGMZvAVgmKWNaAuCizViR8wsUw4NGTEAxPFiBlXChXs4x0VRHj4odxGybUN2ODlFau3eLtHnlFWnj+vXiLKishCvrE0zIP4xCRyd6MnOQRYxCRxcchbnwjJgA8h8SQ4Tb/v2iH195RcxI4SaWmQhiEzRmSmD19IQ+N5Dtv6lJPkqgBDdjLjNxCTk5IvTHjJGw1XHjpIsOH5bn8eP7fwb7CH3HDhHeU6dan8GUZG9ulvOOHbMme3ZCJeH5JwR2dsr91q/XRDgl/qSPgzpehBoOAvLc1iazA5OkZ8xNpnT4YJjS4k6nvDalPgxZWXB1r4MLH/V3hhvpZxQCYN0/K0skjZFcxvk+ebIct2aNSKPCQpklTZ+Ozh19uASv4Z2MM1E0eRRyxxcdX9xvypT+Nnj7KPiCC8TM88YbUlEkHFPHwoViphkzRnScWRH2wgvDm+z52/6nTJGuLi4OHDpLZMUmmNVqnU6ZVUyfLkrDRENNmzZwBG9G6MZ/Yb7SffuAtWvl3sYq2dsreZemq+3XGCy81z96aetW8X+Yn5AmwinxRJVFpITjJWxosExJvb39V7QDrKFzoKgn4yjv6RHbg9dr2UiysuS1PQ/Dfr5Z+8JcOzdXsq/7+mSYn5kpysAMZTs65Dk7G+7e2ajzXoqGgzNQ2bcbNaPfQmVRKzw9lWg9MgaFbc1AkQPezHwUFQ30Q/gL7PLywOtIBOPmm61lz7OyZORfUiJ1jkIRLCS0sTFwscFVqyTiyFjnzMOkxNTUBD4vEP5jh61bReHt22fFO3i9MuO67baB5w9mFrJfe/9+uXZPj7Rz/37pI02EU+KFmqEixeUSJ/WmTcCTT8qzCZUxFdVycuQf3d1tLXxkx8wyzH77TMM4y3t7rTyMzEw5xl6SPBDmmllZMgQdM0Ykril/3tPTP3LLt5iSu3UiVh65AZ7OEajo2gmP14mVh5ZhjnM7PF0jkJOfjQ7komPvYXi9Evnr74cYqmMbsPwNWVmi3yZNkkUE7703/CzrcEJCTeG75cutZULMV+VwyEyit1dqKi1eDHzpS6Gdyv6mpOZm0cfz5ll63umUr2LTpshMRuban30G/PWv1v6CAuD990VhaCKcEi90ZhEpbrdkZM2dK7kW27cDDz4InHGGGK6NwbyiQqKevF5rtA8cL/l93ExkcjPa2vovowpY5itTX8rMHIykcziAzEy426dINnfGFFRmfIEaroMrq0GUS2OjPBs/yahRImHa249/pDrvpSjGfhRnHgGQh66sPGxrr8C/ea/Hwso9mIhjWL+vHKP7WrFwodzWfzI1VMd2pCUzAhFosmd8CcuWSRvmzJGvzeRXOJ2iN80a4cYnAFjJdsbv8fvfSzHeQMuy+puSxo4VH0dTk9zT6PyeHnFZhZvdba85tXGjtK+iQrZHjJCfyPr1UvPR/j3EI6lROTFRZREp/vaWPXtkKGniL1tbRQKddJI8XnlFJIXTaXk/jx0TqTJrlthdDh8WZ7jB37lt1rcwq9zZlkZ18xysLLoDxXndqGhtgIdLsbL3u1hxZCVcjj3W/YzSMavjGYVEhAaulLwNysT+ntF433kOHM4egBxwFDjh9Pbip6e/gPqOKWjonory8oH1mYYaw2+WAd20yQpRHT/eijQOR/D5C+ycHPmIOTkysfJ4JElu9mz52kz0cFaW6NHsbCvy2eOxiv56PKIYx48XwWyvoxRMKNsXTXQ6rZmVPbs7VEa7vwL99FNpj1kqdutW+ckA/f1B0az9lAyo4ksuVFlEysaN8i9saxPJtm+fDPV27hRPqJFEGzbIL/2aa0RSff65CPr8fDlv0SKRZI2Nkho8f37gnAtAlExrq1XN1iyF6nCgzvN3KM47hmIcA3q7UTyyD+jpRV3b38OFB0VRGfuOb4nU415d370qM5rgcZSjszcTf+6+EMd68kA93chED945Oho5DuCNjol4eNVoIMifNZKCf3Yh8P778pFGjbK67uOPRQfv3Bm+4LPb/mtrRcjbHd7d3aLLp0+XRL8//9lyPtu79fBhMbMdOCDX6O2Vrmtr61+afDChvGKF+Cc2b5ZrlJfL19XRIY7uUA5p//HI2LHSrq1bxTxWVhbYH5ROlWDTTfGlA+qziAS3W4LlW1styebxiCIwKb9EVtlyU2t6zhxREEeOiBSaPl28kzt2iES8777gJT6IxEhtYkkzMuRao0cD5eVoyJiMoqNNMiz2xVgWZbejoWCOKK/Ro8Vk9tBDYksxpq7CwuMRUTUj/4Id3ZV4u+csHEMe0NWNVi5EB0Ygo7sT3N6B19tOhzuYpvDhcomgfvRReQ6mKOxhrqZLjLDOzZWPuGtX6HUkghHIf1JSYqXCzJ4tJiqjL82ky0zaOjtFpzLL7MDkWhgfTKg1LlwuqQJvZkkmMc/rlaC5UH4c//bPmmX5QwZb52I4fqNkI5x1RJT4osoiEurqRNKYyqpOp/wbjx61lle1S4U1a2T/tGmSfzFhggwLm5vFa/nBBxLUn50dcDEjALJ/714ZmppQXLMoUkYGKk/KQWuObyW88nKgsxOtHdmonEiiJKqrRXItXQo8+6zEp37966JEiouBvDy4MuoxIWsPCouATO6Gl5wocnYhLw84mDseVFiA0ZmHo/JHfegha4HBd96Rj5SZKR/RdF9fnygPu+Dbv18mdX/4Q2incyCHt1mI0DiiTTTyqFHytZiVZDMzLaViqrl4vSKwjQ8mHKEcaNHBRYvkJxPKj+Pf/tJS4JRTZIYxWOZ1NGs/JZp0UnzpgiqLSNi4UWwZJhZy3z4R0OXlMnz0lwpE1i++tFT2FxWJjaWpSewde/ZIESH/3IuMDJFuhYUSoH/KKXKumYHs3w9kZ6Nm0np4SmfA056DvsY98PQUwJMxGjWFqwNLFZdLPKx1dcA//INcc8oUdFZOxyUn7cDf574KyspAR08WPO0ONB8bAU9PPqryPhv2H9XtFpu9WWDQLFNeWCjKwXTfnDkSK7B9u6wT9dRT0ty9e0W3hkpGC5TslpkpYbim1EVfn5iI8vOlS0eMkNem3EhFhbRlxAjJAcnJsUbz4QrlQIsOhpOgGKz9998/+KwtWivtJQPppPjShYQpCyL6DyLaSkRuInqOiEYGOW43EX1MRBuJaG2cm2lhN0GVlYnwdzrFzrBkSWCpsHDhwCFiVRVw7bXih2hs7J88B4hUyMuTIe2IEaI0DhywihFlZsq5+flATw9c257Fiis+RXFGKxpHz0Xx3AlYcUk9XBNChBS5XFKQqa4OuPRSVOa2oJXEQJxNfWAmgIBMYqCnD0cdo4f9RzULDBpLXW6umIfa2sQBfMUVMhnKypLXa9aIrb69XYRfS4uMrkOZJILVQFq61DKTVVVZ5TyYxX+RlSUzjGXLJLHuT3+SEN7u7v56N1yhPNRaTPE+LxlJJ8WXLlCsVigNeWOiiyFLpPYQ0b8DADN/N8BxuwFUM/OBcK9dXV3Na9dGWa/U1orpqL7eWrSotVUkzeO+eor+oRuA5aWzhwgZD6hJEd69WyKWPB6RouPGWY5oEyI7c6YMtY8ds8qfz5xpxWnOnds/btV4QGtrw/p4xpew4YMO7Pu8C229ucgkYGL+QTioE1wyFo+vGjEswbNsWf9qr06n6Mq9e6VSbVeX1XV1ddLdTU3StoICmYGMHi1O3r4+2W98AJFGy9TWAn/7m0Q59fWJfs7JEQXy+OOhr6OROrFH+zj+ENE6Zq4O9F7CoqGY+TXb5hrI+trJS0OD2EYKC6WAoInztIekBPolBwsRGjlSakyYEJmjR0V6dnWJ19cUEzKFAA8dshQHIBljZWVyzIcfyozGToQGXpNr+NJLucjKy8Ko7nb0dAP7O4sweTIwYXpkiiLQH93kYixaZHVhTg5w+eUyybFz333S3dOmyXZHh6WfAdGbu3bJjGQo0TI1NRJbcMEFopBaWkSRff3r0u777gsdrhvsPirkooMWPUwukiV0dhmAp4O8xwBeIyIG8P8x8y/j1ywbRtKVlsoDsIa+JvMrkFQI9ouvqhIz0549IgEnTbIWQyooEJ+E1yv+iu5ucYqblOOCAlEUgJw7fnzwEqsRUF8vgVpANnJzxddirGRVVeFfJ1jY45VXWolx55xjTbRuumngNexJfrNmSYhtZ6foQI9HwlLnzBl6mKg91NfplDqQ9sS9oYZrasinkq7E1GdBRKuJqD7A4yrbMXcC6AHwhyCXOYuZ5wO4FMAtRHROoIOI6EYiWktEa1tMOEs08Tei2qOZhlICtKZGjORz54qB/swzpUbEs8/KULumRobN+/dbBZPGjhVpOXJkf0Pu8uVRMfA2NIhS8HqtArXMki8YyaWChT3W14dvU7d3d0mJCHJm61qTJ8vMw85wo2VWrx5euKbbLdbFtWslybC5OblCPk25E1OVRqvVKpGQMJ8FABDRNwB8E8AFzNwe4nAQUS2Ao8y8crDjYuKzAPrbF3buFEUhQ3EhQj9BUHuFMe6/+aaYnxwOkZTHjok0nzWrv4HfL6XYnVONOqpBQ2dZRGaQ2lr5CJ17DmLrR21oPZKBHGcG5i3KxSPPjgm7m5Yt61+JFRAd1tgozuVwGcycY9o6VDeNfQZg3El/+pOYpcrLI2+3ud7atVIHyuRqnHGGFfIayWePNoE+r3Gf6YxHMSSlz4KIlgD4DoBzgykKIsoDkMHMR3yvLwZwVxyb2R+7SelLXxIT1Lp1IhkcDjEN+a8nGs71jFQ0hnKzbKupTXHggCiK/HxRFIGKC/mu1c8MUhKZGaSmBlh5pwfF29finLF9aB07Ep4jmbi543nA/eWwpcpQ60QF+UjB2zqMJUIDZTuPHi3R0XZlEW67zfXGjrUWTgIk69rhSHzIZzpldyuJIZF5Fg8AKADwui8s9hEAIKJxRPSy75hSAO8R0SYAHwL4EzP/OTHNtWHCaPfutVbI8Xgk72LXrsjm98FW7tmzR2YTeXniIxk3TsxTXV2DXm44ma8uF7Bi/JMoLuxDY3cZikd4seLctXBNPRaRHSUeYY/DDRM1SV/79kkuxwsvSCzBnj1Da7e53qxZlhnP4bAWQEp0yOdwqwKr+UpJZDTUSUH27wFwme/1TgBz49musKirEyP66tWy7XRay5ReeGFkw7VgK/fs3Wul/xYVSe2onJz+w94ABFvbYTCh0M/cs34cak7fAVf536wD+iKPrAq3TtRwGE60TGWlFOjbvFm+vsJCmUWYFWcbGyNrtz3+wUR7NTfLTCMZTD3RqAqsDvsTm2SJhkotTBjtunVWGrLDIZJm6tTIvKzBpHtFhUgwfyNzCDtLpELBXxh89tFkfO2P1Zg8+giqyvehZuYWuBzbgl4gkF8BSP7Q0Zoa4GtfsxYkNMl5p50m+0xZD3u9p1DXM2axkhIrLzNaQnW44bjDqQqs5isF0HIfQ8PUIigrk8fMmdbrSI3zweoaVFUNyc4SqQnILgxaWoD6zumg3h54jmbB0+7Eyrer4d6RF/ACgSxod94J3HFH/33JuE60yyURVUVF/au05OcDr78eeftjmT0dqJ9DtcnfdAQMrX3JUqNJTWGJR2cWQ8EM08aNk3jQzk6RzCedFJmX1X4tYOCQbwh2lkhNQPaJzZYtgLM4F86CMWhrOoLi7k+AwlLUTbgNLlfZgHMDjTqbm+W1WXshHiPRoY66q6oGzsJeecWqsQhE1v5YJZFFOrofzHQUbqCeIVrBCsNBTWHJgSqLoWCXyO3tVtmOadMitw/EwMAfidCyCwNTed3rzUfRzHxg8VUo6gMaGgOfG8iCZlabsxPLkehQBYnbLc7t1atlVpGTY5UeueCC+LU/HCL1Q0XTdDTcqLNooKaw5ECVxVCJ5jAygXUN7MLAOHmZJT8QGHwUGWjUaaqR2InlSNRfkHR2Sgn0664DrroqsO62K5iqKomG6ukRd9P48eL0HjOmf5J8IkNfIx3dDyXIIRjxClYYjGh+HmXoqLI4wbELg+JimSTNni1RPMbfEWwUGWjUOXasKBuPJz4jUbsg2bdPkupNDmOwWYZdwWzaZGWC5+YCCxaI8tiwAbjkkoHtD8fkFeqYSM1mkY7uo2E6Sqb6VslgClPUwa3AWuHu+eel4ur06eE5QQM5de+5B7j33viVybbHB2zdai0jMnJk8BwTu9O2tbV/EeHSUqlb1dU1sP3hOJpDHTMUZ3WkzvPh5rkMpY2xRMuVJwc6s1D6EalFLNjx8RqF2kfdhw+L76GzU9JSgMDmCvtItajIKpZoFIjTKSYsf2dwOLbzUMcM1f4eyfcyXNNRsvkIksEUpqiyUFKAwUwidkECyKxi0SKrMHAgc4VdwcyYIcu7AlZ0VDATTzi281DHxMv+Phw3WDL6CLRceeJRM5SS1IRjEjFmtMceE+GfkxP+Cnbd3bKY0rnnDlwRz59wlvoMdUwqLBeaCm1U4o/OLJSkJhKTSCTmiqGMVGtqJOmwudlK2h87Vvw09mMGc0YnQyhqKFKhjUr8UWWR7iRTWMsQiNQkEmtzhX9Ff//tUAorFezvqdBGJf6oskhn0iD1NZnCJuvqJMzWZKcD0jb/WU4ohZUK9vdUaKMSX9Rnkc4Mp155kpBMYZPJUidJURKBKot0Jg2kWywL9EWKOn6VExk1Q6UzyWTDGQbJYhKZMwe4+26JmiopkdIgWVnq+FVODBI2syCiWiJq8q2St5GILgty3BIi2kZE24noe/FuZ0qTTDacFMftBl58URRGSYmUc9+8GbjyyuRQZIHQst5KNEm0GernzFzle7zs/yYRZQJ4EMClAE4G8BUiOjnejUxZksmGk+IY98+0acB55wFXXy35GfX1iW5ZYJKtZIeS+iS7GWoBgO2+5VVBRE8BuArAJwltVSqRLDacFCcZs5oHI9lKdiipT6KVxXIi+jqAtQD+lZk9fu+PB/CFbbsRwOmBLkRENwK4EQAqU8wmryQ/qeT+cbulKCQgBRXNQo7xVG4pnt6jBCCmZigiWk1E9QEeVwF4GMBUAFUA9gL4z+Hci5l/yczVzFxdUlIy/MYrio1Ucf8Y85PDYS3o9MEHUr49XspNTWDpSUyVBTNfyMxzAjxeYOb9zNzLzH0AfgUxOfnTBGCCbbvCt09R4kqquH+M+Wn+fGvVQodD1ueIl3JLg/QeJQAJM0MRUTkz7/Vt/j2AQK7CjwBMI6LJECXxZQBfjVMTFaUfqeD+Mb6VjAypvrtli5RuJ4qfcks1/44SHon0WfyMiKoAMIDdAL4JAEQ0DsCvmfkyZu4houUAXgWQCeBRZt6coPYqSsIJ5Quw+1ZKS+VhtuOl6FLJv6OET8JCZ5n5a8x8CjO7mPlKM8tg5j3MfJntuJeZeTozT2Xme4JfUVHSm3B8AcngW0mGNijRJ9F5FoqihEk4voBk8K0kQxuU6JPo0FlFUcIkXF9AMvhWkqENSnTRmYWipAhayFBJJKosFCVFUF+AkkhUWShKiqC+ACWRqM9CUVII9QUoiUKVhaLEGK2TpKQDaoZSlBiidZKUdEGVhaLEEK2TpKQLqiwUJYakwTLoigJAlYWixBTNjVDSBVUWihJDNDdCSRdUWShKDNHcCCVd0NBZRYkxmhuhpAM6s1AURVFCksiV8p4GMMO3ORLAYWauCnDcbgBHAPQC6GHm6jg1UVGUNCLdkyNj/fkSufjRNcxc5VMQfwQwWOT5eb5jVVEoihIx6Z4cGY/Pl3AzFBERgKsBPJnotiiKkp6ke3JkPD5fwpUFgLMB7Gfmz4K8zwBeI6J1RHRjHNulKEqakO7JkfH4fDH1WRDRagBlAd66k5lf8L3+CgafVZzFzE1ENBbA60S0lZnfCXCvGwHcCACVmvGkKIqNykoxzRQXW/vSKTkyHp8vpjMLZr6QmecEeLwAAESUBaAGwNODXKPJ99wM4DkAC4Ic90tmrmbm6pKSkuh/GEVRUpZ0T46Mx+dLtBnqQgBbmbkx0JtElEdEBeY1gIsB1MexfYqipAHpnhwZj8+X6KS8L8PPBEVE4wD8mpkvA1AK4DnxgSMLwBPM/Oe4t1JRlJQn3ZMjY/35EqosmPkbAfbtAXCZ7/VOAHPj3CxFURTFj0SboRRFUZQUQJWFoiiKEhJVFoqiKEpIVFkoiqIoISFmTnQbog4RtQD4PMqXHQPgQJSvmU5o/wRH+2ZwtH8GJ579M5GZAyaqpaWyiAVEtFYLGQZH+yc42jeDo/0zOMnSP2qGUhRFUUKiykJRFEUJiSqL8PllohuQ5Gj/BEf7ZnC0fwYnKfpHfRaKoihKSHRmoSiKooRElYWiKIoSElUWEUBEtUTUREQbfY/LEt2mRENES4hoGxFtJ6LvJbo9yQYR7Saij32/l7WJbk+iIaJHiaiZiOpt+0YR0etE9JnvuXiwa6QzQfonKeSOKovI+TkzV/keLye6MYmEiDIBPAjgUgAnA/gKEZ2c2FYlJef5fi8Jj5VPAn4LYInfvu8BeIOZpwF4w7d9ovJbDOwfIAnkjioLZTgsALCdmXcycxeApwBcleA2KUmMb0nkQ367rwLwO9/r3wH4UjzblEwE6Z+kQJVF5CwnIrdvunjCTpd9jAfwhW270bdPsWAArxHROt868cpASpl5r+/1PsiiZ0p/Ei53VFn4QUSriag+wOMqAA8DmAqgCsBeAP+ZyLYqKcFZzDwfYqq7hYjOSXSDkhmWWH6N5+9PUsidRC+rmnQw84XhHEdEvwLwvzFuTrLTBGCCbbvCt0/xwcxNvudmInoOYrp7J7GtSjr2E1E5M+8lonIAzYluUDLBzPvN60TKHZ1ZRIDvh2z4ewD1wY49QfgIwDQimkxEOZA11V9McJuSBiLKI6IC8xrAxdDfTCBeBHCd7/V1AF5IYFuSjmSROzqziIyfEVEVZJq8G8A3E9qaBMPMPUS0HMCrADIBPMrMmxPcrGSiFMBzRATIf+0JZv5zYpuUWIjoSQCLAYwhokYAPwbwUwDPENH1kKUFrk5cCxNLkP5ZnAxyR8t9KIqiKCFRM5SiKIoSElUWiqIoSkhUWSiKoighUWWhKIqihESVhaIoihISVRaKoihKSFRZKEoKQ0S3E9GIEMfcQ0RfENHReLVLST9UWShKanM7gEGVBYCXIGVGFGXIqLJQTjiI6Ou+Cp6biOhxIppERH/x7XuDiCp9x/2WiB4mojVEtJOIFvuqfm4hot/arneUiH5ORJt955f49lf5znUT0XOmWigRvUVE/05EHxLRp0R0tm9/JhH9BxF95Dvnm779i33nrCKirUT0BxJuAzAOwJtE9Gawz8vMa2xVXRVlSKiyUE4oiGg2gB8AOJ+Z5wL4NoBfAPgdM7sA/AHA/bZTigGcAeD/QGoY/RzAbACn+EowAEAegLXMPBvA25ASDQDwGIDv+q77sW0/AGQx8wLIzMDsvx5AKzOfBuA0ADcQ0WTfe/N8x54MYAqAM5n5fgB7IIsrnTeMblGUkKiyUE40zgfwLDMfAABmPgRRBk/43n8cwFm241/ylc3+GMB+Zv6YmfsAbAYwyXdMH4Cnfa9/D+AsIioCMJKZ3/bt/x0Ae3nyOt/zOtt1LgbwdSLaCOBvAEYDmOZ770NmbvTde6PtHEWJC1pIUFEGp9P33Gd7bbaD/X/CKbhmrtVruw4BuJWZX7UfSESL/e5tP0dR4oLOLJQTjb8A+EciGg0ARDQKwPuQ8uoAcC2AdyO8ZgaApb7XXwXwHjO3AvAYfwSAr0FMVIPxKoCbiCjb17bpvtLmg3EEQEGE7VWUiNHRiXJCwcybiegeAG8TUS+ADQBuBfA/RPR/AbQA+OcIL3sMwAIi+gFk4Z5rfPuvA/CIL7R1ZxjX/TXEvLSepK55C0KvR/1LAH8moj3B/BZE9DOIEhvhK3v9a2auDfWhFMWOlihXlGFCREeZOT/R7VCUWKJmKEVRFCUkOrNQlDSBiP4GwOG3+2vM/HEi2qOkF6osFEVRlJCoGUpRFEUJiSoLRVEUJSSqLBRFUZSQqLJQFEVRQvL/A7u/wrzbizcHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "combined = np.hstack((X_train, y_train))\n",
    "mal = combined[combined[:, 2]==1]\n",
    "ben = combined[combined[:, 2]==0]\n",
    "\n",
    "ax.scatter(mal[:, 0], mal[:, 1], label=\"malignant\", c=\"r\", alpha=0.5)\n",
    "ax.scatter(ben[:, 0], ben[:, 1], label=\"benign\", c=\"b\", alpha=0.5)\n",
    "ax.set_xlabel(\"component_1\")\n",
    "ax.set_ylabel(\"component_2\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b617ade",
   "metadata": {},
   "source": [
    "### Final dimensions of test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b827b2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (455, 2)\n",
      "Y_train: (455, 1)\n",
      "X_test:  (114, 2)\n",
      "Y_test:  (114, 1)\n"
     ]
    }
   ],
   "source": [
    "print('X_train: ' + str(X_train.shape))\n",
    "print('Y_train: ' + str(y_train.shape))\n",
    "print('X_test:  '  + str(X_test.shape))\n",
    "print('Y_test:  '  + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63535d4f",
   "metadata": {},
   "source": [
    "### The Model overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cce2cc6",
   "metadata": {},
   "source": [
    "![alt text](model_nn_scratch.jpg \"overview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1324519",
   "metadata": {},
   "source": [
    "## Doing it the \"easy\" way using keras sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "95555dbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x261ecdcbd60>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    \n",
    "    Dense(units=2, activation=relu),\n",
    "    Dense(units=1, activation=linear)\n",
    "])\n",
    "\n",
    "model.compile(loss=BinaryCrossentropy(from_logits=True),  # to avoid numerical instability\n",
    "              optimizer=Adam(learning_rate=0.01),\n",
    "              metrics=\"accuracy\"\n",
    "             )\n",
    "model.fit(X_train, y_train, epochs=80, verbose=0)  # verbose = 0 to avoid printing too much"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5610e0c6",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4df90265",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0594 - accuracy: 0.9825\n",
      "total cost: 0.059\n",
      "accuracy: 0.982\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test, y_test)\n",
    "print(f\"total cost: {result[0]:.3f}\")\n",
    "print(f\"accuracy: {result[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf855c6c",
   "metadata": {},
   "source": [
    "# Doing it the hard way (from scratch)\n",
    "Let's start with the math behind neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7843ec1c",
   "metadata": {},
   "source": [
    "#### The specified architecture\n",
    "2 features --> 1 hidden layer (2 neuron relu) --> 1 output layer (1 neuron sigmoid)\n",
    "\n",
    "#### Notation\n",
    "$z_{1}^{[1]} =$ linear combination of input (feature) to the first neuron of the first layer <br>\n",
    "$z_{2}^{[1]} =$ linear combination of input (feature) to the second neuron of the first layer <br>\n",
    "\n",
    "$a_{1}^{[1]} =$ output of the first neuron of the first layer<br>\n",
    "$a_{2}^{[1]} =$ output of the second neuron of the first layer<br>\n",
    "\n",
    "$z^{[2]} =$ linear combination of output of the first layer<br>\n",
    "$a^{[2]} =$ output of the only neuron of the second layer (final output)<br>\n",
    "\n",
    "name for each parameters can be seen on the model overview above\n",
    "\n",
    "#### Forward propagation:\n",
    "\n",
    "$$z_{1}^{[1]} = w_{1}^{[1]}x_{1} + w_{2}^{[1]}x_{2} + b_{1}^{[1]} $$ <br>\n",
    "$$z_{2}^{[1]} = w_{3}^{[1]}x_{1} + w_{4}^{[1]}x_{2} + b_{2}^{[1]} $$ <br>\n",
    "$$a_{1}^{[1]} = \\text{relu}(z_{1}^{[1]})$$ <br>\n",
    "$$a_{2}^{[1]} = \\text{relu}(z_{2}^{[1]})$$ <br>\n",
    "$$z^{[2]} = w_{1}^{[2]}x_{1} + w_{2}^{[2]}x_{2} + b^{[2]} $$ <br>\n",
    "$$a^{[2]} = \\frac{1}{1 + e^{-z^{[2]}}}$$ <br>\n",
    "\n",
    "#### cost & loss function: <br>\n",
    "$$\\text{cross entropy cost}(\\hat{a}^{[2]}, \\hat{y}) = -\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{j=1}^{m}y_{ij}\\text{log}(a^{[2]}[ij]) \\space\\text{where n=number of examples, m=number of class}$$ <br>\n",
    "$$\\text{binary cross entropy cost}(\\hat{a}^{[2]}, \\hat{y}) = -\\frac{1}{n}\\sum_{i=1}^{n}y_{i}\\text{log}(a^{[2]}[i]) + (1-y_{i})\\text{log}(1-a^{[2]}[i])$$ <br>\n",
    "\n",
    "#####  Derivative of cost function w.r.t parameters\n",
    "$$\\frac{\\partial ( \\text{binary cross entropy cost}(\\vec{a}^{[2]}, \\vec{y}))}{\\partial \\vec{w}} = \\frac{1}{n}(\\frac {\\partial (\\text{binary cross entropy loss}(a^{[2]}, \\hat{y}))_{1}}{\\partial \\vec{w}} + \\frac {\\partial (\\text{binary cross entropy loss}(a^{[2]}, \\hat{y}))_{2}}{\\partial \\vec{w}}+ ...+ \\frac {\\partial (\\text{binary cross entropy loss}(a^{[2]}, \\hat{y}))_{n}}{\\partial \\vec{w}})\\space \\text{for n=number of examples}$$<br>\n",
    "\n",
    "$$\\text{binary cross entropy loss}(a^{[2]}, \\hat{y}) = -(y_{i}\\text{log}(a^{[2]}) + (1-y_{i})\\text{log}(1-a^{[2]}))$$\n",
    "\n",
    "- Intuitively speaking, the gradient of cost function wrt to parameter $w$ is the average of all the gradient of loss function wrt to parameter $w_{1}, w_{2},..., w_{n}$ where $n=$number of examples.\n",
    "\n",
    "\n",
    "#### Backward propagation:\n",
    "- The point of backpropagation is to determine the gradient of cost function wrt to each parameter $w$\n",
    "- Backpropagation involves the derivation of loss function wrt to each parameters, then averaging them later for gradient descent\n",
    "\n",
    "##### output layer\n",
    "$\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z^{[2]}}=\\frac{\\partial a^{[2]}}{\\partial z^{[2]}}\\times\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial a^{[2]}} = (1-a^{[2]})\\times \\frac{a^{[2]}-y}{a^{[2]}(1-a^{[2]})} = a^{[2]} - y$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial w_{1}^{[2]}} = \\frac{\\partial z^{[2]}}{\\partial w_{1}^{[2]}}\\times\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z^{[2]}} = a_{1}^{[1]}\\times (a^{[2]} - y)$$\n",
    "\n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial w_{2}^{[2]}} = \\frac{\\partial z^{[2]}}{\\partial w_{2}^{[2]}}\\times\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z^{[2]}} = a_{2}^{[1]}\\times (a^{[2]} - y)$$\n",
    "\n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial b^{[2]}} = \\frac{\\partial z^{[2]}}{\\partial b^{[2]}}\\times\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z^{[2]}} = a^{[2]} - y$$\n",
    "\n",
    "\n",
    "##### hidden layer\n",
    "$\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z_{1}^{[1]}}=\\frac{\\partial a_{1}^{[1]}}{\\partial z_{1}^{[1]}}\\times\\frac{\\partial z^{[2]}}{\\partial a_{1}^{[1]}}\\times\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z^{[2]}} = \\text{relu'}(z_1^{[1]}) \\times w_1^{[2]}\\times (a^{[2]} - y)$ <br>\n",
    "\n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial w_{1}^{[1]}} = \\frac{\\partial z_{1}^{[1]}}{\\partial w_{1}^{[1]}}\\times \\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z_{1}^{[1]}} = x_1\\times \\text{relu'}(z_1^{[1]}) \\times w_1^{[2]}\\times (a^{[2]} - y)$$\n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial w_{2}^{[1]}} = \\frac{\\partial z_{1}^{[1]}}{\\partial w_{2}^{[1]}}\\times \\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z_{1}^{[1]}} = x_2\\times \\text{relu'}(z_1^{[1]}) \\times w_1^{[2]}\\times (a^{[2]} - y)$$ \n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial b_{1}^{[1]}} = \\frac{\\partial z_{1}^{[1]}}{\\partial b_{1}^{[1]}}\\times \\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z_{1}^{[1]}} = \\text{relu'}(z_1^{[1]}) \\times w_1^{[2]}\\times (a^{[2]} - y)$$ \n",
    "<br>\n",
    "\n",
    "$\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z_{2}^{[1]}}=\\frac{\\partial a_{1}^{[1]}}{\\partial z_{2}^{[1]}}\\times\\frac{\\partial z^{[2]}}{\\partial a_{1}^{[1]}}\\times\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z^{[2]}} = \\text{relu'}(z_2^{[1]}) \\times w_2^{[2]}\\times (a^{[2]} - y)$ <br>\n",
    "\n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial w_{3}^{[1]}} = \\frac{\\partial z_{2}^{[1]}}{\\partial w_{3}^{[1]}}\\times \\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z_{1}^{[1]}} = x_1\\times \\text{relu'}(z_2^{[1]}) \\times w_2^{[2]}\\times (a^{[2]} - y)$$\n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial w_{4}^{[1]}} = \\frac{\\partial z_{1}^{[1]}}{\\partial w_{4}^{[1]}}\\times \\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z_{1}^{[1]}} = x_2\\times \\text{relu'}(z_1^{[1]}) \\times w_1^{[2]}\\times (a^{[2]} - y)$$ \n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial b_{2}^{[1]}} = \\frac{\\partial z_{1}^{[1]}}{\\partial b_{2}^{[1]}}\\times \\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z_{1}^{[1]}} = \\text{relu'}(z_1^{[1]}) \\times w_1^{[2]}\\times (a^{[2]} - y)$$ \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f41c1db",
   "metadata": {},
   "source": [
    "### No vectorization (just for loop), vanilla gradient descent, and non-modular implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7782d8bb",
   "metadata": {},
   "source": [
    "This implementation is tailored for learning, hence not for production usage. It is meant to be explicit and not use any vectorization. It is also non-modular, as it can only be of the specified architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9564fd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    \n",
    "    def initialize(self):\n",
    "        self.w11, self.w12, self.w13, self.w14, self.w21, self.w22, self.b11, self.b12, self.b2 = np.random.randn(9)\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def binary_cross_entropy(self, x, y_true):\n",
    "        return -(y_true * np.log(x) + (1 - y_true) * np.log(1 - x))\n",
    "    \n",
    "    def derivative_relu(self, x):\n",
    "        return 1 if x>0 else 0\n",
    "    \n",
    "    def forward_prop(self, x1, x2):\n",
    "        forward_dict = {} \n",
    "        \n",
    "        # hidden layer\n",
    "        forward_dict[\"z11\"] = self.w11 * x1 + self.w12 * x2 + self.b11\n",
    "        forward_dict[\"z12\"] = self.w13 * x1 + self.w14 * x2 + self.b12\n",
    "        forward_dict[\"a11\"] = self.relu(forward_dict[\"z11\"])\n",
    "        forward_dict[\"a12\"] = self.relu(forward_dict[\"z12\"])\n",
    "        \n",
    "        # output layer\n",
    "        forward_dict[\"z2\"] = self.w21 * forward_dict[\"a11\"] + self.w22 * forward_dict[\"a12\"] + self.b2\n",
    "        forward_dict[\"a2\"] = self.sigmoid(forward_dict[\"z2\"])\n",
    "        \n",
    "        return forward_dict\n",
    "\n",
    "    def back_prop(self, x1, x2, y_true, forward_dict):\n",
    "        # gradient will be calculated for each training example and averaged later#\n",
    "        \n",
    "        deriva_dict = {}\n",
    "        \n",
    "        # output layer\n",
    "        error = forward_dict[\"a2\"] - y_true\n",
    "        \n",
    "        deriva_dict[\"dloss_dw21\"] = forward_dict[\"a11\"] * error\n",
    "        deriva_dict[\"dloss_dw22\"] = forward_dict[\"a12\"] * error\n",
    "        deriva_dict[\"dloss_db2\"] = error\n",
    "        \n",
    "        # hidden layer\n",
    "        dcost_dz11 = self.derivative_relu(forward_dict[\"z11\"]) * self.w21 * error\n",
    "        dcost_dz12 = self.derivative_relu(forward_dict[\"z12\"]) * self.w22 * error\n",
    "        \n",
    "        deriva_dict[\"dloss_dw11\"] = x1 * dcost_dz11\n",
    "        deriva_dict[\"dloss_dw12\"] = x2 * dcost_dz11\n",
    "        deriva_dict[\"dloss_db11\"] = dcost_dz11\n",
    "        \n",
    "        deriva_dict[\"dloss_dw13\"] = x1 * dcost_dz12\n",
    "        deriva_dict[\"dloss_dw14\"] = x2 * dcost_dz12\n",
    "        deriva_dict[\"dloss_db12\"] = dcost_dz12\n",
    "        \n",
    "        return deriva_dict\n",
    "    \n",
    "    def get_prob_class(self, x1, x2):\n",
    "        prob = self.forward_prop(x1, x2)[\"a2\"]\n",
    "        class_ = self.encode_class(prob)\n",
    "        return prob, class_\n",
    "    \n",
    "    def encode_class(self, x):\n",
    "        return 1 if x>0.5 else 0\n",
    "    \n",
    "    def get_accuracy_cost(self, X, y):\n",
    "        n = X.shape[0]\n",
    "        n_correct = 0\n",
    "        cost = 0\n",
    "        for i in range(n):\n",
    "            x1, x2 = X[i]\n",
    "            prediction = self.get_prob_class(x1, x2)\n",
    "            if prediction[1] == y[i][0]:\n",
    "                n_correct += 1\n",
    "            cost += self.binary_cross_entropy(prediction[0], y[i][0])\n",
    "        return n_correct/n, cost/n\n",
    "\n",
    "    def fit(self, X, y, alpha, epochs):\n",
    "        n, m = X.shape\n",
    "        \n",
    "        self.initialize()\n",
    "        costs = []\n",
    "\n",
    "        for i in range(1, epochs+1):\n",
    "            # Loop to iterate based on specified epoch\n",
    "            \n",
    "            dcost_dw21 = 0\n",
    "            dcost_dw22 = 0\n",
    "            dcost_db2 = 0\n",
    "            dcost_dw11 = 0\n",
    "            dcost_dw12 = 0\n",
    "            dcost_db11 = 0\n",
    "            dcost_dw13 = 0\n",
    "            dcost_dw14 = 0\n",
    "            dcost_db12 = 0\n",
    "            \n",
    "            for j in range(n):\n",
    "                # Loop to find derivatives for each example\n",
    "                \n",
    "                x1 , x2 = X[j]\n",
    "                y_true = y[j][0]\n",
    "                forward_prop = self.forward_prop(x1, x2)\n",
    "                back_prop = self.back_prop(x1, x2, y_true, forward_prop)\n",
    "                \n",
    "                dcost_dw21 += back_prop[\"dloss_dw21\"]\n",
    "                dcost_dw22 += back_prop[\"dloss_dw22\"]\n",
    "                dcost_db2 += back_prop[\"dloss_db2\"]\n",
    "                dcost_dw11 += back_prop[\"dloss_dw11\"]\n",
    "                dcost_dw12 += back_prop[\"dloss_dw12\"]\n",
    "                dcost_db11 += back_prop[\"dloss_db11\"]\n",
    "                dcost_dw13 += back_prop[\"dloss_dw13\"]\n",
    "                dcost_dw14 += back_prop[\"dloss_dw14\"]\n",
    "                dcost_db12 += back_prop[\"dloss_db12\"]\n",
    "            \n",
    "            # The parameter update process\n",
    "            self.w21 -= alpha*(1/n)*dcost_dw21 \n",
    "            self.w22 -= alpha*(1/n)*dcost_dw22 \n",
    "            self.b2  -= alpha*(1/n)*dcost_db2\n",
    "            self.w11 -= alpha*(1/n)*dcost_dw11 \n",
    "            self.w12 -= alpha*(1/n)*dcost_dw12 \n",
    "            self.b11 -= alpha*(1/n)*dcost_db11\n",
    "            self.w13 -= alpha*(1/n)*dcost_dw13\n",
    "            self.w14 -= alpha*(1/n)*dcost_dw14\n",
    "            self.b12 -= alpha*(1/n)*dcost_db12\n",
    "            \n",
    "            accuracy, cost = self.get_accuracy_cost(X, y)\n",
    "            costs.append(cost)\n",
    "            \n",
    "            print(f\"epoch: {i}, cost: {cost:.4f}, accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d3e8e3",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e3e3aaaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, cost: 1.6640, accuracy: 21.10%\n",
      "epoch: 2, cost: 1.1853, accuracy: 22.20%\n",
      "epoch: 3, cost: 0.8256, accuracy: 26.59%\n",
      "epoch: 4, cost: 0.6817, accuracy: 37.58%\n",
      "epoch: 5, cost: 0.6716, accuracy: 38.02%\n",
      "epoch: 6, cost: 0.6649, accuracy: 38.02%\n",
      "epoch: 7, cost: 0.6585, accuracy: 38.90%\n",
      "epoch: 8, cost: 0.6523, accuracy: 39.56%\n",
      "epoch: 9, cost: 0.6462, accuracy: 40.00%\n",
      "epoch: 10, cost: 0.6402, accuracy: 41.10%\n",
      "epoch: 11, cost: 0.6343, accuracy: 41.10%\n",
      "epoch: 12, cost: 0.6284, accuracy: 41.32%\n",
      "epoch: 13, cost: 0.6227, accuracy: 41.54%\n",
      "epoch: 14, cost: 0.6170, accuracy: 83.52%\n",
      "epoch: 15, cost: 0.6114, accuracy: 83.30%\n",
      "epoch: 16, cost: 0.6059, accuracy: 83.52%\n",
      "epoch: 17, cost: 0.6004, accuracy: 83.52%\n",
      "epoch: 18, cost: 0.5951, accuracy: 83.52%\n",
      "epoch: 19, cost: 0.5898, accuracy: 83.52%\n",
      "epoch: 20, cost: 0.5845, accuracy: 83.74%\n",
      "epoch: 21, cost: 0.5794, accuracy: 83.74%\n",
      "epoch: 22, cost: 0.5743, accuracy: 83.96%\n",
      "epoch: 23, cost: 0.5692, accuracy: 84.40%\n",
      "epoch: 24, cost: 0.5643, accuracy: 84.40%\n",
      "epoch: 25, cost: 0.5594, accuracy: 84.62%\n",
      "epoch: 26, cost: 0.5546, accuracy: 84.40%\n",
      "epoch: 27, cost: 0.5498, accuracy: 84.40%\n",
      "epoch: 28, cost: 0.5451, accuracy: 84.62%\n",
      "epoch: 29, cost: 0.5405, accuracy: 84.62%\n",
      "epoch: 30, cost: 0.5359, accuracy: 84.62%\n",
      "epoch: 31, cost: 0.5314, accuracy: 85.05%\n",
      "epoch: 32, cost: 0.5270, accuracy: 85.05%\n",
      "epoch: 33, cost: 0.5227, accuracy: 85.05%\n",
      "epoch: 34, cost: 0.5184, accuracy: 85.05%\n",
      "epoch: 35, cost: 0.5142, accuracy: 85.27%\n",
      "epoch: 36, cost: 0.5101, accuracy: 85.49%\n",
      "epoch: 37, cost: 0.5060, accuracy: 85.71%\n",
      "epoch: 38, cost: 0.5020, accuracy: 85.71%\n",
      "epoch: 39, cost: 0.4981, accuracy: 85.71%\n",
      "epoch: 40, cost: 0.4942, accuracy: 85.71%\n",
      "epoch: 41, cost: 0.4904, accuracy: 85.93%\n",
      "epoch: 42, cost: 0.4867, accuracy: 85.93%\n",
      "epoch: 43, cost: 0.4831, accuracy: 85.93%\n",
      "epoch: 44, cost: 0.4795, accuracy: 85.93%\n",
      "epoch: 45, cost: 0.4761, accuracy: 85.93%\n",
      "epoch: 46, cost: 0.4727, accuracy: 85.93%\n",
      "epoch: 47, cost: 0.4694, accuracy: 86.15%\n",
      "epoch: 48, cost: 0.4661, accuracy: 86.15%\n",
      "epoch: 49, cost: 0.4630, accuracy: 86.15%\n",
      "epoch: 50, cost: 0.4599, accuracy: 86.15%\n",
      "epoch: 51, cost: 0.4568, accuracy: 86.15%\n",
      "epoch: 52, cost: 0.4538, accuracy: 86.15%\n",
      "epoch: 53, cost: 0.4508, accuracy: 86.37%\n",
      "epoch: 54, cost: 0.4479, accuracy: 86.37%\n",
      "epoch: 55, cost: 0.4451, accuracy: 86.37%\n",
      "epoch: 56, cost: 0.4424, accuracy: 86.37%\n",
      "epoch: 57, cost: 0.4396, accuracy: 86.59%\n",
      "epoch: 58, cost: 0.4368, accuracy: 86.59%\n",
      "epoch: 59, cost: 0.4341, accuracy: 86.59%\n",
      "epoch: 60, cost: 0.4315, accuracy: 86.81%\n",
      "epoch: 61, cost: 0.4288, accuracy: 86.81%\n",
      "epoch: 62, cost: 0.4262, accuracy: 86.81%\n",
      "epoch: 63, cost: 0.4236, accuracy: 86.81%\n",
      "epoch: 64, cost: 0.4212, accuracy: 87.03%\n",
      "epoch: 65, cost: 0.4187, accuracy: 87.03%\n",
      "epoch: 66, cost: 0.4164, accuracy: 87.03%\n",
      "epoch: 67, cost: 0.4141, accuracy: 87.03%\n",
      "epoch: 68, cost: 0.4119, accuracy: 87.03%\n",
      "epoch: 69, cost: 0.4097, accuracy: 87.03%\n",
      "epoch: 70, cost: 0.4075, accuracy: 87.03%\n",
      "epoch: 71, cost: 0.4054, accuracy: 87.03%\n",
      "epoch: 72, cost: 0.4034, accuracy: 87.03%\n",
      "epoch: 73, cost: 0.4014, accuracy: 87.25%\n",
      "epoch: 74, cost: 0.3994, accuracy: 87.25%\n",
      "epoch: 75, cost: 0.3975, accuracy: 87.25%\n",
      "epoch: 76, cost: 0.3957, accuracy: 87.03%\n",
      "epoch: 77, cost: 0.3938, accuracy: 87.25%\n",
      "epoch: 78, cost: 0.3920, accuracy: 87.47%\n",
      "epoch: 79, cost: 0.3903, accuracy: 87.47%\n",
      "epoch: 80, cost: 0.3886, accuracy: 87.47%\n",
      "epoch: 81, cost: 0.3869, accuracy: 87.47%\n",
      "epoch: 82, cost: 0.3853, accuracy: 87.47%\n",
      "epoch: 83, cost: 0.3837, accuracy: 87.47%\n",
      "epoch: 84, cost: 0.3821, accuracy: 87.69%\n",
      "epoch: 85, cost: 0.3806, accuracy: 87.69%\n",
      "epoch: 86, cost: 0.3791, accuracy: 87.69%\n",
      "epoch: 87, cost: 0.3775, accuracy: 87.69%\n",
      "epoch: 88, cost: 0.3761, accuracy: 87.69%\n",
      "epoch: 89, cost: 0.3746, accuracy: 87.69%\n",
      "epoch: 90, cost: 0.3732, accuracy: 87.69%\n",
      "epoch: 91, cost: 0.3718, accuracy: 87.69%\n",
      "epoch: 92, cost: 0.3704, accuracy: 88.13%\n",
      "epoch: 93, cost: 0.3690, accuracy: 88.13%\n",
      "epoch: 94, cost: 0.3677, accuracy: 88.13%\n",
      "epoch: 95, cost: 0.3664, accuracy: 88.13%\n",
      "epoch: 96, cost: 0.3652, accuracy: 88.13%\n",
      "epoch: 97, cost: 0.3639, accuracy: 88.13%\n",
      "epoch: 98, cost: 0.3627, accuracy: 88.13%\n",
      "epoch: 99, cost: 0.3615, accuracy: 88.13%\n",
      "epoch: 100, cost: 0.3604, accuracy: 88.13%\n",
      "epoch: 101, cost: 0.3593, accuracy: 88.13%\n",
      "epoch: 102, cost: 0.3582, accuracy: 88.13%\n",
      "epoch: 103, cost: 0.3571, accuracy: 88.13%\n",
      "epoch: 104, cost: 0.3560, accuracy: 88.13%\n",
      "epoch: 105, cost: 0.3550, accuracy: 88.13%\n",
      "epoch: 106, cost: 0.3540, accuracy: 88.13%\n",
      "epoch: 107, cost: 0.3530, accuracy: 88.13%\n",
      "epoch: 108, cost: 0.3520, accuracy: 88.13%\n",
      "epoch: 109, cost: 0.3510, accuracy: 88.13%\n",
      "epoch: 110, cost: 0.3501, accuracy: 88.13%\n",
      "epoch: 111, cost: 0.3491, accuracy: 88.13%\n",
      "epoch: 112, cost: 0.3482, accuracy: 88.13%\n",
      "epoch: 113, cost: 0.3472, accuracy: 88.13%\n",
      "epoch: 114, cost: 0.3462, accuracy: 88.13%\n",
      "epoch: 115, cost: 0.3452, accuracy: 88.13%\n",
      "epoch: 116, cost: 0.3443, accuracy: 88.13%\n",
      "epoch: 117, cost: 0.3434, accuracy: 88.13%\n",
      "epoch: 118, cost: 0.3425, accuracy: 88.13%\n",
      "epoch: 119, cost: 0.3416, accuracy: 88.13%\n",
      "epoch: 120, cost: 0.3407, accuracy: 88.13%\n",
      "epoch: 121, cost: 0.3399, accuracy: 88.13%\n",
      "epoch: 122, cost: 0.3390, accuracy: 88.13%\n",
      "epoch: 123, cost: 0.3382, accuracy: 88.13%\n",
      "epoch: 124, cost: 0.3374, accuracy: 88.13%\n",
      "epoch: 125, cost: 0.3365, accuracy: 88.13%\n",
      "epoch: 126, cost: 0.3357, accuracy: 88.13%\n",
      "epoch: 127, cost: 0.3349, accuracy: 88.13%\n",
      "epoch: 128, cost: 0.3341, accuracy: 88.13%\n",
      "epoch: 129, cost: 0.3332, accuracy: 88.13%\n",
      "epoch: 130, cost: 0.3324, accuracy: 88.13%\n",
      "epoch: 131, cost: 0.3315, accuracy: 88.13%\n",
      "epoch: 132, cost: 0.3307, accuracy: 88.13%\n",
      "epoch: 133, cost: 0.3299, accuracy: 88.13%\n",
      "epoch: 134, cost: 0.3292, accuracy: 88.13%\n",
      "epoch: 135, cost: 0.3284, accuracy: 88.13%\n",
      "epoch: 136, cost: 0.3276, accuracy: 88.13%\n",
      "epoch: 137, cost: 0.3269, accuracy: 88.13%\n",
      "epoch: 138, cost: 0.3262, accuracy: 88.13%\n",
      "epoch: 139, cost: 0.3254, accuracy: 88.13%\n",
      "epoch: 140, cost: 0.3247, accuracy: 88.13%\n",
      "epoch: 141, cost: 0.3240, accuracy: 88.13%\n",
      "epoch: 142, cost: 0.3233, accuracy: 88.13%\n",
      "epoch: 143, cost: 0.3227, accuracy: 88.13%\n",
      "epoch: 144, cost: 0.3220, accuracy: 88.13%\n",
      "epoch: 145, cost: 0.3214, accuracy: 88.13%\n",
      "epoch: 146, cost: 0.3207, accuracy: 88.13%\n",
      "epoch: 147, cost: 0.3201, accuracy: 88.13%\n",
      "epoch: 148, cost: 0.3194, accuracy: 88.13%\n",
      "epoch: 149, cost: 0.3188, accuracy: 88.35%\n",
      "epoch: 150, cost: 0.3182, accuracy: 88.35%\n",
      "epoch: 151, cost: 0.3176, accuracy: 88.35%\n",
      "epoch: 152, cost: 0.3170, accuracy: 88.35%\n",
      "epoch: 153, cost: 0.3164, accuracy: 88.35%\n",
      "epoch: 154, cost: 0.3158, accuracy: 88.35%\n",
      "epoch: 155, cost: 0.3152, accuracy: 88.35%\n",
      "epoch: 156, cost: 0.3146, accuracy: 88.35%\n",
      "epoch: 157, cost: 0.3140, accuracy: 88.35%\n",
      "epoch: 158, cost: 0.3134, accuracy: 88.35%\n",
      "epoch: 159, cost: 0.3128, accuracy: 88.35%\n",
      "epoch: 160, cost: 0.3123, accuracy: 88.35%\n",
      "epoch: 161, cost: 0.3117, accuracy: 88.57%\n",
      "epoch: 162, cost: 0.3111, accuracy: 88.57%\n",
      "epoch: 163, cost: 0.3106, accuracy: 88.57%\n",
      "epoch: 164, cost: 0.3100, accuracy: 88.57%\n",
      "epoch: 165, cost: 0.3095, accuracy: 88.57%\n",
      "epoch: 166, cost: 0.3090, accuracy: 88.57%\n",
      "epoch: 167, cost: 0.3085, accuracy: 88.79%\n",
      "epoch: 168, cost: 0.3080, accuracy: 88.79%\n",
      "epoch: 169, cost: 0.3075, accuracy: 88.57%\n",
      "epoch: 170, cost: 0.3070, accuracy: 88.57%\n",
      "epoch: 171, cost: 0.3065, accuracy: 88.57%\n",
      "epoch: 172, cost: 0.3060, accuracy: 88.57%\n",
      "epoch: 173, cost: 0.3055, accuracy: 88.57%\n",
      "epoch: 174, cost: 0.3050, accuracy: 88.57%\n",
      "epoch: 175, cost: 0.3046, accuracy: 88.57%\n",
      "epoch: 176, cost: 0.3041, accuracy: 88.57%\n",
      "epoch: 177, cost: 0.3037, accuracy: 88.57%\n",
      "epoch: 178, cost: 0.3032, accuracy: 88.57%\n",
      "epoch: 179, cost: 0.3028, accuracy: 88.57%\n",
      "epoch: 180, cost: 0.3024, accuracy: 88.57%\n",
      "epoch: 181, cost: 0.3019, accuracy: 88.57%\n",
      "epoch: 182, cost: 0.3015, accuracy: 88.57%\n",
      "epoch: 183, cost: 0.3011, accuracy: 88.57%\n",
      "epoch: 184, cost: 0.3007, accuracy: 88.57%\n",
      "epoch: 185, cost: 0.3003, accuracy: 88.57%\n",
      "epoch: 186, cost: 0.2999, accuracy: 88.57%\n",
      "epoch: 187, cost: 0.2995, accuracy: 88.57%\n",
      "epoch: 188, cost: 0.2991, accuracy: 88.57%\n",
      "epoch: 189, cost: 0.2987, accuracy: 88.57%\n",
      "epoch: 190, cost: 0.2983, accuracy: 88.35%\n",
      "epoch: 191, cost: 0.2979, accuracy: 88.35%\n",
      "epoch: 192, cost: 0.2975, accuracy: 88.35%\n",
      "epoch: 193, cost: 0.2971, accuracy: 89.01%\n",
      "epoch: 194, cost: 0.2967, accuracy: 89.01%\n",
      "epoch: 195, cost: 0.2963, accuracy: 89.01%\n",
      "epoch: 196, cost: 0.2960, accuracy: 89.01%\n",
      "epoch: 197, cost: 0.2956, accuracy: 89.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 198, cost: 0.2952, accuracy: 89.01%\n",
      "epoch: 199, cost: 0.2948, accuracy: 89.01%\n",
      "epoch: 200, cost: 0.2945, accuracy: 89.01%\n"
     ]
    }
   ],
   "source": [
    "classifier = NeuralNetwork()\n",
    "classifier.fit(X_train, y_train, 0.1, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598580c2",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c8a6dd82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 0.247\n",
      "accuracy: 0.921\n"
     ]
    }
   ],
   "source": [
    "result = classifier.get_accuracy_cost(X_test, y_test)\n",
    "\n",
    "print(f\"cost: {result[1]:.3f}\")\n",
    "print(f\"accuracy: {result[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe28c3c",
   "metadata": {},
   "source": [
    "## Bonus: Vanilla gradient descent vs mini batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c0b02f",
   "metadata": {},
   "source": [
    "The following is the same neural network with the same architecture but with mini batch gradient descent instead of vanilla gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "65243079",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork_batch():\n",
    "    \n",
    "    def initialize(self):\n",
    "        self.w11, self.w12, self.w13, self.w14, self.w21, self.w22, self.b11, self.b12, self.b2 = np.random.randn(9)\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def binary_cross_entropy(self, x, y_true):\n",
    "        return -(y_true * np.log(x) + (1 - y_true) * np.log(1 - x))\n",
    "    \n",
    "    def derivative_relu(self, x):\n",
    "        return 1 if x>0 else 0\n",
    "    \n",
    "    def forward_prop(self, x1, x2):\n",
    "        forward_dict = {} \n",
    "        \n",
    "        # hidden layer\n",
    "        forward_dict[\"z11\"] = self.w11 * x1 + self.w12 * x2 + self.b11\n",
    "        forward_dict[\"z12\"] = self.w13 * x1 + self.w14 * x2 + self.b12\n",
    "        forward_dict[\"a11\"] = self.relu(forward_dict[\"z11\"])\n",
    "        forward_dict[\"a12\"] = self.relu(forward_dict[\"z12\"])\n",
    "        \n",
    "        # output layer\n",
    "        forward_dict[\"z2\"] = self.w21 * forward_dict[\"a11\"] + self.w22 * forward_dict[\"a12\"] + self.b2\n",
    "        forward_dict[\"a2\"] = self.sigmoid(forward_dict[\"z2\"])\n",
    "        \n",
    "        return forward_dict\n",
    "\n",
    "    def back_prop(self, x1, x2, y_true, forward_dict):\n",
    "        # gradient will be calculated for each training example and averaged later#\n",
    "        \n",
    "        deriva_dict = {}\n",
    "        \n",
    "        # output layer\n",
    "        error = forward_dict[\"a2\"] - y_true\n",
    "        \n",
    "        deriva_dict[\"dloss_dw21\"] = forward_dict[\"a11\"] * error\n",
    "        deriva_dict[\"dloss_dw22\"] = forward_dict[\"a12\"] * error\n",
    "        deriva_dict[\"dloss_db2\"] = error\n",
    "        \n",
    "        # hidden layer\n",
    "        dcost_dz11 = self.derivative_relu(forward_dict[\"z11\"]) * self.w21 * error\n",
    "        dcost_dz12 = self.derivative_relu(forward_dict[\"z12\"]) * self.w22 * error\n",
    "        \n",
    "        deriva_dict[\"dloss_dw11\"] = x1 * dcost_dz11\n",
    "        deriva_dict[\"dloss_dw12\"] = x2 * dcost_dz11\n",
    "        deriva_dict[\"dloss_db11\"] = dcost_dz11\n",
    "        \n",
    "        deriva_dict[\"dloss_dw13\"] = x1 * dcost_dz12\n",
    "        deriva_dict[\"dloss_dw14\"] = x2 * dcost_dz12\n",
    "        deriva_dict[\"dloss_db12\"] = dcost_dz12\n",
    "        \n",
    "        return deriva_dict\n",
    "    \n",
    "    def get_prob_class(self, x1, x2):\n",
    "        prob = self.forward_prop(x1, x2)[\"a2\"]\n",
    "        class_ = self.encode_class(prob)\n",
    "        return prob, class_\n",
    "    \n",
    "    def encode_class(self, x):\n",
    "        return 1 if x>0.5 else 0\n",
    "    \n",
    "    def get_accuracy_cost(self, X, y):\n",
    "        n = X.shape[0]\n",
    "        n_correct = 0\n",
    "        cost_total = 0\n",
    "        for i in range(n):\n",
    "            x1, x2 = X[i]\n",
    "            prediction = self.get_prob_class(x1, x2)\n",
    "            if prediction[1] == y[i][0]:\n",
    "                n_correct += 1\n",
    "            cost_total += self.binary_cross_entropy(prediction[0], y[i][0])\n",
    "        return n_correct/n, cost_total/n\n",
    "\n",
    "    def fit(self, X, y, alpha, epochs, batch_size=32):\n",
    "        n, m = X.shape\n",
    "        combined = np.hstack([X, y])\n",
    "        self.initialize()\n",
    "        n_iter_batch = np.ceil(n/batch_size)  # Number of training iteration for one epoch\n",
    "        \n",
    "\n",
    "        for i in range(1, epochs+1):\n",
    "            dcost_dw21 = 0\n",
    "            dcost_dw22 = 0\n",
    "            dcost_db2 = 0\n",
    "            dcost_dw11 = 0\n",
    "            dcost_dw12 = 0\n",
    "            dcost_db11 = 0\n",
    "            dcost_dw13 = 0\n",
    "            dcost_dw14 = 0\n",
    "            dcost_db12 = 0\n",
    "            \n",
    "            #Shuffled the dataset before splitting\n",
    "            combined_shuffled = np.random.permutation(combined)\n",
    "            X_shuffled = combined_shuffled[:, 0:-1]\n",
    "            y_shuffled = combined_shuffled[:, -1].reshape(-1, 1)\n",
    "            \n",
    "            costs_batch = 0\n",
    "            accuracies_batch = 0\n",
    "            for k in tqdm(range(0, n, batch_size), position=0):\n",
    "                X_batch = X_shuffled[k: k+batch_size]\n",
    "                y_batch = y_shuffled[k: k+batch_size]\n",
    "                n_batch = X_batch.shape[0]\n",
    "            \n",
    "                for j in range(n_batch):\n",
    "                    x1 , x2 = X_batch[j]\n",
    "                    y_true = y_batch[j][0]\n",
    "                    forward_prop = self.forward_prop(x1, x2)\n",
    "                    back_prop = self.back_prop(x1, x2, y_true, forward_prop)\n",
    "\n",
    "                    dcost_dw21 += back_prop[\"dloss_dw21\"]\n",
    "                    dcost_dw22 += back_prop[\"dloss_dw22\"]\n",
    "                    dcost_db2 += back_prop[\"dloss_db2\"]\n",
    "                    dcost_dw11 += back_prop[\"dloss_dw11\"]\n",
    "                    dcost_dw12 += back_prop[\"dloss_dw12\"]\n",
    "                    dcost_db11 += back_prop[\"dloss_db11\"]\n",
    "                    dcost_dw13 += back_prop[\"dloss_dw13\"]\n",
    "                    dcost_dw14 += back_prop[\"dloss_dw14\"]\n",
    "                    dcost_db12 += back_prop[\"dloss_db12\"]\n",
    "\n",
    "                self.w21 -= alpha*(1/n)*dcost_dw21 \n",
    "                self.w22 -= alpha*(1/n)*dcost_dw22 \n",
    "                self.b2  -= alpha*(1/n)*dcost_db2\n",
    "                self.w11 -= alpha*(1/n)*dcost_dw11 \n",
    "                self.w12 -= alpha*(1/n)*dcost_dw12 \n",
    "                self.b11 -= alpha*(1/n)*dcost_db11\n",
    "                self.w13 -= alpha*(1/n)*dcost_dw13\n",
    "                self.w14 -= alpha*(1/n)*dcost_dw14\n",
    "                self.b12 -= alpha*(1/n)*dcost_db12\n",
    "\n",
    "                accuracy, cost = self.get_accuracy_cost(X_batch, y_batch)\n",
    "                costs_batch += cost\n",
    "                accuracies_batch += accuracy\n",
    "            \n",
    "            # Reported cost and accuracy are averaged across the mini batch\n",
    "            # of a given epoch\n",
    "            epoch_cost = costs_batch/n_iter_batch\n",
    "            epoch_accuracy = accuracies_batch/n_iter_batch\n",
    "            print(f\"epoch: {i}, cost: {epoch_cost:.4f}, accuracy: {epoch_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672481ba",
   "metadata": {},
   "source": [
    "### Training using small epoch (30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc64ef6",
   "metadata": {},
   "source": [
    "#### With mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3d365253",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 377.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, cost: 0.5522, accuracy: 0.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 256.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, cost: 0.3144, accuracy: 0.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 203.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, cost: 0.2553, accuracy: 0.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 378.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, cost: 0.2363, accuracy: 0.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 381.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, cost: 0.2295, accuracy: 0.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 270.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, cost: 0.2264, accuracy: 0.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 323.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, cost: 0.2235, accuracy: 0.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 387.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8, cost: 0.2137, accuracy: 0.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 366.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9, cost: 0.2182, accuracy: 0.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 409.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, cost: 0.2168, accuracy: 0.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 443.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11, cost: 0.2237, accuracy: 0.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 381.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12, cost: 0.1982, accuracy: 0.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 388.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13, cost: 0.2284, accuracy: 0.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 354.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14, cost: 0.1985, accuracy: 0.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 351.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15, cost: 0.1973, accuracy: 0.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 244.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16, cost: 0.2020, accuracy: 0.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 353.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17, cost: 0.1994, accuracy: 0.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 347.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18, cost: 0.1865, accuracy: 0.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 392.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19, cost: 0.1857, accuracy: 0.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 346.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, cost: 0.1844, accuracy: 0.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 232.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21, cost: 0.1889, accuracy: 0.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 245.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22, cost: 0.1861, accuracy: 0.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 399.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23, cost: 0.1806, accuracy: 0.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 370.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24, cost: 0.2016, accuracy: 0.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 228.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25, cost: 0.1766, accuracy: 0.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 329.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26, cost: 0.1731, accuracy: 0.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 276.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27, cost: 0.1710, accuracy: 0.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 267.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28, cost: 0.1866, accuracy: 0.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 238.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29, cost: 0.1748, accuracy: 0.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 206.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30, cost: 0.1683, accuracy: 0.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "classifier_batch = NeuralNetwork_batch()\n",
    "classifier_batch.fit(X_train, y_train, 0.1, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89f97e3",
   "metadata": {},
   "source": [
    "#### Without mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "15e1457e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, cost: 1.3219, accuracy: 25.49%\n",
      "epoch: 2, cost: 0.9042, accuracy: 49.45%\n",
      "epoch: 3, cost: 0.7690, accuracy: 61.98%\n",
      "epoch: 4, cost: 0.7222, accuracy: 62.86%\n",
      "epoch: 5, cost: 0.6971, accuracy: 62.86%\n",
      "epoch: 6, cost: 0.6795, accuracy: 62.86%\n",
      "epoch: 7, cost: 0.6652, accuracy: 62.86%\n",
      "epoch: 8, cost: 0.6528, accuracy: 62.86%\n",
      "epoch: 9, cost: 0.6416, accuracy: 62.86%\n",
      "epoch: 10, cost: 0.6311, accuracy: 62.86%\n",
      "epoch: 11, cost: 0.6211, accuracy: 62.86%\n",
      "epoch: 12, cost: 0.6115, accuracy: 62.86%\n",
      "epoch: 13, cost: 0.6024, accuracy: 62.86%\n",
      "epoch: 14, cost: 0.5936, accuracy: 62.86%\n",
      "epoch: 15, cost: 0.5854, accuracy: 62.86%\n",
      "epoch: 16, cost: 0.5773, accuracy: 62.86%\n",
      "epoch: 17, cost: 0.5694, accuracy: 62.86%\n",
      "epoch: 18, cost: 0.5617, accuracy: 62.86%\n",
      "epoch: 19, cost: 0.5541, accuracy: 62.86%\n",
      "epoch: 20, cost: 0.5468, accuracy: 62.86%\n",
      "epoch: 21, cost: 0.5397, accuracy: 62.86%\n",
      "epoch: 22, cost: 0.5328, accuracy: 62.86%\n",
      "epoch: 23, cost: 0.5261, accuracy: 62.86%\n",
      "epoch: 24, cost: 0.5197, accuracy: 62.86%\n",
      "epoch: 25, cost: 0.5134, accuracy: 62.86%\n",
      "epoch: 26, cost: 0.5072, accuracy: 62.86%\n",
      "epoch: 27, cost: 0.5012, accuracy: 62.86%\n",
      "epoch: 28, cost: 0.4954, accuracy: 62.86%\n",
      "epoch: 29, cost: 0.4896, accuracy: 62.86%\n",
      "epoch: 30, cost: 0.4840, accuracy: 62.86%\n"
     ]
    }
   ],
   "source": [
    "classifier = NeuralNetwork()\n",
    "classifier.fit(X_train, y_train, 0.1, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93f3701",
   "metadata": {},
   "source": [
    "### Evaluation & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "692f6920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost without minibatch: 0.414\n",
      "accuracy without minibatch: 0.623\n",
      "cost with minibatch: 0.097\n",
      "accuracy with minibatch: 0.982\n"
     ]
    }
   ],
   "source": [
    "classifier_ = classifier.get_accuracy_cost(X_test, y_test)\n",
    "\n",
    "print(f\"cost without minibatch: {classifier_[1]:.3f}\")\n",
    "print(f\"accuracy without minibatch: {classifier_[0]:.3f}\")\n",
    "\n",
    "classifier_batch_ = classifier_batch.get_accuracy_cost(X_test, y_test)\n",
    "\n",
    "print(f\"cost with minibatch: {classifier_batch_[1]:.3f}\")\n",
    "print(f\"accuracy with minibatch: {classifier_batch_[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebcf361",
   "metadata": {},
   "source": [
    "It appears that, in the long run, mini batch gradient descent can achieve better metrics using less epoch, hence more efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b479547",
   "metadata": {},
   "source": [
    "# That's it. not bad at all :p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

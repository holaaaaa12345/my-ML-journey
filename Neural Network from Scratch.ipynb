{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02737cbd",
   "metadata": {},
   "source": [
    "# Neural network learning from scratch (Math + Numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7a2fbf",
   "metadata": {},
   "source": [
    "This is a major milestone during my self-taught machine learning journey. Understanding the backpropagation and calculus in it is quite an experience. That said, this is really the simplest implementation :p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "285ac7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Normalization\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid, softmax\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df75c1df",
   "metadata": {},
   "source": [
    "### Objective: Determine a tumor's malignancy given its texture_mean and radius_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9e7e12",
   "metadata": {},
   "source": [
    "#### The dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c8caac3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>radius_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10.38</td>\n",
       "      <td>17.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17.77</td>\n",
       "      <td>20.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>21.25</td>\n",
       "      <td>19.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>20.38</td>\n",
       "      <td>11.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>14.34</td>\n",
       "      <td>20.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>1</td>\n",
       "      <td>22.39</td>\n",
       "      <td>21.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>1</td>\n",
       "      <td>28.25</td>\n",
       "      <td>20.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1</td>\n",
       "      <td>28.08</td>\n",
       "      <td>16.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>1</td>\n",
       "      <td>29.33</td>\n",
       "      <td>20.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>0</td>\n",
       "      <td>24.54</td>\n",
       "      <td>7.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     diagnosis  texture_mean  radius_mean\n",
       "0            1         10.38        17.99\n",
       "1            1         17.77        20.57\n",
       "2            1         21.25        19.69\n",
       "3            1         20.38        11.42\n",
       "4            1         14.34        20.29\n",
       "..         ...           ...          ...\n",
       "564          1         22.39        21.56\n",
       "565          1         28.25        20.13\n",
       "566          1         28.08        16.60\n",
       "567          1         29.33        20.60\n",
       "568          0         24.54         7.76\n",
       "\n",
       "[569 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"bdiag.csv\")[[\"diagnosis\", \"texture_mean\", \"radius_mean\"]]\n",
    "df[\"diagnosis\"] = df[\"diagnosis\"].replace({\"M\":1, \"B\":0})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b617ade",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b641944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"texture_mean\", \"radius_mean\"]].values\n",
    "y = df[\"diagnosis\"].values.reshape(-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b827b2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (455, 2)\n",
      "Y_train: (455, 1)\n",
      "X_test:  (114, 2)\n",
      "Y_test:  (114, 1)\n"
     ]
    }
   ],
   "source": [
    "print('X_train: ' + str(X_train.shape))\n",
    "print('Y_train: ' + str(y_train.shape))\n",
    "print('X_test:  '  + str(X_test.shape))\n",
    "print('Y_test:  '  + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7056c1",
   "metadata": {},
   "source": [
    "### Feature normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c85e917",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = Normalization(axis=1)\n",
    "norm.adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d033f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(norm(X_train))  # converts back to numpy because tensor is slower\n",
    "X_test = np.array(norm(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63535d4f",
   "metadata": {},
   "source": [
    "### The Model overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cce2cc6",
   "metadata": {},
   "source": [
    "![alt text](model_nn_scratch.jpg \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1324519",
   "metadata": {},
   "source": [
    "## Doing it the \"easy\" way (using library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95555dbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "15/15 [==============================] - 4s 8ms/step - loss: 0.6008 - accuracy: 0.6242\n",
      "Epoch 2/30\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.5939 - accuracy: 0.6242\n",
      "Epoch 3/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5872 - accuracy: 0.6242\n",
      "Epoch 4/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5807 - accuracy: 0.6242\n",
      "Epoch 5/30\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5742 - accuracy: 0.6242\n",
      "Epoch 6/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5681 - accuracy: 0.6242\n",
      "Epoch 7/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5619 - accuracy: 0.6242\n",
      "Epoch 8/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5561 - accuracy: 0.6242\n",
      "Epoch 9/30\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5503 - accuracy: 0.6242\n",
      "Epoch 10/30\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.5446 - accuracy: 0.6242\n",
      "Epoch 11/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5391 - accuracy: 0.6242\n",
      "Epoch 12/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5337 - accuracy: 0.6242\n",
      "Epoch 13/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5285 - accuracy: 0.6242\n",
      "Epoch 14/30\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5229 - accuracy: 0.6242\n",
      "Epoch 15/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5178 - accuracy: 0.6242\n",
      "Epoch 16/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5127 - accuracy: 0.6242\n",
      "Epoch 17/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5074 - accuracy: 0.6242\n",
      "Epoch 18/30\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.5025 - accuracy: 0.6242\n",
      "Epoch 19/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.4975 - accuracy: 0.6242\n",
      "Epoch 20/30\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4927 - accuracy: 0.6242\n",
      "Epoch 21/30\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4875 - accuracy: 0.6242\n",
      "Epoch 22/30\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4825 - accuracy: 0.6242\n",
      "Epoch 23/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.4776 - accuracy: 0.6242\n",
      "Epoch 24/30\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4724 - accuracy: 0.6242\n",
      "Epoch 25/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.4671 - accuracy: 0.6264\n",
      "Epoch 26/30\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4618 - accuracy: 0.6264\n",
      "Epoch 27/30\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4561 - accuracy: 0.6286\n",
      "Epoch 28/30\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4504 - accuracy: 0.6308\n",
      "Epoch 29/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.4445 - accuracy: 0.6418\n",
      "Epoch 30/30\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4385 - accuracy: 0.6440\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x284a16fdaf0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    \n",
    "    Dense(units=2, activation=relu),\n",
    "    Dense(units=1, activation=linear)\n",
    "])\n",
    "\n",
    "model.compile(loss=BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=Adam(learning_rate=0.001),\n",
    "              metrics=\"accuracy\"\n",
    "             )\n",
    "model.fit(X_train, y_train, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5610e0c6",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4df90265",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4603 - accuracy: 0.6491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.46033743023872375, 0.6491228342056274]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf855c6c",
   "metadata": {},
   "source": [
    "# Doing it the hard way (scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce210e61",
   "metadata": {},
   "source": [
    "### The math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7843ec1c",
   "metadata": {},
   "source": [
    "2 features --> 1 hidden layer (2 neuron relu) --> 1 output layer (1 neuron sigmoid)\n",
    "\n",
    "#### Forward propagation:\n",
    "\n",
    "$$z_{1}^{[1]} = w_{1}^{[1]}x_{1} + w_{2}^{[1]}x_{2} + b_{1}^{[1]} $$ <br>\n",
    "$$z_{2}^{[1]} = w_{3}^{[1]}x_{1} + w_{4}^{[1]}x_{2} + b_{2}^{[1]} $$ <br>\n",
    "$$a_{1}^{[1]} = \\text{relu}(z_{1}^{[1]})$$ <br>\n",
    "$$a_{2}^{[1]} = \\text{relu}(z_{2}^{[1]})$$ <br>\n",
    "$$z^{[2]} = w_{1}^{[2]}x_{1} + w_{2}^{[2]}x_{2} + b^{[2]} $$ <br>\n",
    "$$a^{[2]} = \\frac{1}{1 + e^{-z^{[2]}}}$$ <br>\n",
    "\n",
    "#### cost & loss function: <br>\n",
    "$$\\text{cross entropy cost}(\\hat{a}^{[2]}, \\hat{y}) = -\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{j=1}^{m}y_{ij}\\text{log}(a^{[2]}[ij])$$ <br>\n",
    "$$\\text{binary cross entropy cost}(\\hat{a}^{[2]}, \\hat{y}) = -\\frac{1}{n}\\sum_{i=1}^{n}y_{i}\\text{log}(a^{[2]}[i]) + (1-y_{i})\\text{log}(1-a^{[2]}[i])$$ <br>\n",
    "\n",
    "#####  Derivative of cost function w.r.t parameters\n",
    "$$\\frac{\\partial ( \\text{binary cross entropy cost}(\\vec{a}^{[2]}, \\vec{y}))}{\\partial \\vec{w}} = \\frac{1}{n}(\\frac {\\partial (\\text{binary cross entropy loss}(a^{[2]}, \\hat{y}))_{1}}{\\partial \\vec{w}} + \\frac {\\partial (\\text{binary cross entropy loss}(a^{[2]}, \\hat{y}))_{2}}{\\partial \\vec{w}}+ ...+ \\frac {\\partial (\\text{binary cross entropy loss}(a^{[2]}, \\hat{y}))_{n}}{\\partial \\vec{w}})\\space \\text{for n=number of examples}$$<br>\n",
    "\n",
    "$$\\text{binary cross entropy loss}(a^{[2]}, \\hat{y}) = -(y_{i}\\text{log}(a^{[2]}) + (1-y_{i})\\text{log}(1-a^{[2]}))$$\n",
    "\n",
    "- The point of backpropagation is to determine the gradient of cost function wrt to each parameter $w$\n",
    "- Intuitively speaking, the gradient of cost function wrt to parameter $w$ is the average of all the gradient of loss function wrt to parameter $w_{1}, w_{2},..., w_{n}$ where $n=$number of examples.\n",
    "- Backpropagation involves the derivation of loss function wrt to each parameters, then averaging them later for gradient descent\n",
    "\n",
    "#### Backward propagation:\n",
    "\n",
    "##### output layer\n",
    "$\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z^{[2]}}=\\frac{\\partial a^{[2]}}{\\partial z^{[2]}}\\times\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial a^{[2]}} = (1-a^{[2]})\\times \\frac{a^{[2]}-y}{a^{[2]}(1-a^{[2]})} = a^{[2]} - y$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial w_{1}^{[2]}} = \\frac{\\partial z^{[2]}}{\\partial w_{1}^{[2]}}\\times\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z^{[2]}} = a_{1}^{[1]}\\times (a^{[2]} - y)$$\n",
    "\n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial w_{2}^{[2]}} = \\frac{\\partial z^{[2]}}{\\partial w_{2}^{[2]}}\\times\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z^{[2]}} = a_{2}^{[1]}\\times (a^{[2]} - y)$$\n",
    "\n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial b^{[2]}} = \\frac{\\partial z^{[2]}}{\\partial b^{[2]}}\\times\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z^{[2]}} = a^{[2]} - y$$\n",
    "\n",
    "\n",
    "##### hidden layer\n",
    "$\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z_{1}^{[1]}}=\\frac{\\partial a_{1}^{[1]}}{\\partial z_{1}^{[1]}}\\times\\frac{\\partial z^{[2]}}{\\partial a_{1}^{[1]}}\\times\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z^{[2]}} = \\text{relu'}(z_1^{[1]}) \\times w_1^{[2]}\\times (a^{[2]} - y)$ <br>\n",
    "\n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial w_{1}^{[1]}} = \\frac{\\partial z_{1}^{[1]}}{\\partial w_{1}^{[1]}}\\times \\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z_{1}^{[1]}} = x_1\\times \\text{relu'}(z_1^{[1]}) \\times w_1^{[2]}\\times (a^{[2]} - y)$$\n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial w_{2}^{[1]}} = \\frac{\\partial z_{1}^{[1]}}{\\partial w_{2}^{[1]}}\\times \\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z_{1}^{[1]}} = x_2\\times \\text{relu'}(z_1^{[1]}) \\times w_1^{[2]}\\times (a^{[2]} - y)$$ \n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial b_{1}^{[1]}} = \\frac{\\partial z_{1}^{[1]}}{\\partial b_{1}^{[1]}}\\times \\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z_{1}^{[1]}} = \\text{relu'}(z_1^{[1]}) \\times w_1^{[2]}\\times (a^{[2]} - y)$$ \n",
    "<br>\n",
    "\n",
    "$\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z_{2}^{[1]}}=\\frac{\\partial a_{1}^{[1]}}{\\partial z_{2}^{[1]}}\\times\\frac{\\partial z^{[2]}}{\\partial a_{1}^{[1]}}\\times\\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z^{[2]}} = \\text{relu'}(z_2^{[1]}) \\times w_2^{[2]}\\times (a^{[2]} - y)$ <br>\n",
    "\n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial w_{3}^{[1]}} = \\frac{\\partial z_{2}^{[1]}}{\\partial w_{3}^{[1]}}\\times \\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z_{1}^{[1]}} = x_1\\times \\text{relu'}(z_2^{[1]}) \\times w_2^{[2]}\\times (a^{[2]} - y)$$\n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial w_{4}^{[1]}} = \\frac{\\partial z_{1}^{[1]}}{\\partial w_{4}^{[1]}}\\times \\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z_{1}^{[1]}} = x_2\\times \\text{relu'}(z_1^{[1]}) \\times w_1^{[2]}\\times (a^{[2]} - y)$$ \n",
    "$$\\frac{\\partial\\text{cost}(a^{[2]})}{\\partial b_{2}^{[1]}} = \\frac{\\partial z_{1}^{[1]}}{\\partial b_{2}^{[1]}}\\times \\frac{\\partial \\text{cost}(a^{[2]})}{\\partial z_{1}^{[1]}} = \\text{relu'}(z_1^{[1]}) \\times w_1^{[2]}\\times (a^{[2]} - y)$$ \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f41c1db",
   "metadata": {},
   "source": [
    "### No vectorization and non-modular implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7782d8bb",
   "metadata": {},
   "source": [
    "This implementation is tailored for learning, hence not for production usage. It is meant to be explicit and not use any vectorization. It is also non-modular, as it can only be of the specified architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65243079",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    \n",
    "    def initialize(self):\n",
    "        self.w11, self.w12, self.w13, self.w14, self.w21, self.w22, self.b11, self.b12, self.b2 = np.random.randn(9)\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def binary_cross_entropy(self, x, y_true):\n",
    "        return -(y_true * np.log(x) + (1 - y_true) * np.log(1 - x))\n",
    "    \n",
    "    def derivative_relu(self, x):\n",
    "        return 1 if x>0 else 0\n",
    "    \n",
    "    def forward_prop(self, x1, x2):\n",
    "        forward_dict = {} \n",
    "        \n",
    "        # hidden layer\n",
    "        forward_dict[\"z11\"] = self.w11 * x1 + self.w12 * x2 + self.b11\n",
    "        forward_dict[\"z12\"] = self.w13 * x1 + self.w14 * x2 + self.b12\n",
    "        forward_dict[\"a11\"] = self.relu(forward_dict[\"z11\"])\n",
    "        forward_dict[\"a12\"] = self.relu(forward_dict[\"z12\"])\n",
    "        \n",
    "        # output layer\n",
    "        forward_dict[\"z2\"] = self.w21 * forward_dict[\"a11\"] + self.w22 * forward_dict[\"a12\"] + self.b2\n",
    "        forward_dict[\"a2\"] = self.sigmoid(forward_dict[\"z2\"])\n",
    "        \n",
    "        return forward_dict\n",
    "\n",
    "    def back_prop(self, x1, x2, y_true, forward_dict):\n",
    "        # gradient will be calculated for each training example and averaged later#\n",
    "        \n",
    "        deriva_dict = {}\n",
    "        \n",
    "        # output layer\n",
    "        error = forward_dict[\"a2\"] - y_true\n",
    "        \n",
    "        deriva_dict[\"dloss_dw21\"] = forward_dict[\"a11\"] * error\n",
    "        deriva_dict[\"dloss_dw22\"] = forward_dict[\"a12\"] * error\n",
    "        deriva_dict[\"dloss_db2\"] = error\n",
    "        \n",
    "        # hidden layer\n",
    "        dcost_dz11 = self.derivative_relu(forward_dict[\"z11\"]) * self.w21 * error\n",
    "        dcost_dz12 = self.derivative_relu(forward_dict[\"z12\"]) * self.w22 * error\n",
    "        \n",
    "        deriva_dict[\"dloss_dw11\"] = x1 * dcost_dz11\n",
    "        deriva_dict[\"dloss_dw12\"] = x2 * dcost_dz11\n",
    "        deriva_dict[\"dloss_db11\"] = dcost_dz11\n",
    "        \n",
    "        deriva_dict[\"dloss_dw13\"] = x1 * dcost_dz12\n",
    "        deriva_dict[\"dloss_dw14\"] = x2 * dcost_dz12\n",
    "        deriva_dict[\"dloss_db12\"] = dcost_dz12\n",
    "        \n",
    "        return deriva_dict\n",
    "    \n",
    "    def get_prob_class(self, x1, x2):\n",
    "        prob = self.forward_prop(x1, x2)[\"a2\"]\n",
    "        class_ = self.encode_class(prob)\n",
    "        return prob, class_\n",
    "    \n",
    "    def encode_class(self, x):\n",
    "        return 1 if x>0.5 else 0\n",
    "    \n",
    "    def get_accuracy_cost(self, X, y):\n",
    "        n = X.shape[0]\n",
    "        n_correct = 0\n",
    "        cost = 0\n",
    "        for i in range(n):\n",
    "            x1, x2 = X[i]\n",
    "            prediction = self.get_prob_class(x1, x2)\n",
    "            if prediction[1] == y[i][0]:\n",
    "                n_correct += 1\n",
    "            cost += self.binary_cross_entropy(prediction[0], y[i][0])\n",
    "        return n_correct/n, cost\n",
    "\n",
    "    def fit(self, X, y, alpha, epochs):\n",
    "        n, m = X.shape\n",
    "        \n",
    "        self.initialize()\n",
    "        costs = []\n",
    "\n",
    "        for i in range(1, epochs+1):\n",
    "            dcost_dw21 = 0\n",
    "            dcost_dw22 = 0\n",
    "            dcost_db2 = 0\n",
    "            dcost_dw11 = 0\n",
    "            dcost_dw12 = 0\n",
    "            dcost_db11 = 0\n",
    "            dcost_dw13 = 0\n",
    "            dcost_dw14 = 0\n",
    "            dcost_db12 = 0\n",
    "            \n",
    "            for j in range(n):\n",
    "                x1 , x2 = X[j]\n",
    "                y_true = y[j][0]\n",
    "                forward_prop = self.forward_prop(x1, x2)\n",
    "                back_prop = self.back_prop(x1, x2, y_true, forward_prop)\n",
    "                \n",
    "                dcost_dw21 += back_prop[\"dloss_dw21\"]\n",
    "                dcost_dw22 += back_prop[\"dloss_dw22\"]\n",
    "                dcost_db2 += back_prop[\"dloss_db2\"]\n",
    "                dcost_dw11 += back_prop[\"dloss_dw11\"]\n",
    "                dcost_dw12 += back_prop[\"dloss_dw12\"]\n",
    "                dcost_db11 += back_prop[\"dloss_db11\"]\n",
    "                dcost_dw13 += back_prop[\"dloss_dw13\"]\n",
    "                dcost_dw14 += back_prop[\"dloss_dw14\"]\n",
    "                dcost_db12 += back_prop[\"dloss_db12\"]\n",
    "                    \n",
    "            self.w21 -= alpha*(1/n)*dcost_dw21 \n",
    "            self.w22 -= alpha*(1/n)*dcost_dw22 \n",
    "            self.b2  -= alpha*(1/n)*dcost_db2\n",
    "            self.w11 -= alpha*(1/n)*dcost_dw11 \n",
    "            self.w12 -= alpha*(1/n)*dcost_dw12 \n",
    "            self.b11 -= alpha*(1/n)*dcost_db11\n",
    "            self.w13 -= alpha*(1/n)*dcost_dw13\n",
    "            self.w14 -= alpha*(1/n)*dcost_dw14\n",
    "            self.b12 -= alpha*(1/n)*dcost_db12\n",
    "            \n",
    "            accuracy, cost = self.get_accuracy_cost(X, y)\n",
    "            costs.append(cost)\n",
    "            \n",
    "            print(f\"epoch: {i}, cost: {cost:.2f}, accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d3e8e3",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3e3aaaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, cost: 1538.06, accuracy: 37.58%\n",
      "epoch: 2, cost: 1408.38, accuracy: 37.58%\n",
      "epoch: 3, cost: 1289.91, accuracy: 37.58%\n",
      "epoch: 4, cost: 1180.47, accuracy: 37.58%\n",
      "epoch: 5, cost: 1078.23, accuracy: 37.58%\n",
      "epoch: 6, cost: 981.71, accuracy: 37.58%\n",
      "epoch: 7, cost: 889.76, accuracy: 37.58%\n",
      "epoch: 8, cost: 801.73, accuracy: 37.58%\n",
      "epoch: 9, cost: 717.54, accuracy: 37.58%\n",
      "epoch: 10, cost: 637.78, accuracy: 37.58%\n",
      "epoch: 11, cost: 563.64, accuracy: 37.80%\n",
      "epoch: 12, cost: 496.53, accuracy: 40.44%\n",
      "epoch: 13, cost: 437.57, accuracy: 43.96%\n",
      "epoch: 14, cost: 387.07, accuracy: 50.33%\n",
      "epoch: 15, cost: 344.58, accuracy: 55.38%\n",
      "epoch: 16, cost: 309.22, accuracy: 60.66%\n",
      "epoch: 17, cost: 279.88, accuracy: 64.40%\n",
      "epoch: 18, cost: 255.56, accuracy: 68.79%\n",
      "epoch: 19, cost: 235.33, accuracy: 73.19%\n",
      "epoch: 20, cost: 218.39, accuracy: 74.29%\n",
      "epoch: 21, cost: 204.14, accuracy: 76.70%\n",
      "epoch: 22, cost: 192.12, accuracy: 79.56%\n",
      "epoch: 23, cost: 181.93, accuracy: 80.88%\n",
      "epoch: 24, cost: 173.25, accuracy: 81.54%\n",
      "epoch: 25, cost: 165.82, accuracy: 82.86%\n",
      "epoch: 26, cost: 159.41, accuracy: 83.30%\n",
      "epoch: 27, cost: 153.89, accuracy: 83.96%\n",
      "epoch: 28, cost: 149.12, accuracy: 84.62%\n",
      "epoch: 29, cost: 144.96, accuracy: 85.05%\n",
      "epoch: 30, cost: 141.33, accuracy: 86.15%\n",
      "epoch: 31, cost: 138.16, accuracy: 86.81%\n",
      "epoch: 32, cost: 135.38, accuracy: 87.69%\n",
      "epoch: 33, cost: 132.94, accuracy: 88.79%\n",
      "epoch: 34, cost: 130.78, accuracy: 88.79%\n",
      "epoch: 35, cost: 128.86, accuracy: 89.01%\n",
      "epoch: 36, cost: 127.17, accuracy: 88.57%\n",
      "epoch: 37, cost: 125.67, accuracy: 88.35%\n",
      "epoch: 38, cost: 124.34, accuracy: 88.57%\n",
      "epoch: 39, cost: 123.16, accuracy: 87.91%\n",
      "epoch: 40, cost: 122.10, accuracy: 88.13%\n",
      "epoch: 41, cost: 121.17, accuracy: 88.13%\n",
      "epoch: 42, cost: 120.33, accuracy: 88.35%\n",
      "epoch: 43, cost: 119.59, accuracy: 88.13%\n",
      "epoch: 44, cost: 118.92, accuracy: 88.57%\n",
      "epoch: 45, cost: 118.32, accuracy: 88.79%\n",
      "epoch: 46, cost: 117.79, accuracy: 88.79%\n",
      "epoch: 47, cost: 117.32, accuracy: 89.01%\n",
      "epoch: 48, cost: 116.89, accuracy: 89.01%\n",
      "epoch: 49, cost: 116.50, accuracy: 88.57%\n",
      "epoch: 50, cost: 116.16, accuracy: 88.57%\n",
      "epoch: 51, cost: 115.85, accuracy: 88.57%\n",
      "epoch: 52, cost: 115.57, accuracy: 88.79%\n",
      "epoch: 53, cost: 115.32, accuracy: 88.79%\n",
      "epoch: 54, cost: 115.09, accuracy: 88.79%\n",
      "epoch: 55, cost: 114.88, accuracy: 89.01%\n",
      "epoch: 56, cost: 114.69, accuracy: 89.23%\n",
      "epoch: 57, cost: 114.52, accuracy: 89.23%\n",
      "epoch: 58, cost: 114.37, accuracy: 89.23%\n",
      "epoch: 59, cost: 114.23, accuracy: 89.23%\n",
      "epoch: 60, cost: 114.10, accuracy: 89.23%\n",
      "epoch: 61, cost: 113.98, accuracy: 89.23%\n",
      "epoch: 62, cost: 113.87, accuracy: 89.23%\n",
      "epoch: 63, cost: 113.78, accuracy: 89.23%\n",
      "epoch: 64, cost: 113.69, accuracy: 89.23%\n",
      "epoch: 65, cost: 113.61, accuracy: 89.23%\n",
      "epoch: 66, cost: 113.53, accuracy: 89.45%\n",
      "epoch: 67, cost: 113.46, accuracy: 89.45%\n",
      "epoch: 68, cost: 113.39, accuracy: 89.45%\n",
      "epoch: 69, cost: 113.33, accuracy: 89.45%\n",
      "epoch: 70, cost: 113.28, accuracy: 89.45%\n",
      "epoch: 71, cost: 113.23, accuracy: 89.67%\n",
      "epoch: 72, cost: 113.18, accuracy: 89.67%\n",
      "epoch: 73, cost: 113.13, accuracy: 89.89%\n",
      "epoch: 74, cost: 113.09, accuracy: 89.89%\n",
      "epoch: 75, cost: 113.05, accuracy: 89.89%\n",
      "epoch: 76, cost: 113.01, accuracy: 89.89%\n",
      "epoch: 77, cost: 112.97, accuracy: 90.33%\n",
      "epoch: 78, cost: 112.94, accuracy: 90.33%\n",
      "epoch: 79, cost: 112.91, accuracy: 90.33%\n",
      "epoch: 80, cost: 112.87, accuracy: 90.33%\n",
      "epoch: 81, cost: 112.84, accuracy: 90.11%\n",
      "epoch: 82, cost: 112.81, accuracy: 89.89%\n",
      "epoch: 83, cost: 112.79, accuracy: 89.89%\n",
      "epoch: 84, cost: 112.76, accuracy: 89.89%\n",
      "epoch: 85, cost: 112.73, accuracy: 89.89%\n",
      "epoch: 86, cost: 112.71, accuracy: 89.89%\n",
      "epoch: 87, cost: 112.68, accuracy: 89.89%\n",
      "epoch: 88, cost: 112.66, accuracy: 89.89%\n",
      "epoch: 89, cost: 112.63, accuracy: 89.89%\n",
      "epoch: 90, cost: 112.61, accuracy: 89.89%\n",
      "epoch: 91, cost: 112.59, accuracy: 89.89%\n",
      "epoch: 92, cost: 112.57, accuracy: 89.89%\n",
      "epoch: 93, cost: 112.55, accuracy: 89.89%\n",
      "epoch: 94, cost: 112.52, accuracy: 89.89%\n",
      "epoch: 95, cost: 112.50, accuracy: 90.11%\n",
      "epoch: 96, cost: 112.48, accuracy: 90.11%\n",
      "epoch: 97, cost: 112.46, accuracy: 90.11%\n",
      "epoch: 98, cost: 112.44, accuracy: 90.11%\n",
      "epoch: 99, cost: 112.42, accuracy: 90.11%\n",
      "epoch: 100, cost: 112.40, accuracy: 90.11%\n"
     ]
    }
   ],
   "source": [
    "classifier = NeuralNetwork()\n",
    "classifier.fit(X_train, y_train, 0.1, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93f3701",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b9d9374",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9122807017543859, 35.2690970777859)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.get_accuracy_cost(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b479547",
   "metadata": {},
   "source": [
    "#### not bad at all :p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
